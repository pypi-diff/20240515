# Comparing `tmp/semantic_link_sempy-0.7.2-py3-none-any.whl.zip` & `tmp/semantic_link_sempy-0.7.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,105 +1,103 @@
-Zip file size: 2967347 bytes, number of entries: 103
--rw-rw-r--  2.0 unx     1097 b- defN 24-Mar-21 23:44 sempy/__init__.py
--rw-rw-r--  2.0 unx      497 b- defN 24-Mar-21 23:48 sempy/_version.py
--rw-rw-r--  2.0 unx      140 b- defN 24-Mar-21 23:44 sempy/dotnet.runtime.config.json
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-21 23:44 sempy/_metadata/__init__.py
--rw-rw-r--  2.0 unx    11141 b- defN 24-Mar-21 23:44 sempy/_metadata/_mdataframe.py
--rw-rw-r--  2.0 unx      828 b- defN 24-Mar-21 23:44 sempy/_metadata/_meta_utils.py
--rw-rw-r--  2.0 unx     3017 b- defN 24-Mar-21 23:44 sempy/_metadata/_mseries.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-21 23:44 sempy/_utils/__init__.py
--rw-rw-r--  2.0 unx    17194 b- defN 24-Mar-21 23:44 sempy/_utils/_log.py
--rw-rw-r--  2.0 unx     1180 b- defN 24-Mar-21 23:44 sempy/_utils/_ordered_set.py
--rw-rw-r--  2.0 unx     6195 b- defN 24-Mar-21 23:44 sempy/_utils/_pandas_utils.py
--rw-rw-r--  2.0 unx      109 b- defN 24-Mar-21 23:44 sempy/dependencies/__init__.py
--rw-rw-r--  2.0 unx    10216 b- defN 24-Mar-21 23:44 sempy/dependencies/_find.py
--rw-rw-r--  2.0 unx     3897 b- defN 24-Mar-21 23:44 sempy/dependencies/_plot.py
--rw-rw-r--  2.0 unx     6023 b- defN 24-Mar-21 23:44 sempy/dependencies/_stats.py
--rw-rw-r--  2.0 unx     4181 b- defN 24-Mar-21 23:44 sempy/dependencies/_validate.py
--rw-rw-r--  2.0 unx     3543 b- defN 24-Mar-21 23:44 sempy/fabric/__init__.py
--rw-rw-r--  2.0 unx      959 b- defN 24-Mar-21 23:44 sempy/fabric/_cache.py
--rw-rw-r--  2.0 unx      495 b- defN 24-Mar-21 23:44 sempy/fabric/_datacategory.py
--rw-rw-r--  2.0 unx     1520 b- defN 24-Mar-21 23:44 sempy/fabric/_daxmagics.py
--rw-rw-r--  2.0 unx     4314 b- defN 24-Mar-21 23:44 sempy/fabric/_environment.py
--rw-rw-r--  2.0 unx    56394 b- defN 24-Mar-21 23:44 sempy/fabric/_flat.py
--rw-rw-r--  2.0 unx     3703 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_annotations.py
--rw-rw-r--  2.0 unx      992 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_apps.py
--rw-rw-r--  2.0 unx     2908 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_calculation_items.py
--rw-rw-r--  2.0 unx    14653 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_columns.py
--rw-rw-r--  2.0 unx      835 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_dataflows.py
--rw-rw-r--  2.0 unx     3086 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_datasources.py
--rw-rw-r--  2.0 unx      765 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_gateways.py
--rw-rw-r--  2.0 unx     5048 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_hierarchies.py
--rw-rw-r--  2.0 unx     7609 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_partitions.py
--rw-rw-r--  2.0 unx     2194 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_perspectives.py
--rw-rw-r--  2.0 unx     7272 b- defN 24-Mar-21 23:44 sempy/fabric/_flat_list_relationships.py
--rw-rw-r--  2.0 unx     1256 b- defN 24-Mar-21 23:44 sempy/fabric/_metadatakeys.py
--rw-rw-r--  2.0 unx     2790 b- defN 24-Mar-21 23:44 sempy/fabric/_token_provider.py
--rw-rw-r--  2.0 unx     9009 b- defN 24-Mar-21 23:44 sempy/fabric/_utils.py
--rw-rw-r--  2.0 unx      384 b- defN 24-Mar-21 23:44 sempy/fabric/_client/__init__.py
--rw-rw-r--  2.0 unx     2715 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_adomd_connection.py
--rw-rw-r--  2.0 unx    24963 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_base_dataset_client.py
--rw-rw-r--  2.0 unx      639 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_connection_mode.py
--rw-rw-r--  2.0 unx     1515 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_dataset_onelake_import.py
--rw-rw-r--  2.0 unx     9807 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_dataset_rest_client.py
--rw-rw-r--  2.0 unx    13385 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_dataset_xmla_client.py
--rw-rw-r--  2.0 unx     7719 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_fabric_rest_api.py
--rw-rw-r--  2.0 unx    14867 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_pbi_rest_api.py
--rw-rw-r--  2.0 unx     1673 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_refresh_execution_details.py
--rw-rw-r--  2.0 unx     9301 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_rest_client.py
--rw-rw-r--  2.0 unx     3681 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_tools.py
--rw-rw-r--  2.0 unx     2906 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_utils.py
--rw-rw-r--  2.0 unx    25316 b- defN 24-Mar-21 23:44 sempy/fabric/_client/_workspace_client.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-21 23:44 sempy/fabric/_dataframe/__init__.py
--rw-rw-r--  2.0 unx    26953 b- defN 24-Mar-21 23:44 sempy/fabric/_dataframe/_fabric_dataframe.py
--rw-rw-r--  2.0 unx     1764 b- defN 24-Mar-21 23:44 sempy/fabric/_dataframe/_fabric_series.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-21 23:44 sempy/fabric/_trace/__init__.py
--rw-rw-r--  2.0 unx    11613 b- defN 24-Mar-21 23:44 sempy/fabric/_trace/_trace.py
--rw-rw-r--  2.0 unx     7110 b- defN 24-Mar-21 23:44 sempy/fabric/_trace/_trace_connection.py
--rw-rw-r--  2.0 unx      288 b- defN 24-Mar-21 23:44 sempy/fabric/exceptions/__init__.py
--rw-rw-r--  2.0 unx     2705 b- defN 24-Mar-21 23:44 sempy/fabric/exceptions/_exceptions.py
--rw-rw-r--  2.0 unx      503 b- defN 24-Mar-21 23:44 sempy/fabric/matcher/__init__.py
--rw-rw-r--  2.0 unx     3701 b- defN 24-Mar-21 23:44 sempy/fabric/matcher/_matcher.py
--rw-rw-r--  2.0 unx      462 b- defN 24-Mar-21 23:44 sempy/functions/__init__.py
--rw-rw-r--  2.0 unx     6093 b- defN 24-Mar-21 23:44 sempy/functions/_decorator.py
--rw-rw-r--  2.0 unx     6226 b- defN 24-Mar-21 23:44 sempy/functions/_function.py
--rw-rw-r--  2.0 unx     6444 b- defN 24-Mar-21 23:44 sempy/functions/_matcher_dataframe.py
--rw-rw-r--  2.0 unx     1602 b- defN 24-Mar-21 23:44 sempy/functions/_matcher_series.py
--rw-rw-r--  2.0 unx     4810 b- defN 24-Mar-21 23:44 sempy/functions/_registry.py
--rw-rw-r--  2.0 unx     2156 b- defN 24-Mar-21 23:44 sempy/functions/_util.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Mar-21 23:44 sempy/functions/_dataframe/__init__.py
--rw-rw-r--  2.0 unx     1192 b- defN 24-Mar-21 23:44 sempy/functions/_dataframe/_sdataframe.py
--rw-rw-r--  2.0 unx     1188 b- defN 24-Mar-21 23:44 sempy/functions/_dataframe/_sseries.py
--rw-rw-r--  2.0 unx      335 b- defN 24-Mar-21 23:44 sempy/functions/matcher/__init__.py
--rw-rw-r--  2.0 unx     5880 b- defN 24-Mar-21 23:44 sempy/functions/matcher/_matcher.py
+Zip file size: 2966225 bytes, number of entries: 101
+-rw-rw-r--  2.0 unx     1097 b- defN 24-May-14 10:06 sempy/__init__.py
+-rw-rw-r--  2.0 unx      497 b- defN 24-May-14 10:11 sempy/_version.py
+-rw-rw-r--  2.0 unx      167 b- defN 24-May-14 10:06 sempy/dotnet.runtime.config.json
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-14 10:06 sempy/_metadata/__init__.py
+-rw-rw-r--  2.0 unx    11141 b- defN 24-May-14 10:06 sempy/_metadata/_mdataframe.py
+-rw-rw-r--  2.0 unx      828 b- defN 24-May-14 10:06 sempy/_metadata/_meta_utils.py
+-rw-rw-r--  2.0 unx     3017 b- defN 24-May-14 10:06 sempy/_metadata/_mseries.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-14 10:06 sempy/_utils/__init__.py
+-rw-rw-r--  2.0 unx    17194 b- defN 24-May-14 10:06 sempy/_utils/_log.py
+-rw-rw-r--  2.0 unx     1180 b- defN 24-May-14 10:06 sempy/_utils/_ordered_set.py
+-rw-rw-r--  2.0 unx     6195 b- defN 24-May-14 10:06 sempy/_utils/_pandas_utils.py
+-rw-rw-r--  2.0 unx      109 b- defN 24-May-14 10:06 sempy/dependencies/__init__.py
+-rw-rw-r--  2.0 unx    10216 b- defN 24-May-14 10:06 sempy/dependencies/_find.py
+-rw-rw-r--  2.0 unx     3897 b- defN 24-May-14 10:06 sempy/dependencies/_plot.py
+-rw-rw-r--  2.0 unx     6023 b- defN 24-May-14 10:06 sempy/dependencies/_stats.py
+-rw-rw-r--  2.0 unx     4181 b- defN 24-May-14 10:06 sempy/dependencies/_validate.py
+-rw-rw-r--  2.0 unx     3543 b- defN 24-May-14 10:06 sempy/fabric/__init__.py
+-rw-rw-r--  2.0 unx      959 b- defN 24-May-14 10:06 sempy/fabric/_cache.py
+-rw-rw-r--  2.0 unx      495 b- defN 24-May-14 10:06 sempy/fabric/_datacategory.py
+-rw-rw-r--  2.0 unx     1520 b- defN 24-May-14 10:06 sempy/fabric/_daxmagics.py
+-rw-rw-r--  2.0 unx     4946 b- defN 24-May-14 10:06 sempy/fabric/_environment.py
+-rw-rw-r--  2.0 unx    56530 b- defN 24-May-14 10:06 sempy/fabric/_flat.py
+-rw-rw-r--  2.0 unx     3727 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_annotations.py
+-rw-rw-r--  2.0 unx      992 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_apps.py
+-rw-rw-r--  2.0 unx     2932 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_calculation_items.py
+-rw-rw-r--  2.0 unx    14677 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_columns.py
+-rw-rw-r--  2.0 unx      835 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_dataflows.py
+-rw-rw-r--  2.0 unx     3110 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_datasources.py
+-rw-rw-r--  2.0 unx      765 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_gateways.py
+-rw-rw-r--  2.0 unx     5072 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_hierarchies.py
+-rw-rw-r--  2.0 unx     7697 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_partitions.py
+-rw-rw-r--  2.0 unx     2218 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_perspectives.py
+-rw-rw-r--  2.0 unx     7317 b- defN 24-May-14 10:06 sempy/fabric/_flat_list_relationships.py
+-rw-rw-r--  2.0 unx     1256 b- defN 24-May-14 10:06 sempy/fabric/_metadatakeys.py
+-rw-rw-r--  2.0 unx     2743 b- defN 24-May-14 10:06 sempy/fabric/_token_provider.py
+-rw-rw-r--  2.0 unx     9111 b- defN 24-May-14 10:06 sempy/fabric/_utils.py
+-rw-rw-r--  2.0 unx      384 b- defN 24-May-14 10:06 sempy/fabric/_client/__init__.py
+-rw-rw-r--  2.0 unx     2715 b- defN 24-May-14 10:06 sempy/fabric/_client/_adomd_connection.py
+-rw-rw-r--  2.0 unx    28046 b- defN 24-May-14 10:06 sempy/fabric/_client/_base_dataset_client.py
+-rw-rw-r--  2.0 unx      639 b- defN 24-May-14 10:06 sempy/fabric/_client/_connection_mode.py
+-rw-rw-r--  2.0 unx     1541 b- defN 24-May-14 10:06 sempy/fabric/_client/_dataset_onelake_import.py
+-rw-rw-r--  2.0 unx     9814 b- defN 24-May-14 10:06 sempy/fabric/_client/_dataset_rest_client.py
+-rw-rw-r--  2.0 unx    13483 b- defN 24-May-14 10:06 sempy/fabric/_client/_dataset_xmla_client.py
+-rw-rw-r--  2.0 unx     7719 b- defN 24-May-14 10:06 sempy/fabric/_client/_fabric_rest_api.py
+-rw-rw-r--  2.0 unx    14867 b- defN 24-May-14 10:06 sempy/fabric/_client/_pbi_rest_api.py
+-rw-rw-r--  2.0 unx     1673 b- defN 24-May-14 10:06 sempy/fabric/_client/_refresh_execution_details.py
+-rw-rw-r--  2.0 unx     9301 b- defN 24-May-14 10:06 sempy/fabric/_client/_rest_client.py
+-rw-rw-r--  2.0 unx     3681 b- defN 24-May-14 10:06 sempy/fabric/_client/_tools.py
+-rw-rw-r--  2.0 unx     2906 b- defN 24-May-14 10:06 sempy/fabric/_client/_utils.py
+-rw-rw-r--  2.0 unx    25368 b- defN 24-May-14 10:06 sempy/fabric/_client/_workspace_client.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-14 10:06 sempy/fabric/_dataframe/__init__.py
+-rw-rw-r--  2.0 unx    27600 b- defN 24-May-14 10:06 sempy/fabric/_dataframe/_fabric_dataframe.py
+-rw-rw-r--  2.0 unx     1764 b- defN 24-May-14 10:06 sempy/fabric/_dataframe/_fabric_series.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-14 10:06 sempy/fabric/_trace/__init__.py
+-rw-rw-r--  2.0 unx    11613 b- defN 24-May-14 10:06 sempy/fabric/_trace/_trace.py
+-rw-rw-r--  2.0 unx     7110 b- defN 24-May-14 10:06 sempy/fabric/_trace/_trace_connection.py
+-rw-rw-r--  2.0 unx      288 b- defN 24-May-14 10:06 sempy/fabric/exceptions/__init__.py
+-rw-rw-r--  2.0 unx     2705 b- defN 24-May-14 10:06 sempy/fabric/exceptions/_exceptions.py
+-rw-rw-r--  2.0 unx      503 b- defN 24-May-14 10:06 sempy/fabric/matcher/__init__.py
+-rw-rw-r--  2.0 unx     3701 b- defN 24-May-14 10:06 sempy/fabric/matcher/_matcher.py
+-rw-rw-r--  2.0 unx      462 b- defN 24-May-14 10:06 sempy/functions/__init__.py
+-rw-rw-r--  2.0 unx     6093 b- defN 24-May-14 10:06 sempy/functions/_decorator.py
+-rw-rw-r--  2.0 unx     6226 b- defN 24-May-14 10:06 sempy/functions/_function.py
+-rw-rw-r--  2.0 unx     6444 b- defN 24-May-14 10:06 sempy/functions/_matcher_dataframe.py
+-rw-rw-r--  2.0 unx     1602 b- defN 24-May-14 10:06 sempy/functions/_matcher_series.py
+-rw-rw-r--  2.0 unx     4810 b- defN 24-May-14 10:06 sempy/functions/_registry.py
+-rw-rw-r--  2.0 unx     2156 b- defN 24-May-14 10:06 sempy/functions/_util.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-May-14 10:06 sempy/functions/_dataframe/__init__.py
+-rw-rw-r--  2.0 unx     1192 b- defN 24-May-14 10:06 sempy/functions/_dataframe/_sdataframe.py
+-rw-rw-r--  2.0 unx     1188 b- defN 24-May-14 10:06 sempy/functions/_dataframe/_sseries.py
+-rw-rw-r--  2.0 unx      335 b- defN 24-May-14 10:06 sempy/functions/matcher/__init__.py
+-rw-rw-r--  2.0 unx     5880 b- defN 24-May-14 10:06 sempy/functions/matcher/_matcher.py
 -rwxr--r--  2.0 unx   170496 b- defN 20-Oct-13 00:20 sempy/lib/Apache.Arrow.dll
 -rwxr--r--  2.0 unx    13312 b- defN 23-May-10 17:41 sempy/lib/IronCompress.dll
 -rwxr--r--  2.0 unx   846392 b- defN 23-Nov-02 21:14 sempy/lib/Microsoft.AnalysisServices.AdomdClient.dll
 -rwxr--r--  2.0 unx  1146416 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.Core.dll
 -rwxr--r--  2.0 unx    98864 b- defN 23-Nov-02 21:14 sempy/lib/Microsoft.AnalysisServices.Runtime.Core.dll
 -rwxr--r--  2.0 unx    96816 b- defN 23-Nov-02 21:14 sempy/lib/Microsoft.AnalysisServices.Runtime.Windows.dll
 -rwxr--r--  2.0 unx   563248 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.Tabular.Json.dll
 -rwxr--r--  2.0 unx  1640496 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.Tabular.dll
 -rwxr--r--  2.0 unx   675888 b- defN 23-Nov-02 21:15 sempy/lib/Microsoft.AnalysisServices.dll
 -rwxr--r--  2.0 unx   597632 b- defN 23-Jan-31 23:19 sempy/lib/Microsoft.Data.Analysis.dll
--rw-rw-r--  2.0 unx    16896 b- defN 24-Mar-21 23:46 sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll
+-rw-rw-r--  2.0 unx    16896 b- defN 24-May-14 10:08 sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll
 -rwxr--r--  2.0 unx    64960 b- defN 23-Mar-02 23:04 sempy/lib/Microsoft.IO.RecyclableMemoryStream.dll
 -rwxr--r--  2.0 unx  1402840 b- defN 22-Apr-04 18:15 sempy/lib/Microsoft.Identity.Client.dll
 -rwxr--r--  2.0 unx    48256 b- defN 23-Jan-31 23:19 sempy/lib/Microsoft.ML.DataView.dll
 -rwxr--r--  2.0 unx   692736 b- defN 23-Jun-30 10:39 sempy/lib/Parquet.dll
 -rwxr--r--  2.0 unx    41472 b- defN 23-Mar-27 11:54 sempy/lib/Snappier.dll
 -rwxr--r--  2.0 unx   442368 b- defN 23-Apr-06 12:25 sempy/lib/ZstdSharp.dll
--rw-rw-r--  2.0 unx      383 b- defN 24-Mar-21 23:44 sempy/relationships/__init__.py
--rw-rw-r--  2.0 unx    14768 b- defN 24-Mar-21 23:44 sempy/relationships/_find.py
--rw-rw-r--  2.0 unx      278 b- defN 24-Mar-21 23:44 sempy/relationships/_multiplicity.py
--rw-rw-r--  2.0 unx     5548 b- defN 24-Mar-21 23:44 sempy/relationships/_plot.py
--rw-rw-r--  2.0 unx     5665 b- defN 24-Mar-21 23:44 sempy/relationships/_stats.py
--rw-rw-r--  2.0 unx     3209 b- defN 24-Mar-21 23:44 sempy/relationships/_utils.py
--rw-rw-r--  2.0 unx     5934 b- defN 24-Mar-21 23:44 sempy/relationships/_validate.py
--rw-rw-r--  2.0 unx       91 b- defN 24-Mar-21 23:44 sempy/samples/__init__.py
--rw-rw-r--  2.0 unx     4086 b- defN 24-Mar-21 23:44 sempy/samples/_samples.py
--rw-rw-r--  2.0 unx    12690 b- defN 24-Mar-21 23:48 semantic_link_sempy-0.7.2.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx     4972 b- defN 24-Mar-21 23:48 semantic_link_sempy-0.7.2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-Mar-21 23:48 semantic_link_sempy-0.7.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx        6 b- defN 24-Mar-21 23:48 semantic_link_sempy-0.7.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     9358 b- defN 24-Mar-21 23:48 semantic_link_sempy-0.7.2.dist-info/RECORD
-103 files, 9031124 bytes uncompressed, 2952423 bytes compressed:  67.3%
+-rw-rw-r--  2.0 unx      383 b- defN 24-May-14 10:06 sempy/relationships/__init__.py
+-rw-rw-r--  2.0 unx    14768 b- defN 24-May-14 10:06 sempy/relationships/_find.py
+-rw-rw-r--  2.0 unx      278 b- defN 24-May-14 10:06 sempy/relationships/_multiplicity.py
+-rw-rw-r--  2.0 unx     5548 b- defN 24-May-14 10:06 sempy/relationships/_plot.py
+-rw-rw-r--  2.0 unx     5665 b- defN 24-May-14 10:06 sempy/relationships/_stats.py
+-rw-rw-r--  2.0 unx     3209 b- defN 24-May-14 10:06 sempy/relationships/_utils.py
+-rw-rw-r--  2.0 unx     5934 b- defN 24-May-14 10:06 sempy/relationships/_validate.py
+-rw-rw-r--  2.0 unx    12690 b- defN 24-May-14 10:11 semantic_link_sempy-0.7.3.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx     4972 b- defN 24-May-14 10:11 semantic_link_sempy-0.7.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-May-14 10:11 semantic_link_sempy-0.7.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        6 b- defN 24-May-14 10:11 semantic_link_sempy-0.7.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     9196 b- defN 24-May-14 10:11 semantic_link_sempy-0.7.3.dist-info/RECORD
+101 files, 9031825 bytes uncompressed, 2951553 bytes compressed:  67.3%
```

## zipnote {}

```diff
@@ -282,29 +282,23 @@
 
 Filename: sempy/relationships/_utils.py
 Comment: 
 
 Filename: sempy/relationships/_validate.py
 Comment: 
 
-Filename: sempy/samples/__init__.py
+Filename: semantic_link_sempy-0.7.3.dist-info/LICENSE.txt
 Comment: 
 
-Filename: sempy/samples/_samples.py
+Filename: semantic_link_sempy-0.7.3.dist-info/METADATA
 Comment: 
 
-Filename: semantic_link_sempy-0.7.2.dist-info/LICENSE.txt
+Filename: semantic_link_sempy-0.7.3.dist-info/WHEEL
 Comment: 
 
-Filename: semantic_link_sempy-0.7.2.dist-info/METADATA
+Filename: semantic_link_sempy-0.7.3.dist-info/top_level.txt
 Comment: 
 
-Filename: semantic_link_sempy-0.7.2.dist-info/WHEEL
-Comment: 
-
-Filename: semantic_link_sempy-0.7.2.dist-info/top_level.txt
-Comment: 
-
-Filename: semantic_link_sempy-0.7.2.dist-info/RECORD
+Filename: semantic_link_sempy-0.7.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sempy/_version.py

```diff
@@ -4,18 +4,18 @@
 # unpacked source archive. Distribution tarballs contain a pre-generated copy
 # of this file.
 
 import json
 
 version_json = '''
 {
- "date": "2024-03-21T01:43:13+0000",
+ "date": "2024-05-14T09:37:13+0000",
  "dirty": false,
  "error": null,
- "full-revisionid": "2c28e5781d373cdffbf3528e579fce75079b8c9e",
- "version": "0.7.2"
+ "full-revisionid": "736fa20e797ea45532c4f226bcd374f337341865",
+ "version": "0.7.3"
 }
 '''  # END VERSION_JSON
 
 
 def get_versions():
     return json.loads(version_json)
```

## sempy/dotnet.runtime.config.json

### Pretty-printed

 * *Similarity: 0.8333333333333333%*

 * *Differences: {"'runtimeOptions'": "{'rollForward': 'Major'}"}*

```diff
@@ -1,9 +1,10 @@
 {
     "runtimeOptions": {
         "framework": {
             "name": "Microsoft.NETCore.App",
             "version": "6.0.0"
         },
+        "rollForward": "Major",
         "tfm": "net6.0"
     }
 }
```

## sempy/fabric/_environment.py

```diff
@@ -1,13 +1,16 @@
 import os
-from typing import Optional, Any
+from typing import Optional, Dict, Any
 from urllib.parse import quote, urlparse
 
 fs_client: Optional[Any] = None
 environment: Optional[str] = None
+on_fabric: Optional[bool] = None
+on_jupyter: Optional[bool] = None
+jupyter_config: Optional[Dict[str, str]] = None
 
 # FIXME: we need a proper API to get base URL from spark config, which currently doesn't seem to exist
 # the current hack aligns with https://dev.azure.com/powerbi/Embedded/_git/BugbashTool?path=/PowerBIEmbedded/PowerBIEmbedded/App.config&_a=contents&version=GBmaster
 # sovereign clouds are skipped for now.
 SUPPORTED_ENVIRONMENTS = {
     "onebox": "onebox-redirect.analysis.windows-int.net/",
     "daily":  "dailyapi.powerbi.com/",
@@ -97,15 +100,21 @@
     str
         Onelake endpoint.
     """
     return urlparse(_get_trident_config("trident.onelake.endpoint")).netloc
 
 
 def _get_trident_config(key: str) -> str:
-    if _on_fabric():
+    if _on_jupyter():
+        global jupyter_config
+        if jupyter_config is None:
+            from synapse.ml.internal_utils.session_utils import get_fabric_context
+            jupyter_config = get_fabric_context()
+        return jupyter_config.get(key, "")
+    elif _on_fabric():
         from pyspark import SparkContext
         sc = SparkContext.getOrCreate()
         value = sc._jsc.hadoopConfiguration().get(key)
         assert isinstance(value, str)
         return value
     else:
         return "local"
@@ -158,8 +167,18 @@
         if environment not in SUPPORTED_ENVIRONMENTS:
             raise ValueError(f"Unsupported environment '{environment}'. We support {list(SUPPORTED_ENVIRONMENTS.keys())}")
 
     return environment
 
 
 def _on_fabric() -> bool:
-    return 'AZURE_SERVICE' in os.environ
+    global on_fabric
+    if on_fabric is None:
+        on_fabric = "AZURE_SERVICE" in os.environ
+    return on_fabric
+
+
+def _on_jupyter() -> bool:
+    global on_jupyter
+    if on_jupyter is None:
+        on_jupyter = _on_fabric() and "PYTHONUSERBASE" in os.environ
+    return on_jupyter
```

## sempy/fabric/_flat.py

```diff
@@ -71,27 +71,27 @@
     _get_or_create_workspace_client(workspace).refresh_tom_cache()
 
 
 @log
 def get_roles(
     dataset: Union[str, UUID],
     include_members: bool = False,
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     Retrieve all roles associated with the dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
     include_members : bool, default=False
         Whether or not to include members for each role.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `role <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.role?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
@@ -120,25 +120,25 @@
 
     return collection_to_dataframe(collection, extraction_def, additional_xmla_properties)
 
 
 @log
 def get_row_level_security_permissions(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     Retrieve row level security permissions for a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `tablepermission <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.tablepermission?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
@@ -163,53 +163,53 @@
         for table_permission in role.TablePermissions
     ]
 
     return collection_to_dataframe(collection, extraction_def, additional_xmla_properties)
 
 
 @log
-def list_datasets(workspace: Optional[Union[str, UUID]] = None, mode: str = "xmla", additional_xmla_properties: Optional[List[str]] = None) -> pd.DataFrame:
+def list_datasets(workspace: Optional[Union[str, UUID]] = None, mode: str = "xmla", additional_xmla_properties: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:
     """
     List datasets in a `Fabric workspace <https://learn.microsoft.com/en-us/fabric/get-started/workspaces>`_.
 
     Parameters
     ----------
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
     mode : str, default="xmla"
         Whether to use the XMLA "xmla" or REST API "rest".
         See `REST docs <https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasets>`_ for returned fields.
-    additional_xmla_properties : list[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `model <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.model?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
 
     Returns
     -------
     pandas.DataFrame
         Dataframe listing databases and their attributes.
     """
     return _get_or_create_workspace_client(workspace).get_datasets(mode, additional_xmla_properties)
 
 
 @log
 def list_measures(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     Retrieve all measures associated with the given dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `measure <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.measure?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
@@ -436,15 +436,15 @@
 
 @log
 def list_tables(
     dataset: Union[str, UUID],
     include_columns: bool = False,
     include_partitions: bool = False,
     extended: bool = False,
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all tables in a dataset.
 
     Parameters
     ----------
@@ -455,15 +455,15 @@
         Cannot be combined with include_partitions or extended.
     include_partitions : bool, default=False
         Whether or not to include partition level information.
         Cannot be combined with include_columns or extended.
     extended : bool, default False
         Fetches extended table information information.
         Cannot be combined with include_columns or include_partitions.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `table <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.table?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
@@ -526,15 +526,15 @@
     elif include_partitions:
         df = df.explode(["Partition Name", "Partition Refreshed Time"])
     elif extended:
         # Need to use something unique (e.g. SemPyInternalTableName) to avoid throwing a warning
         # as e.g. Name might overlap w/ other tables when resolving metadata
         df_ext = evaluate_dax(dataset,
                               """
-                              SELECT
+                              SELECT DISTINCT
                                   [DIMENSION_NAME]        AS [SemPyInternalTableName],
                                   [DIMENSION_CARDINALITY] AS [SemPyTableRowCount]
                               FROM
                                 $SYSTEM.MDSCHEMA_DIMENSIONS
                               WHERE
                                 [DIMENSION_TYPE] <> 2
                               """,
@@ -549,26 +549,26 @@
 
     return df
 
 
 @log
 def list_translations(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all translations in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
-        Additional XMLA `tramslation <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.translation?view=analysisservices-dotnet>`_
+    additional_xmla_properties : Union[str, List[str]], default=None
+        Additional XMLA `tramslation <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.translation?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
     Returns
@@ -618,26 +618,26 @@
 
     return collection_to_dataframe(collection, extraction_def, additional_xmla_properties)
 
 
 @log
 def list_expressions(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all expressions in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
-        Additional XMLA `expression <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.expression?view=analysisservices-dotnet>`_
+    additional_xmla_properties : Union[str, List[str]], default=None
+        Additional XMLA `expression <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.namedexpression?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
     Returns
@@ -759,16 +759,18 @@
         Verbosity. 0 means no verbosity.
 
     Returns
     -------
     FabricDataFrame
         :class:`~sempy.fabric.FabricDataFrame` holding the result of the DAX query.
     """
-    client: DatasetXmlaClient = _get_or_create_workspace_client(workspace).get_dataset_client(dataset, mode=ConnectionMode.XMLA)  # type: ignore
-    return client.evaluate_dax(dax_string, verbose)
+
+    # creating client directly to avoid any workspace access if not needed
+    # a user can have access via XMLA to a dataset, but may not have access to the workspace
+    return DatasetXmlaClient(workspace, dataset).evaluate_dax(dax_string, verbose)
 
 
 @log
 def execute_xmla(
     dataset: Union[str, UUID],
     xmla_command: str,
     workspace: Optional[Union[str, UUID]] = None
@@ -841,15 +843,15 @@
         Verbosity. 0 means no verbosity.
 
     Returns
     -------
     tuple of FabricDataFrame and pd.DataFrame
         Result of the DAX query as a :class:`~sempy.fabric.FabricDataFrame`, and the corresponding trace logs (without the SessionID column).
     """
-    client: DatasetXmlaClient = _get_or_create_workspace_client(workspace).get_dataset_client(dataset, mode=ConnectionMode.XMLA)  # type: ignore
+    client = DatasetXmlaClient(workspace, dataset)
 
     if clear_cache:
         client._clear_analysis_services_cache()
 
     with TraceConnection(client) as trace_connection:
         event_schema = trace_event_schema if trace_event_schema else Trace.get_default_query_trace_schema()
         adomd_session_id = client.get_adomd_connection().get_or_create_connection().SessionID
@@ -987,14 +989,15 @@
         or if no lakehouse attached, resolves to the workspace of the notebook.
 
     Returns
     -------
     str
         The workspace name.
     """
+
     return _get_or_create_workspace_client(workspace).get_workspace_name()
 
 
 @log
 def create_trace_connection(
     dataset: Union[str, UUID],
     workspace: Optional[Union[str, UUID]] = None
```

## sempy/fabric/_flat_list_annotations.py

```diff
@@ -7,25 +7,25 @@
 
 from typing import List, Optional, Union
 
 
 @log
 def list_annotations(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all annotations in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `annotation <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.annotation?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
```

## sempy/fabric/_flat_list_calculation_items.py

```diff
@@ -5,25 +5,25 @@
 from sempy._utils._log import log
 from typing import List, Optional, Union
 
 
 @log
 def list_calculation_items(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all calculation items for each group in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `calculationitem <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.calculationitem?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
```

## sempy/fabric/_flat_list_columns.py

```diff
@@ -11,29 +11,29 @@
 
 
 @log
 def list_columns(
     dataset: Union[str, UUID],
     table: Optional[str] = None,
     extended: Optional[bool] = False,
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all columns for all tables in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
     table : str, default=None
         Name of the table.
     extended : bool, default=False
         Fetches extended column information.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `column <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.column?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
```

## sempy/fabric/_flat_list_datasources.py

```diff
@@ -8,25 +8,25 @@
 
 from typing import List, Optional, Union
 
 
 @log
 def list_datasources(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all datasources in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `datasource <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.datasource?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
```

## sempy/fabric/_flat_list_hierarchies.py

```diff
@@ -9,27 +9,27 @@
 from typing import List, Optional, Union
 
 
 @log
 def list_hierarchies(
     dataset: Union[str, UUID],
     extended: Optional[bool] = False,
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List hierarchies in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
     extended : bool, default=False
         Fetches extended column information.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `level <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.level?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
         Use Parent to navigate to the parent level.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
```

## sempy/fabric/_flat_list_partitions.py

```diff
@@ -10,29 +10,29 @@
 
 
 @log
 def list_partitions(
     dataset: Union[str, UUID],
     table: Optional[str] = None,
     extended: Optional[bool] = False,
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all partitions in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
     table : str, default=None
         Name of the table.
     extended : bool, default=False
         Fetches extended column information.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `partition <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.partition?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
         Use Parent to navigate to the parent level.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
@@ -120,18 +120,18 @@
 
         df_table_parts = pd.merge(df_parts, df_table, on='SemPyTableID')
 
         # Shows Record Count, Segment Count, Records per Segment
         df_stats = evaluate_dax(dataset,
                                 """
                                 SELECT
-                                    [PartitionStorageID] AS [SemPyPartitionStorageID],
-                                    [RecordCount]        AS [SemPyRecordCount],
-                                    [SegmentCount]       AS [SemPySegmentCount],
-                                    [RecordsPerSegment]  AS [SemPyRecordsPerSegment]
+                                    [PartitionStorageID]                 AS [SemPyPartitionStorageID],
+                                    [RecordCount]                        AS [SemPyRecordCount],
+                                    [SegmentCount]                       AS [SemPySegmentCount],
+                                    [RecordsPerSegment] / [SegmentCount] AS [SemPyRecordsPerSegment]
                                 FROM
                                     $SYSTEM.TMSCHEMA_SEGMENT_MAP_STORAGES
                                 """,
                                 workspace=workspace)
 
         # Used to map Partition Storage IDs to Partition IDs
         df_id_map = evaluate_dax(dataset,
```

## sempy/fabric/_flat_list_perspectives.py

```diff
@@ -7,25 +7,25 @@
 
 from typing import List, Optional, Union
 
 
 @log
 def list_perspectives(
     dataset: Union[str, UUID],
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all perspectives in a dataset.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `perspective <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.perspective?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID.
         Defaults to None which resolves to the workspace of the attached lakehouse
         or if no lakehouse attached, resolves to the workspace of the notebook.
```

## sempy/fabric/_flat_list_relationships.py

```diff
@@ -12,15 +12,15 @@
 
 def _list_relationships_extended(
     df: pd.DataFrame,
     dataset: Union[str, UUID],
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
 
-    df_columns = list_columns(dataset, extended=True)[['Table Name', 'Column Name', 'Column Cardinality']]
+    df_columns = list_columns(dataset, extended=True, workspace=workspace)[['Table Name', 'Column Name', 'Column Cardinality']]
 
     df_columns_from = df_columns.rename(columns={'Column Cardinality': 'Max From Cardinality'})
     df_columns_to = df_columns.rename(columns={'Column Cardinality': 'Max To Cardinality'})
 
     df = (pd.merge(df, df_columns_from, how='left', left_on=['From Table', 'From Column'], right_on=['Table Name', 'Column Name'])
             .drop(['Table Name', 'Column Name'], axis=1))
     df = (pd.merge(df, df_columns_to,   how='left', left_on=['To Table',   'To Column'],   right_on=['Table Name', 'Column Name'])  # noqa: E272
@@ -98,28 +98,28 @@
     return df
 
 
 @log
 def list_relationships(
     dataset: Union[str, UUID],
     extended: Optional[bool] = False,
-    additional_xmla_properties: Optional[List[str]] = None,
+    additional_xmla_properties: Optional[Union[str, List[str]]] = None,
     calculate_missing_rows: Optional[bool] = False,
     workspace: Optional[Union[str, UUID]] = None
 ) -> pd.DataFrame:
     """
     List all relationship found within the Power BI model.
 
     Parameters
     ----------
     dataset : str or uuid.UUID
         Name or UUID of the dataset.
     extended : bool, default=False
         Fetches extended column information.
-    additional_xmla_properties : List[str], default=None
+    additional_xmla_properties : Union[str, List[str]], default=None
         Additional XMLA `relationship <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.relationship?view=analysisservices-dotnet>`_
         properties to include in the returned dataframe.
         Use Parent to navigate to the parent level.
     calculate_missing_rows : bool, default=False
         Calculate the number of missing rows in the relationship.
     workspace : str or uuid.UUID, default=None
         The Fabric workspace name or UUID object containing the workspace ID. Defaults to None
```

## sempy/fabric/_token_provider.py

```diff
@@ -55,16 +55,16 @@
 
         Returns
         -------
         str
             Token acquired from Trident libraries.
         """
         try:
-            from trident_token_library_wrapper import PyTridentTokenLibrary
-            return PyTridentTokenLibrary.get_access_token("pbi")
+            from notebookutils.credentials import getToken
+            return getToken("pbi")
         except ImportError:
             raise RuntimeError("No token_provider specified and unable to obtain token from the environment")
 
 
 def _get_token_expiry_raw_timestamp(token: str) -> int:
     try:
         payload = jwt.decode(token, options={"verify_signature": False})
```

## sempy/fabric/_utils.py

```diff
@@ -3,15 +3,15 @@
 import string
 from collections import defaultdict
 import uuid
 from operator import attrgetter
 
 from sempy.relationships._multiplicity import Multiplicity
 
-from typing import Any, Callable, Dict, Iterable, List, Tuple, Optional, TYPE_CHECKING
+from typing import Any, Callable, Dict, Iterable, List, Tuple, Optional, Union, TYPE_CHECKING
 
 if TYPE_CHECKING:
     from sempy.fabric import FabricDataFrame
 
 
 def _get_relationships(named_dataframes: Dict[str, "FabricDataFrame"]) -> pd.DataFrame:
 
@@ -145,18 +145,21 @@
 def convert_space_delimited_case_to_pascal(col_name: str) -> str:
     """
     Convert Space Delimited Case to PascalCase.
     """
     return col_name.replace(" ", "")
 
 
-def get_properties(obj, properties: Optional[List[str]] = None) -> Dict[str, Any]:
+def get_properties(obj, properties: Optional[Union[str, List[str]]] = None) -> Dict[str, Any]:
     if properties is None:
         return {}
 
+    if isinstance(properties, str):
+        properties = [properties]
+
     result = {}
     for prop in properties:
         # support both pascal and space delimited case
         prop_dotnet = convert_space_delimited_case_to_pascal(prop)
         prop_column = convert_pascal_case_to_space_delimited(prop)
 
         # get the value of the property
@@ -180,15 +183,15 @@
             value = str(value)
 
         result[prop_column] = value
 
     return result
 
 
-def collection_to_dataframe(collection: Iterable, definition: List[Tuple[str, Callable, str]], additional_properties: Optional[List[str]] = None) -> pd.DataFrame:
+def collection_to_dataframe(collection: Iterable, definition: List[Tuple[str, Callable, str]], additional_properties: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:
     """
     Convert a collection of objects to a Pandas DataFrame.
 
     Parameters
     ----------
     collection : Iterable
         The collection to convert.
```

## sempy/fabric/_client/_base_dataset_client.py

```diff
@@ -14,14 +14,149 @@
 
 from typing import Any, Optional, Union, List, Dict, Tuple, TYPE_CHECKING
 
 if TYPE_CHECKING:
     from sempy.fabric._client import WorkspaceClient
 
 
+class WorkspaceDatasetResolver:
+    """
+    Resolves workspace and dataset names to their respective IDs and makes sure to avoid unnecessary API calls.
+    """
+    def __init__(self,
+                 workspace: Union[str, UUID, "WorkspaceClient"],
+                 dataset: Union[str, UUID],
+                 token_provider: TokenProvider):
+
+        self._workspace_input = workspace
+        self._dataset_input = dataset
+        self._token_provider = token_provider
+
+        self._workspace_client = None
+        self._rest_api = None
+
+        self._dataset_name = None
+        self._dataset_id = None
+
+    @property
+    def workspace_client(self):
+        from sempy.fabric._client import WorkspaceClient
+
+        if self._workspace_client is None:
+            if isinstance(self._workspace_input, WorkspaceClient):
+                self._workspace_client = self._workspace_input
+            else:
+                self._workspace_client = WorkspaceClient(self._workspace_input)
+
+        return self._workspace_client
+
+    @property
+    def rest_api(self):
+        if self._rest_api is None:
+            self._rest_api = _PBIRestAPI(token_provider=self._token_provider)
+
+        return self._rest_api
+
+    @property
+    def workspace_name_without_explicit_resolution(self) -> Optional[str]:
+        if isinstance(self._workspace_input, str) and not is_valid_uuid(self._workspace_input):
+            return self._workspace_input
+
+        if self._workspace_client is not None:
+            return self._workspace_client.get_workspace_name()
+
+        return None
+
+    @property
+    def workspace_name(self) -> str:
+        ws = self.workspace_name_without_explicit_resolution
+
+        if ws is None:
+            ws = self.workspace_client.get_workspace_name()
+
+        return ws
+
+    @property
+    def workspace_id_without_explicit_resolution(self) -> Optional[str]:
+        if isinstance(self._workspace_input, UUID):
+            return str(self._workspace_input)
+
+        if self._workspace_client is not None:
+            return self._workspace_client.get_workspace_id()
+
+        return None
+
+    @property
+    def workspace_id(self) -> str:
+        id = self.workspace_id_without_explicit_resolution
+
+        if id is None:
+            id = self.workspace_client.get_workspace_id()
+
+        return id
+
+    @property
+    def dataset_name_without_explicit_resolution(self) -> Optional[str]:
+        if isinstance(self._dataset_input, str) and not is_valid_uuid(self._dataset_input):
+            return self._dataset_input
+
+        return None
+
+    @property
+    def dataset_name(self):
+        if self._dataset_name is None:
+            self._dataset_name = self.dataset_name_without_explicit_resolution
+
+            if self._dataset_name is None:
+                self._resolve_dataset_name_and_id()
+
+        return self._dataset_name
+
+    @property
+    def dataset_id(self):
+        self._resolve_dataset_name_and_id()
+
+        return self._dataset_id
+
+    def _resolve_dataset_name_and_id(self):
+        if self._dataset_name and self._dataset_id:
+            return
+
+        if isinstance(self._dataset_input, UUID):
+            self._dataset_id = str(self._dataset_input)
+            self._dataset_name = self.rest_api.get_dataset_name_from_id(str(self._dataset_input), self.workspace_name)
+        elif isinstance(self._dataset_input, str):
+            # It is possible to use UUID formatted strings as dataset name, so we need to
+            # check first if a name exists before testing for UUID format:
+            try:
+                self._dataset_id = self._get_dataset_id_from_name(self._dataset_input, self.workspace_name)
+                self._dataset_name = self._dataset_input
+            except DatasetNotFoundException:
+                if is_valid_uuid(self._dataset_input):
+                    self._dataset_id = self._dataset_input
+
+                    self._dataset_name = self.rest_api.get_dataset_name_from_id(self._dataset_input, self.workspace_name)
+                else:
+                    raise
+        else:
+            raise TypeError(f"Unexpected type {type(self._dataset_input)} for \"dataset\"")
+
+    def _get_dataset_id_from_name(self, dataset_name: str, workspace_name: str) -> str:
+        workspace_id = self.workspace_id
+        if workspace_id is None:
+            raise WorkspaceNotFoundException(workspace_name)
+        datasets = self.rest_api.get_workspace_datasets(workspace_name, str(workspace_id))
+
+        for item in datasets:
+            if item["name"] == dataset_name:
+                return item["id"]
+
+        raise DatasetNotFoundException(dataset_name, str(workspace_name))
+
+
 class BaseDatasetClient():
     """
     Client for access to Power BI data in a specific dataset (database).
 
     Each client will usually map to a Dataset (Database) i.e. one or more clients can be instantiated
     within each accessed workspace.
 
@@ -34,59 +169,21 @@
     token_provider : TokenProvider, default=None
         Implementation of TokenProvider that can provide auth token
         for access to the PowerBI workspace. Will attempt to acquire token
         from its execution environment if not provided.
     """
     def __init__(
             self,
-            workspace: Union[str, "WorkspaceClient"],
+            workspace: Union[str, UUID, "WorkspaceClient"],
             dataset: Union[str, UUID],
             token_provider: Optional[TokenProvider] = None
     ):
-        from sempy.fabric._client import WorkspaceClient
         self.token_provider = token_provider or SynapseTokenProvider()
 
-        self._workspace_client: WorkspaceClient
-        if isinstance(workspace, WorkspaceClient):
-            self._workspace_client = workspace
-        else:
-            self._workspace_client = WorkspaceClient(workspace, self.token_provider)
-
-        self._rest_api = _PBIRestAPI(token_provider=self.token_provider)
-
-        workspace_name = self._workspace_client.get_workspace_name()
-
-        if isinstance(dataset, UUID):
-            self._dataset_id = str(dataset)
-            self._dataset_name = self._rest_api.get_dataset_name_from_id(str(dataset), workspace_name)
-        elif isinstance(dataset, str):
-            # It is possible to use UUID formatted strings as dataset name, so we need to
-            # check first if a name exists before testing for UUID format:
-            try:
-                self._dataset_id = self._get_dataset_id_from_name(dataset, workspace_name)
-                self._dataset_name = dataset
-            except DatasetNotFoundException:
-                if is_valid_uuid(dataset):
-                    self._dataset_id = dataset
-                    self._dataset_name = self._rest_api.get_dataset_name_from_id(dataset, workspace_name)
-                else:
-                    raise
-        else:
-            raise TypeError(f"Unexpected type {type(dataset)} for \"dataset\"")
-
-    def _get_dataset_id_from_name(self, dataset_name: str, workspace_name: str) -> str:
-        workspace_id = self._workspace_client.get_workspace_id_from_name(workspace_name)
-        if workspace_id is None:
-            raise WorkspaceNotFoundException(workspace_name)
-        datasets = self._rest_api.get_workspace_datasets(workspace_name, str(workspace_id))
-
-        for item in datasets:
-            if item["name"] == dataset_name:
-                return item["id"]
-        raise DatasetNotFoundException(dataset_name, str(workspace_name))
+        self.resolver = WorkspaceDatasetResolver(workspace, dataset, self.token_provider)
 
     def evaluate_dax(self, query: str, verbose: int = 0) -> FabricDataFrame:
         """
         Retrieve results of DAX query as a FabricDataFrame.
 
         Parameters
         ----------
@@ -97,15 +194,15 @@
 
         Returns
         -------
         FabricDataFrame
             FabricDataFrame converted from the results of a DAX query.
         """
         df = self._evaluate_dax(query, verbose)
-        return FabricDataFrame(df, dataset=self._dataset_name, workspace=self._workspace_client.get_workspace_name())
+        return FabricDataFrame(df, dataset=self.resolver.dataset_name, workspace=self.resolver.workspace_name)
 
     @abstractmethod
     def _evaluate_dax(self, query: str, verbose: int = 0) -> pd.DataFrame:
         """
         Retrieve results of DAX query as a pandas DataFrame.
 
         Parameters
@@ -258,15 +355,15 @@
             Verbosity. 0 means no verbosity.
 
         Returns
         -------
         FabricDataFrame
             DataFrame with metadata from the PBI model.
         """
-        database = self._workspace_client.get_dataset(self._dataset_name)
+        database = self.resolver.workspace_client.get_dataset(self.resolver.dataset_name)
         for table in database.Model.Tables:
             if table.Name == table_name:
                 pandas_df = self._get_pandas_table(table.Name, num_rows, verbose)
 
                 if not fully_qualified_columns:
                     pandas_df = self._simplify_col_names(pandas_df)
 
@@ -275,15 +372,15 @@
                     for relationship in database.Model.Relationships:
                         if relationship.FromTable.Name == table.Name:
                             self._populate_relationship_meta(relationship, meta_df, exclude_internal)
                 if multiindex_hierarchies:
                     meta_df = self._convert_hierarchies(meta_df, table, fully_qualified_columns)
                 return meta_df
 
-        raise ValueError(f"'{table_name}' is not a valid table in Dataset '{self._dataset_name}'")
+        raise ValueError(f"'{table_name}' is not a valid table in Dataset '{self.resolver.dataset_name}'")
 
     def resolve_metadata(self, columns: List[str], verbose: int = 0) -> Dict[str, Any]:
         """
         Resolve column names to their Power BI metadata.
 
         Parameters
         ----------
@@ -296,15 +393,15 @@
             Verbosity. 0 means no verbosity.
 
         Returns
         -------
         Dict of str
             Dictionary containing mapping of column name to its metadata.
         """
-        database = self._workspace_client.get_dataset(self._dataset_name)
+        database = self.resolver.workspace_client.get_dataset(self.resolver.dataset_name)
 
         column_map = defaultdict(lambda: [])
         for table in database.Model.Tables:
             for column in table.Columns:
                 column_data = self._get_column_data(table, column)
 
                 # Any syntax for column names valid for DAX is valid here
@@ -332,17 +429,17 @@
             if num_column_matches == 1:
                 if verbose > 0:
                     print(f"Column '{column}' matched to '{column_data_list[0]}'")
 
                 column_metadata[column] = column_data_list[0]
             elif num_column_matches == 0:
                 if verbose > 0:
-                    print(f"Column '{column}' not found in dataset '{self._dataset_name}'")
+                    print(f"Column '{column}' not found in dataset '{self.resolver.dataset_name}'")
             else:
-                warnings.warn(f"Ambiguous column name '{column}' found in dataset '{self._dataset_name}': '{column_data_list}'")
+                warnings.warn(f"Ambiguous column name '{column}' found in dataset '{self.resolver.dataset_name}': '{column_data_list}'")
 
         return column_metadata
 
     def _get_pandas_table(self, table_name, num_rows, verbose):
         if num_rows is None:
             dax_query = f"EVALUATE '{table_name}'"
         else:
@@ -401,20 +498,26 @@
         column_data = {
             # TODO: should we put these into an enum?
             MetadataKeys.TABLE: table.Name,
             MetadataKeys.COLUMN: column.Name
         }
 
         # table level
-        if self._dataset_name:
-            column_data[MetadataKeys.DATASET] = self._dataset_name
-        if self._workspace_client._workspace_id:
-            column_data[MetadataKeys.WORKSPACE_ID] = self._workspace_client._workspace_id
-        if self._workspace_client._workspace_name:
-            column_data[MetadataKeys.WORKSPACE_NAME] = self._workspace_client._workspace_name
+        # make sure we don't do service calls to resolve the metadata
+        dataset_name = self.resolver.dataset_name_without_explicit_resolution
+        workspace_id = self.resolver.workspace_id_without_explicit_resolution
+        workspace_name = self.resolver.workspace_name_without_explicit_resolution
+
+        if dataset_name:
+            column_data[MetadataKeys.DATASET] = dataset_name
+        if workspace_id:
+            column_data[MetadataKeys.WORKSPACE_ID] = workspace_id
+        if workspace_name:
+            column_data[MetadataKeys.WORKSPACE_NAME] = workspace_name
+
         if table.Annotations:
             # Excluding keys that are present in all tables or are not valuable to the user to reduce clutter. Ex:
             # {'LinkedQueryName': 'Sales'}
             table_annotations = self._extract_annotations(table, exclude_keys=["LinkedQueryName"])
             if len(table_annotations) > 0:
                 column_data[MetadataKeys.TABLE_ANNOTATIONS] = table_annotations
 
@@ -492,15 +595,15 @@
                 col_meta[MetadataKeys.RELATIONSHIP] = {
                     "to_table": to_table,
                     "to_column": to_column,
                     "multiplicity": to_multiplicity(relationship)
                 }
 
     def _add_column_metadata_from_tuples(self, df: Union[pd.DataFrame, FabricDataFrame], column_tuples: List[Tuple[str, str]]) -> FabricDataFrame:
-        database = self._workspace_client.get_dataset(self._dataset_name)
+        database = self.resolver.workspace_client.get_dataset(self.resolver.dataset_name)
         tables = {table.Name: table for table in database.Model.Tables}
 
         if isinstance(df, FabricDataFrame):
             meta_df = df
         else:
             meta_df = FabricDataFrame(df)
 
@@ -523,19 +626,19 @@
                 col_names.append(match.group(1))
             else:
                 col_names.append(col)
         df.columns = col_names
         return df
 
     def __repr__(self):
-        return f"PowerBIClient('{self._workspace_client.get_workspace_name()}[{self._dataset_name}]')"
+        return f"PowerBIClient('{self.resolver.workspace_name}[{self.resolver.dataset_name}]')"
 
     def _show_relationship(self, relationship, exclude_internal):
         if exclude_internal:
-            if self._workspace_client._is_internal(relationship.FromTable) or self._workspace_client._is_internal(relationship.ToTable):
+            if self.resolver.workspace_client._is_internal(relationship.FromTable) or self.resolver.workspace_client._is_internal(relationship.ToTable):
                 return False
         return True
 
 
 def _parse_column_reference(column_spec: str) -> Tuple[Optional[str], str]:
     # Simplistic parsing that does not yet take into account escaping of
     # square brackets and assumes that single quotes have to be present on both sides.
```

## sempy/fabric/_client/_dataset_onelake_import.py

```diff
@@ -21,18 +21,20 @@
         BaseDatasetClient.__init__(self, workspace, dataset, token_provider)
 
     def _get_pandas_table(self, table_name, num_rows, verbose):
         from pyspark.sql import SparkSession
 
         spark = SparkSession.builder.getOrCreate()
 
-        workspace_id = self._workspace_client.get_workspace_id()
+        workspace_id = self.resolver.workspace_id
+        dataset_id = self.resolver.dataset_id
+
         host = _get_onelake_endpoint()
 
-        url = f"abfss://{workspace_id}@{host}/{self._dataset_id}/Tables/{table_name}"
+        url = f"abfss://{workspace_id}@{host}/{dataset_id}/Tables/{table_name}"
 
         # the Spark config is only relevant at toPandas(), but to be avoid any future mistakes, let's wrap the whole.
         with SparkConfigTemporarily(spark, "spark.sql.parquet.datetimeRebaseModeInRead", "CORRECTED"):
             df = spark.read.format("delta").load(url)
 
             if num_rows is not None:
                 df = df.limit(num_rows)
```

## sempy/fabric/_client/_dataset_rest_client.py

```diff
@@ -88,33 +88,33 @@
             commit_mode: str = "transactional",
             retry_count: int = 1,
             objects: Optional[List] = None,
             apply_refresh_policy: bool = True,
             effective_date: datetime.date = datetime.date.today(),
             verbose: int = 0
     ) -> str:
-        workspace_id = self._workspace_client.get_workspace_id()
-        workspace_name = self._workspace_client.get_workspace_name()
-        dataset_id = self._dataset_id
-        poll_url = self._rest_api.refresh_post(dataset_id, workspace_id, workspace_name,
-                                               refresh_type, max_parallelism, commit_mode,
-                                               retry_count, objects, apply_refresh_policy, effective_date,
-                                               verbose)
+        workspace_id = self.resolver.workspace_id
+        workspace_name = self.resolver.workspace_name
+        dataset_id = self.resolver.dataset_id
+        poll_url = self.resolver.rest_api.refresh_post(dataset_id, workspace_id, workspace_name,
+                                                       refresh_type, max_parallelism, commit_mode,
+                                                       retry_count, objects, apply_refresh_policy, effective_date,
+                                                       verbose)
         return poll_url
 
     def get_refresh_execution_details(
             self,
             refresh_request_id: Union[str, UUID]
     ) -> RefreshExecutionDetails:
         # see https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-refresh-execution-details
-        payload = self._rest_api.get_refresh_execution_details(
-            self._dataset_id,
+        payload = self.resolver.rest_api.get_refresh_execution_details(
+            self.resolver.dataset_id,
             str(refresh_request_id),
-            self._workspace_client.get_workspace_id(),
-            self._workspace_client.get_workspace_name()
+            self.resolver.workspace_id,
+            self.resolver.workspace_name
         )
 
         objects = rename_and_validate_from_records(
             payload.get("objects"),
             [
                 ("table",     "Table",     "str"),
                 ("partition", "Partition", "str?"),
@@ -154,18 +154,18 @@
             refresh_attempts    =refresh_attempts                                     # noqa: E251, E221
         )
 
     def list_refresh_history(
             self,
             top_n: Optional[int] = None
     ) -> pd.DataFrame:
-        workspace_id = self._workspace_client.get_workspace_id()
-        workspace_name = self._workspace_client.get_workspace_name()
-        dataset_id = self._dataset_id
-        payload = self._rest_api.list_refresh_history(dataset_id, workspace_id, workspace_name, top_n)
+        workspace_id = self.resolver.workspace_id
+        workspace_name = self.resolver.workspace_name
+        dataset_id = self.resolver.dataset_id
+        payload = self.resolver.rest_api.list_refresh_history(dataset_id, workspace_id, workspace_name, top_n)
 
         return rename_and_validate_from_records(payload, [
             ("id",                   "Id",                     "str"),
             ("requestId",            "Request Id",             "str"),
             ("startTime",            "Start Time",             "datetime64[ns]"),
             ("endTime",              "End Time",               "datetime64[ns]?"),
             ("refreshType",          "Refresh Type",           "str"),
@@ -173,15 +173,15 @@
             ("status",               "Status",                 "str"),
             # missing from public docs
             ("extendedStatus",       "Extended Status",        "str?"),
             ("refreshAttempts",      "Refresh Attempts",       "str?"),
             ])
 
     def _evaluate_dax(self, query: str, verbose: int = 0) -> pd.DataFrame:
-        rows = self._rest_api.execute_dax_query(self._dataset_id, query)
+        rows = self.resolver.rest_api.execute_dax_query(self.resolver.dataset_id, query)
         return pd.DataFrame(rows)
 
     def _evaluate_measure(
         self,
         measure: Union[str, List[str]],
         groupby_columns: List[Tuple[str, str]],
         filters: Dict[Tuple[str, str], List[str]],
@@ -198,17 +198,17 @@
         filter_obj: List[Dict[str, list]] = []
         for table_col, filter_lst in filters.items():
             target = [{"table": table_col[0], "column": table_col[1]}]
             # REST API requires the "in" parameter to have every object as its own list
             filter_in = [[obj] for obj in filter_lst]
             filter_obj.append({"target": target, "in": filter_in})
 
-        columns, rows = self._rest_api.calculate_measure(self._dataset_id, measure_obj, groupby_columns_obj, filter_obj, num_rows, verbose)
+        columns, rows = self.resolver.rest_api.calculate_measure(self.resolver.dataset_id, measure_obj, groupby_columns_obj, filter_obj, num_rows, verbose)
         if not columns:
             col_names = [f"{g[0]}[{g[1]}]" for g in groupby_columns]
             col_names.extend(measure)
             return pd.DataFrame({}, columns=col_names)
         else:
             return self._format_measure_df(columns, rows, measure)
 
     def __repr__(self) -> str:
-        return f"DatasetRestClient('{self._workspace_client.get_workspace_name()}[{self._dataset_name}]')"
+        return f"DatasetRestClient('{self.resolver.workspace_name}[{self.resolver.dataset_name}]')"
```

## sempy/fabric/_client/_dataset_xmla_client.py

```diff
@@ -1,17 +1,17 @@
 import os
 import tempfile
 import pandas as pd
 from uuid import UUID, uuid4
 import warnings
 
-from sempy.fabric._client._utils import _init_analysis_services, _create_tom_server, _build_adomd_connection_string
 from sempy.fabric._client._base_dataset_client import BaseDatasetClient
+from sempy.fabric._client._utils import _init_analysis_services, _create_tom_server, _build_adomd_connection_string
 from sempy.fabric._client._adomd_connection import AdomdConnection
-from sempy.fabric._environment import _get_workspace_url
+from sempy.fabric._environment import _get_workspace_url, get_workspace_id
 from sempy.fabric._token_provider import TokenProvider
 from sempy.fabric._utils import clr_to_pandas_dtype
 
 from sempy._utils._log import log_xmla, log
 
 from typing import Optional, Union, TYPE_CHECKING, List, Tuple, Dict
 
@@ -41,33 +41,36 @@
     token_provider : TokenProvider, default=None
         Implementation of TokenProvider that can provide auth token
         for access to the PowerBI workspace. Will attempt to acquire token
         from its execution environment if not provided.
     """
     def __init__(
             self,
-            workspace: Union[str, "WorkspaceClient"],
+            workspace: Union[str, UUID, "WorkspaceClient", None],
             dataset: Union[str, UUID],
             token_provider: Optional[TokenProvider] = None
     ):
         _init_analysis_services()
 
+        if workspace is None:
+            workspace = get_workspace_id()
+
         BaseDatasetClient.__init__(self, workspace, dataset, token_provider)
 
         self.adomd_connection: Optional[AdomdConnection] = None
 
     def _get_connected_dataset_server(self, readonly: bool = True):
         connection_string = self._get_dax_connection_string(readonly)
         tom_server = _create_tom_server(connection_string, self.token_provider)
 
         return tom_server
 
     def _get_dax_connection_string(self, readonly: bool = True) -> str:
-        workspace_url = _get_workspace_url(self._workspace_client.get_workspace_name())
-        return _build_adomd_connection_string(workspace_url, self._dataset_name, readonly)
+        workspace_url = _get_workspace_url(self.resolver.workspace_name)
+        return _build_adomd_connection_string(workspace_url, self.resolver.dataset_name, readonly)
 
     def _evaluate_dax(self, query: str, verbose: int = 0, batch_size: int = 100000) -> pd.DataFrame:
         return self._get_DAX(dax_string=query, batch_size=batch_size, verbose=verbose)
 
     def _evaluate_measure(
         self,
         measure: Union[str, List[str]],
@@ -215,15 +218,15 @@
                 if command is not None:
                     command.Dispose()
 
     def _clear_analysis_services_cache(self) -> None:
         xmla_command = f"""
             <ClearCache xmlns="http://schemas.microsoft.com/analysisservices/2003/engine">
                 <Object>
-                    <DatabaseID>{self._dataset_id}</DatabaseID>
+                    <DatabaseID>{self.resolver.dataset_id}</DatabaseID>
                 </Object>
             </ClearCache>
         """
         rows = self._execute_xmla(xmla_command)
         if rows != 1:
             warnings.warn("Failed to clear cache.")
 
@@ -268,8 +271,8 @@
 
         if len(conversion_map) > 0:
             return df.astype(conversion_map)
         else:
             return df
 
     def __repr__(self):
-        return f"DatasetXmlaClient('{self._workspace_client.get_workspace_name()}[{self._dataset_name}]')"
+        return f"DatasetXmlaClient('{self.resolver.workspace_name}[{self.resolver.dataset_name}]')"
```

## sempy/fabric/_client/_workspace_client.py

```diff
@@ -161,15 +161,15 @@
 
         # ?readonly enables connections to read-only replicas (see https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-scale-out-app)
         workspace_url = _get_workspace_url(self.get_workspace_name())
         connection_str = _build_adomd_connection_string(workspace_url, readonly=readonly)
 
         return _create_tom_server(connection_str, self.token_provider)
 
-    def get_datasets(self, mode: str, additional_xmla_properties: Optional[List[str]] = None) -> pd.DataFrame:
+    def get_datasets(self, mode: str, additional_xmla_properties: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:
         """
         Get a list of datasets in a PowerBI workspace.
 
         Each dataset is derived from
         `Microsoft.AnalysisServices.Tabular.Database <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.database?view=analysisservices-dotnet>`__
 
         The dataframe contains the following columns:
@@ -258,15 +258,15 @@
         -------
         Dataset
             PowerBI Dataset represented as TOM Database object.
         """
         client = self.get_dataset_client(dataset)
 
         for db in self._get_readonly_tom_server().Databases:
-            if db.Name == client._dataset_name:
+            if db.Name == client.resolver.dataset_name:
                 return db
 
         # Executing the following is very unlikely, because an exception should have
         # occured during dataset resolution. The only conceivable way is if the dataset
         # got deleted before we retrieved the list with self.get_connection().Databases.
         raise DatasetNotFoundException(str(dataset), self.get_workspace_name())
 
@@ -352,36 +352,36 @@
         if self.tom_server_readonly is not None:
             # cleanup (True = drop session)
             self.tom_server_readonly.Disconnect(True)
             self.tom_server_readonly.Dispose()
 
         self.tom_server_readonly = None
 
-    def list_measures(self, dataset: Union[str, UUID], additional_xmla_properties: Optional[List[str]] = None) -> pd.DataFrame:
+    def list_measures(self, dataset: Union[str, UUID], additional_xmla_properties: Optional[Union[str, List[str]]] = None) -> pd.DataFrame:
         """
         Retrieve all measures associated with the given dataset.
 
         Each measure is derived from
         `Microsoft.AnalysisServices.Tabular.Measure <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.measure?view=analysisservices-dotnet>`__
 
         Parameters
         ----------
         dataset : str or UUID
             Name or UUID of the dataset to list the measures for.
-        additional_xmla_properties : List[str], default=None
+        additional_xmla_properties : Union[str, List[str]], default=None
             Additional XMLA `measure <https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.measure?view=analysisservices-dotnet>`_
             properties to include in the returned dataframe.
 
         Returns
         -------
         DataFrame
             Pandas DataFrame listing measures and their attributes.
         """
         client = self.get_dataset_client(dataset)
-        database = self.get_dataset(client._dataset_name)
+        database = self.get_dataset(client.resolver.dataset_name)
 
         # see https://learn.microsoft.com/en-us/dotnet/api/microsoft.analysisservices.tabular.measure?view=analysisservices-dotnet
         # (table, measure)
         extraction_def = [
             ("Table Name",               lambda r: r[0].Name,                   "str"),   # noqa: E272
             ("Measure Name",             lambda r: r[1].Name,                   "str"),   # noqa: E272
             ("Measure Expression",       lambda r: r[1].Expression,             "str"),   # noqa: E272
```

## sempy/fabric/_dataframe/_fabric_dataframe.py

```diff
@@ -1,13 +1,14 @@
 import pandas as pd
 from uuid import UUID
 from numpy import ndarray
 from typing import Any, Callable, List, Iterable, Dict, Union, Optional, Tuple, TYPE_CHECKING
 from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, BooleanType, BinaryType, TimestampType
 
+from sempy.fabric.exceptions import WorkspaceNotFoundException
 from sempy.fabric._utils import SparkConfigTemporarily
 from sempy.dependencies._find import _find_dependencies_with_stats
 from sempy.dependencies._stats import DataFrameDependencyStats
 from sempy.functions import _SDataFrame
 from sempy.dependencies._validate import (
     _drop_dependency_violations,
     _list_dependency_violations,
@@ -76,15 +77,15 @@
             from sempy.fabric._cache import _get_or_create_workspace_client
             from Microsoft.AnalysisServices import OperationException
 
             try:
                 self.column_metadata = _get_or_create_workspace_client(workspace) \
                     .get_dataset_client(dataset) \
                     .resolve_metadata(self.columns, verbose)
-            except OperationException as e:
+            except (WorkspaceNotFoundException, OperationException) as e:
                 if verbose > 0:
                     print(f"Warning: failed to resolve column metadata: {e}")
 
                 self.column_metadata = {}
 
     @property
     def _constructor_sliced(self) -> Callable:
@@ -481,27 +482,33 @@
         FabricDataFrame
             New dataframe with constraint determinant -> dependent enforced.
         """
 
         return _drop_dependency_violations(self, determinant_col, dependent_col, verbose=verbose)
 
     @log
-    def to_lakehouse_table(self, name: str, mode: Optional[str] = "error", spark_schema: Optional[StructType] = None):
+    def to_lakehouse_table(self,
+                           name: str,
+                           mode: Optional[str] = "error",
+                           spark_schema: Optional[StructType] = None,
+                           delta_column_mapping_mode: str = "name") -> None:
         """
         Write the data to OneLake as a Delta table with VOrdering enabled.
 
         Parameters
         ----------
         name : str
             The name of the table to write to.
         mode : str, default="error"
             Specifies the behavior when table already exists, by default "error".
             Details of the modes are available in the `Spark docs <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html>`_.
         spark_schema : pyspark.sql.types.StructType, default=None
             Specifies the schema of spark table to which the dataframe will be written in the lakehouse. If not provided, will be auto-generated via _pandas_to_spark_schema function.
+        delta_column_mapping_mode : str, default="name"
+            Specifies the `column mapping mode <https://docs.delta.io/latest/delta-column-mapping.html>`_ to be used for the delta table. By default, it is set to "name".
         """
         from pyspark.sql import SparkSession
 
         spark = SparkSession.builder.getOrCreate()
 
         df_converted = self
 
@@ -546,21 +553,24 @@
         # "spark.sql.parquet.int96RebaseModeInWrite" is not a static configuration, so it can be modified
         # during the session. Static configurations (such as delta extensions) have to be modified before
         # session is created. Fabric does not set this option as of November 2023:
 
         with SparkConfigTemporarily(spark, "spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED"):
             spark_df = spark.createDataFrame(df_converted, schema=converted_schema)
 
-            (spark_df.write
-                .option("parquet.vorder.enabled", True)
-                .mode(mode)
-                .format("delta")
-                # enable column mapping to support special characters common w/ Power BI (e.g. [])
-                .option("delta.columnMapping.mode", "name")
-                .saveAsTable(name))
+            write_op = (spark_df.write
+                                .option("parquet.vorder.enabled", True)
+                                .mode(mode)
+                                .format("delta"))
+
+            if delta_column_mapping_mode is not None:
+                # if 'name' is enable column mapping to support special characters common w/ Power BI (e.g. [])
+                write_op = write_op.option("delta.columnMapping.mode", delta_column_mapping_mode)
+
+            write_op.saveAsTable(name)
 
     # repeating functions to make sure they show up in the docs
 
     @log
     def to_parquet(self, path: str, *args, **kwargs) -> None:
         """
         Write DataFrame to a parquet file specified by path parameter using `Arrow <https://arrow.apache.org/docs/python/index.html>`_ including metadata.
```

## sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll

### pedump {}

```diff
@@ -1,12 +1,12 @@
 
 COFF Header:
 	                Machine: 0x8664
 	               Sections: 0x0002
-	             Time stamp: 0xb950e091
+	             Time stamp: 0x8783d5d5
 	Pointer to Symbol Table: 0x00000000
 	   	   Symbol Count: 0x00000000
 	   Optional Header Size: 0x00f0
 	   	Characteristics: 0x2022
 
 PE Header:
 	         Magic (0x010b): 0x020b
```

## sempy/relationships/_find.py

```diff
@@ -81,26 +81,26 @@
         else:
             return Multiplicity.MANY_TO_MANY
 
 
 def _relationship_tuples_to_pandas(tuples: List[Any]):
     return pd.DataFrame(tuples, columns=[
         'Multiplicity',
-        'From Table',
-        'From Column',
         'To Table',
         'To Column',
-        'Coverage From',
+        'From Table',
+        'From Column',
         'Coverage To',
-        'Null Count From',
+        'Coverage From',
         'Null Count To',
-        'Unique Count From',
+        'Null Count From',
         'Unique Count To',
-        'Row Count From',
-        'Row Count To'
+        'Unique Count From',
+        'Row Count To',
+        'Row Count From'
     ])
 
 
 def _convert_rels_to_pandas(relationships: List, stats: Dict):
     enriched_tuples = []
     for stype_from, column_from, stype_to, column_to, coverage_from, coverage_to in relationships:
         stats_from = stats[stype_from][column_from]
```

## Comparing `semantic_link_sempy-0.7.2.dist-info/LICENSE.txt` & `semantic_link_sempy-0.7.3.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `semantic_link_sempy-0.7.2.dist-info/METADATA` & `semantic_link_sempy-0.7.3.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: semantic-link-sempy
-Version: 0.7.2
+Version: 0.7.3
 Summary: Semantic link for Microsoft Fabric
 Home-page: https://learn.microsoft.com/en-us/fabric/data-science/semantic-link-overview
 Author: Microsoft
 Author-email: semanticdatascience@service.microsoft.com
 License: proprietary and confidential
 Platform: Microsoft Fabric
 Classifier: Development Status :: 4 - Beta
```

## Comparing `semantic_link_sempy-0.7.2.dist-info/RECORD` & `semantic_link_sempy-0.7.3.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 sempy/__init__.py,sha256=dWTQUV0PwmqLhH16VILNwlVOe4rS9ualRDHImePARXY,1097
-sempy/_version.py,sha256=8PqC0zKSt-NPfSYs9KN-BgfgnoIiOpQy5TwZcW7Qiyg,497
-sempy/dotnet.runtime.config.json,sha256=aBp_ndPstjfi8RmMnL2dDhtCmUwhHGTI_-L7o_LARCs,140
+sempy/_version.py,sha256=R-Yrg1Zv2hqwf4mJjODC7n8LvgDsqMVOX0jCf71W05M,497
+sempy/dotnet.runtime.config.json,sha256=syhDFQv6cEmZnE1WtFjNe3NwhsIsnd-CFULv-vEWOFI,167
 sempy/_metadata/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sempy/_metadata/_mdataframe.py,sha256=g2__5z9lbykHxEqfnUPrzK6ONmYXtRt-L3NSM5zlq5s,11141
 sempy/_metadata/_meta_utils.py,sha256=vyibpCuB-yhmW0pvjf13LoTs9F7g2AoxbmAyNmOsHmQ,828
 sempy/_metadata/_mseries.py,sha256=m_QHccOylCAgwEGdaDVTun-6CA7dZ8TYwS3o6vXqC2E,3017
 sempy/_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sempy/_utils/_log.py,sha256=-BCfTW4Xv8uVmM75h_HItEyCZlmxZ6jzJMnqQ_ZBkEw,17194
 sempy/_utils/_ordered_set.py,sha256=y_SiXpSdtviZxgIJjSxnLxGg_2qs-yCn_YX-0EAi8nI,1180
@@ -14,46 +14,46 @@
 sempy/dependencies/_plot.py,sha256=-hbsCVhAXg9yepXi6mUfhqBVQG06J_4ZNSFQdcqMCFg,3897
 sempy/dependencies/_stats.py,sha256=820yzmpQuE3d8pPVL0oJsZmgiplba9ta3grsThDX51I,6023
 sempy/dependencies/_validate.py,sha256=e0vhuxldnSPMUhArwdtDjNwItv6KMQxR79Ci4ShOg3A,4181
 sempy/fabric/__init__.py,sha256=8jCjJmp5VUSI44DqtEXtr4LPCv7XxZYZQiN4krGyVjs,3543
 sempy/fabric/_cache.py,sha256=Ta-qFzAYxhUg_YBlQrZUgcYJn2qdNIYP1y3q1VX7fkA,959
 sempy/fabric/_datacategory.py,sha256=3KOnC5swyCcTz2auPOiATeqPc6Kf5HGCkVDvdJ44iR0,495
 sempy/fabric/_daxmagics.py,sha256=w2uhNgHwhJq_DFnHJQLvzW8x8pv20kVGNnSEa7Tpd_E,1520
-sempy/fabric/_environment.py,sha256=YkLFpWghQToJcDav3B6d7o-jAZyQaYuzhA69RGGUnMw,4314
-sempy/fabric/_flat.py,sha256=UABhGbBWnIUCauDriRguEfknjfCYQTg5uVIgs1hYq0g,56394
-sempy/fabric/_flat_list_annotations.py,sha256=fL6EjaehTtbHgQaAsf1BO8dI-nu9sbRVHElqmW43J0E,3703
+sempy/fabric/_environment.py,sha256=BpXC7lUN8U5xFLO0MK2it3mwGlA2qLtfIJFY60aAcus,4946
+sempy/fabric/_flat.py,sha256=g7mm7meVIZ_pW_B4ImcSkxcmMLnDQhayU8pRjJly28Y,56530
+sempy/fabric/_flat_list_annotations.py,sha256=tn9MDeEQh104mDV7PG_EeLOFTqDS0KvFh7C0dfmSB9Y,3727
 sempy/fabric/_flat_list_apps.py,sha256=p_lj3-zt2Fn8egCwCBcPajbMkH_FfRadtWlg-WF__po,992
-sempy/fabric/_flat_list_calculation_items.py,sha256=TGDlHY_lhJuUzLnX3fZNZwmZl0wXmu0qheVGeMdgRrM,2908
-sempy/fabric/_flat_list_columns.py,sha256=ZiC6SHwcwD53jZD-vdmwQjzzAB6TuhEh1hmbEbe-urk,14653
+sempy/fabric/_flat_list_calculation_items.py,sha256=zNL2z57NxBkm1qmU749WYTyQmhyE50klaIoTK4LX6nQ,2932
+sempy/fabric/_flat_list_columns.py,sha256=O9D5h48oxpUUSKG1Hv2JBKCby-LhfRBBZK-nWoSigJM,14677
 sempy/fabric/_flat_list_dataflows.py,sha256=5024cfPday5GE-jcdPiEIDGyiVVT1Tv9mSGqw1yO27E,835
-sempy/fabric/_flat_list_datasources.py,sha256=9gQgOM1MB4JxFii5YbL6_uyS9Y22GyofXmtTzH2Z58A,3086
+sempy/fabric/_flat_list_datasources.py,sha256=JM-Nw_kFbHSpNSuMkHB4gyFQmCA9e-zMUZijx9vxoYo,3110
 sempy/fabric/_flat_list_gateways.py,sha256=VJm6wr4Cq_UAwQU7gReGuPILqaNfHg9ICSQMbCUGs2M,765
-sempy/fabric/_flat_list_hierarchies.py,sha256=JYghqXCQB937ERvDo3biZZ2jdDHvGEf0XJOw_zvqWCs,5048
-sempy/fabric/_flat_list_partitions.py,sha256=K4_GGxl5FQLUqyzMrGZElKKEaz4lr8B26IWw4LCroX4,7609
-sempy/fabric/_flat_list_perspectives.py,sha256=lWi9SC5rhGLR3LCcsuX7Ofesu4BUWSm-mBCzZrZokfc,2194
-sempy/fabric/_flat_list_relationships.py,sha256=qKDfPN1EZGah1G3nnQ2m9ciiWAbavliRbgOj_6jeUGE,7272
+sempy/fabric/_flat_list_hierarchies.py,sha256=k6IE6JO55i5wzAPz7Jw9qKzUARlmXnTiS4NiwyW2oU0,5072
+sempy/fabric/_flat_list_partitions.py,sha256=diTLZQHbL_2zNWfJiu6OK0Hr0i_dEeNtKrBYIA7vTzQ,7697
+sempy/fabric/_flat_list_perspectives.py,sha256=TErOIp9aLc0glX7Q7io9TYEqCpvH06i-Fe6x16K2GXE,2218
+sempy/fabric/_flat_list_relationships.py,sha256=zG7YHgbZn5pYSaj_x0iaHS76uIbiicfT6Vxp5Kj_biA,7317
 sempy/fabric/_metadatakeys.py,sha256=H1_V1UpeBQB8YksjlRZD8rZwBF0HppxKsKffoCkrWmI,1256
-sempy/fabric/_token_provider.py,sha256=saAHITkSWIBsMpTBUBZjtTbGf-jr2s605HdpbU6BYQk,2790
-sempy/fabric/_utils.py,sha256=4wRI4qrPbEy5_ijD3tDAN0kC_zQw6oZa5lRGlPW0IM8,9009
+sempy/fabric/_token_provider.py,sha256=TTTt7Jyy5dh9FyGs632LizDYzB5e_2czQMyfjaSc_Nc,2743
+sempy/fabric/_utils.py,sha256=rrIbE-gmfBYg3l2B1oVnwBvwxbYXuz2New-yd9DsuuE,9111
 sempy/fabric/_client/__init__.py,sha256=LYN6FfmHhqjKiTAFhtMCBzqsFCXFKw-OC6R4LIwlOdc,384
 sempy/fabric/_client/_adomd_connection.py,sha256=c_AvSBCchcEOGPmRFHEvhR0MNGRZqTrNDEkTbs652lg,2715
-sempy/fabric/_client/_base_dataset_client.py,sha256=7FKc8WOVJRawTb1ZnPtNPC1du3RpC4X8ioq6Vh_mygg,24963
+sempy/fabric/_client/_base_dataset_client.py,sha256=ZpZWS_Kvs3zDdqJJzYx24IDLmvtNA-26znqlaQem1cs,28046
 sempy/fabric/_client/_connection_mode.py,sha256=fq0Y5dOJWCyfURO2H9DaQl_X1gTh0dhuly3v6krX4m0,639
-sempy/fabric/_client/_dataset_onelake_import.py,sha256=dKT6mL-1uVWz6OrRHL9TljWs5Ml-km6kzFiKIyCYXig,1515
-sempy/fabric/_client/_dataset_rest_client.py,sha256=dYxoYIwcxIMCbAdmLFXqeiAJkO1_SGMhKzDyVKcuUJI,9807
-sempy/fabric/_client/_dataset_xmla_client.py,sha256=_o4uE4MMyKqs2LZdGIunXjigJ0wzBKnuZa-R-9EGsBQ,13385
+sempy/fabric/_client/_dataset_onelake_import.py,sha256=Dbrp8qkp1UYfmtU50OOsnU2JCUqb1HpWjbZ9U7Abpgs,1541
+sempy/fabric/_client/_dataset_rest_client.py,sha256=4mgCZTUTgBbeBhTH1dGbYFLS8j_8DQCI1Vt_kUJeBpQ,9814
+sempy/fabric/_client/_dataset_xmla_client.py,sha256=cTTRdtpimC1Sn28VzogPex0vkakPpqOoT_p9m5VWPgk,13483
 sempy/fabric/_client/_fabric_rest_api.py,sha256=FxiV2yESqm_HI-ovzAZESIkB-Hq9S9iNEsLjFW43rzo,7719
 sempy/fabric/_client/_pbi_rest_api.py,sha256=MkC8lEGmbQplvW9WzNo15xlAxTjFMEEcyzcW5PB4AxY,14867
 sempy/fabric/_client/_refresh_execution_details.py,sha256=MWyFsIfcVz-Rc64zz7T8ohw2bamw9-IBNijDs7IDGds,1673
 sempy/fabric/_client/_rest_client.py,sha256=AjkbO4DPysnnu_9zQFlumcToW5aIwol3P-472vhReSw,9301
 sempy/fabric/_client/_tools.py,sha256=1jT2AXNGlIEtlyjp2Gi9x595Uf6ndHAft90cGoQZ_Fs,3681
 sempy/fabric/_client/_utils.py,sha256=RWDUtsRgMa1WvQSj4PET0DAvLq97k2QXkcpeoKtsngs,2906
-sempy/fabric/_client/_workspace_client.py,sha256=-TuOKhdfN6fBz9vNLTwfHNBrUdrlgVcpdImqhtV1KCs,25316
+sempy/fabric/_client/_workspace_client.py,sha256=tzF8CUC8GPdSlNq5rhPsf9bM7GB2KWjYnFCBTSqzYrk,25368
 sempy/fabric/_dataframe/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sempy/fabric/_dataframe/_fabric_dataframe.py,sha256=e-15egcjjE5dNOpOlLHsaFlrCwdgilgkC8ofWf-HpMg,26953
+sempy/fabric/_dataframe/_fabric_dataframe.py,sha256=evTCTYe5MKl4HHI3VlBeCIbsa7Ku4x-TW87WpvlWzPE,27600
 sempy/fabric/_dataframe/_fabric_series.py,sha256=yHxNKV0k645wuhsTXddgkjJHBOdNdhE5revxzt0PizY,1764
 sempy/fabric/_trace/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 sempy/fabric/_trace/_trace.py,sha256=9L_y969xv7AfZOZKXLNoYn3xQ0Pcqw6n-owKd8iRO6g,11613
 sempy/fabric/_trace/_trace_connection.py,sha256=eKGjT62PgkbjYaQEDOLKN_i-SAi7cDgwQauiBmZaIOs,7110
 sempy/fabric/exceptions/__init__.py,sha256=UOgxb19UduZMsdTulnXpAWyu3iNewQvJYCL_kS4g5VE,288
 sempy/fabric/exceptions/_exceptions.py,sha256=h7msbKn4oGyxHen3L-KnRZMMvfLKgQFQ5UpdW97UO-A,2705
 sempy/fabric/matcher/__init__.py,sha256=dW1vzY_aqFz4LFFJ26BqDCrNm9iZ6YQzpQn3XLrJbP0,503
@@ -76,28 +76,26 @@
 sempy/lib/Microsoft.AnalysisServices.Core.dll,sha256=QvK5VbMLvvumiPp14xPiVuKr5vxVBHPR1KpwYKZcd3k,1146416
 sempy/lib/Microsoft.AnalysisServices.Runtime.Core.dll,sha256=-GMhBTIFwyGHgxuI2n3ef9LeXzFxMi_BVmp5Mg3tRTA,98864
 sempy/lib/Microsoft.AnalysisServices.Runtime.Windows.dll,sha256=UsBO2Q234tpNThKtdKWye-hpbBp1pCf7ERyuUvNyFfM,96816
 sempy/lib/Microsoft.AnalysisServices.Tabular.Json.dll,sha256=Vux47roxP1ZzZ18a9mn2hRimPW2FtnTlKDDVRjIywKM,563248
 sempy/lib/Microsoft.AnalysisServices.Tabular.dll,sha256=JHv1zt3Ll_RaakDyTa9t7uP11kKY6Gc-Lny_i2zei9A,1640496
 sempy/lib/Microsoft.AnalysisServices.dll,sha256=eP-vkBavaDeFsDJbbIygWolYpM2ztX0PwvHcYUfAT8M,675888
 sempy/lib/Microsoft.Data.Analysis.dll,sha256=3BS8jT5NStf1pzTmZiOvaRnise4H_i0kEdErXYMIGeo,597632
-sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll,sha256=V4fjEnLrCZMoH-tYx48iRLCbBQQdPa2dTCW6pB6YXmI,16896
+sempy/lib/Microsoft.Fabric.SemanticLink.XmlaTools.dll,sha256=dSSQSpaFBgODq6V6acj-cuKKLJyCJjnwN-JZzVfpEHs,16896
 sempy/lib/Microsoft.IO.RecyclableMemoryStream.dll,sha256=16gd7FsEaR0aOqXUjWRy6EvWURKRo4f4XjisuQW-hnA,64960
 sempy/lib/Microsoft.Identity.Client.dll,sha256=dFt6IcdI1VuXXjZMvd5hkVYiZNhGGfsY5Bp3zRXY7LA,1402840
 sempy/lib/Microsoft.ML.DataView.dll,sha256=DJyxFgRHKSapziu3x6GUzQ1GhG0ZT6BZVeiHkpaYWyU,48256
 sempy/lib/Parquet.dll,sha256=lh1KdXIMxFPi6QZL9dHxfC9EdccDBEMxpL23imDuFl4,692736
 sempy/lib/Snappier.dll,sha256=J1Ow6eRuoU35HB1Yu6AXK632onDtf6f2kmnTH5spBPU,41472
 sempy/lib/ZstdSharp.dll,sha256=1m_GkXcBMZCydBqxxxJdOwgZzqEgxZ27k-D9s7aRzO8,442368
 sempy/relationships/__init__.py,sha256=1FjMh0veMEwrsChncYjPaCoQwvktnzENJLRzwOW3IwA,383
-sempy/relationships/_find.py,sha256=PDBvN1VN7fL-EDoIJ0ztJrJ8WIaSDs6kV8lPCZ_WCB4,14768
+sempy/relationships/_find.py,sha256=6xMexd1ahCha3i9ameKbKTxogHf0gcKb1x73NSz6FG0,14768
 sempy/relationships/_multiplicity.py,sha256=_iSAD2AxBth7upIiUsd0Flf2EwwFQ1-StiLvudn_rJ0,278
 sempy/relationships/_plot.py,sha256=Uu8pDVm9VV3tAV53cJC3ZoPNXU7YNlhB992ZoydvBpg,5548
 sempy/relationships/_stats.py,sha256=OStKIemRVIpCmb9OS85yYovvxGw5V6NfmhGyCi8Nq80,5665
 sempy/relationships/_utils.py,sha256=32KtjQALb7ADuzGNNY2w0u83Pb5DFJXDGWM1DHvS4Us,3209
 sempy/relationships/_validate.py,sha256=tWOtQgXxg3iX7Exioh_TH4II1bLLNKAIHdT0a3I5zsg,5934
-sempy/samples/__init__.py,sha256=oK-VwEf1ah0_hM_zbwp7KVcznBuP2Xv2ldW8HIu3zoc,91
-sempy/samples/_samples.py,sha256=0HxnBnlTlXiEWIrDGWCGPAkmqezmsNGEEOCmLDIiCH0,4086
-semantic_link_sempy-0.7.2.dist-info/LICENSE.txt,sha256=s6ujR17r98d7cZDEpzrNo04NvEWOyVr0IPe8Eynozwc,12690
-semantic_link_sempy-0.7.2.dist-info/METADATA,sha256=qL9eeYAiaYgJGzDkjAfZZlRVwCRDBeuxCjHP67gA6Zw,4972
-semantic_link_sempy-0.7.2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-semantic_link_sempy-0.7.2.dist-info/top_level.txt,sha256=mptJr2o3N2Z496h_roahVjdqNE7Kgsi0mHWtCxiR0ME,6
-semantic_link_sempy-0.7.2.dist-info/RECORD,,
+semantic_link_sempy-0.7.3.dist-info/LICENSE.txt,sha256=s6ujR17r98d7cZDEpzrNo04NvEWOyVr0IPe8Eynozwc,12690
+semantic_link_sempy-0.7.3.dist-info/METADATA,sha256=joKnaKsHrNm_pDzfnBfFepxbTrngg54CJNnp-QFtXLY,4972
+semantic_link_sempy-0.7.3.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+semantic_link_sempy-0.7.3.dist-info/top_level.txt,sha256=mptJr2o3N2Z496h_roahVjdqNE7Kgsi0mHWtCxiR0ME,6
+semantic_link_sempy-0.7.3.dist-info/RECORD,,
```

