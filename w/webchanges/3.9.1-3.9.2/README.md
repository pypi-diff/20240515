# Comparing `tmp/webchanges-3.9.1-py3-none-any.whl.zip` & `tmp/webchanges-3.9.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 95402 bytes, number of entries: 23
--rw-r--r--  2.0 unx     1891 b- defN 22-Jan-28 08:42 webchanges/__init__.py
--rw-r--r--  2.0 unx     9835 b- defN 22-Jan-28 08:42 webchanges/cli.py
--rw-r--r--  2.0 unx    27307 b- defN 22-Jan-28 08:42 webchanges/command.py
--rw-r--r--  2.0 unx     9493 b- defN 22-Jan-28 08:42 webchanges/config.py
--rw-r--r--  2.0 unx    56151 b- defN 22-Jan-28 08:42 webchanges/filters.py
--rw-r--r--  2.0 unx    21606 b- defN 22-Jan-28 08:42 webchanges/handler.py
--rw-r--r--  2.0 unx    67575 b- defN 22-Jan-28 08:42 webchanges/jobs.py
--rw-r--r--  2.0 unx     5275 b- defN 22-Jan-28 08:42 webchanges/mailer.py
--rw-r--r--  2.0 unx     4174 b- defN 22-Jan-28 08:42 webchanges/main.py
--rw-r--r--  2.0 unx    65049 b- defN 22-Jan-28 08:42 webchanges/reporters.py
--rw-r--r--  2.0 unx    51935 b- defN 22-Jan-28 08:42 webchanges/storage.py
--rw-r--r--  2.0 unx     4498 b- defN 22-Jan-28 08:42 webchanges/storage_minidb.py
--rw-r--r--  2.0 unx    10430 b- defN 22-Jan-28 08:42 webchanges/util.py
--rw-r--r--  2.0 unx     8185 b- defN 22-Jan-28 08:42 webchanges/worker.py
--rw-r--r--  2.0 unx        0 b- defN 22-Jan-28 08:42 webchanges/_vendored/__init__.py
--rw-r--r--  2.0 unx    17527 b- defN 22-Jan-28 08:42 webchanges/_vendored/packaging_version.py
--rw-r--r--  2.0 unx     2720 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/COPYING
--rw-r--r--  2.0 unx    11276 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       52 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       11 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/zip-safe
-?rw-rw-r--  2.0 unx     1867 b- defN 22-Jan-28 08:42 webchanges-3.9.1.dist-info/RECORD
-23 files, 376950 bytes uncompressed, 92406 bytes compressed:  75.5%
+Zip file size: 99373 bytes, number of entries: 23
+-rw-r--r--  2.0 unx     2001 b- defN 22-Apr-13 23:53 webchanges/__init__.py
+-rw-r--r--  2.0 unx    10058 b- defN 22-Apr-13 23:53 webchanges/cli.py
+-rw-r--r--  2.0 unx    28877 b- defN 22-Apr-13 23:53 webchanges/command.py
+-rw-r--r--  2.0 unx    10414 b- defN 22-Apr-13 23:53 webchanges/config.py
+-rw-r--r--  2.0 unx    56595 b- defN 22-Apr-13 23:53 webchanges/filters.py
+-rw-r--r--  2.0 unx    26040 b- defN 22-Apr-13 23:53 webchanges/handler.py
+-rw-r--r--  2.0 unx    71766 b- defN 22-Apr-13 23:53 webchanges/jobs.py
+-rw-r--r--  2.0 unx     5384 b- defN 22-Apr-13 23:53 webchanges/mailer.py
+-rw-r--r--  2.0 unx     4283 b- defN 22-Apr-13 23:53 webchanges/main.py
+-rw-r--r--  2.0 unx    68352 b- defN 22-Apr-13 23:53 webchanges/reporters.py
+-rw-r--r--  2.0 unx    55722 b- defN 22-Apr-13 23:53 webchanges/storage.py
+-rw-r--r--  2.0 unx     5385 b- defN 22-Apr-13 23:53 webchanges/storage_minidb.py
+-rw-r--r--  2.0 unx    10549 b- defN 22-Apr-13 23:53 webchanges/util.py
+-rw-r--r--  2.0 unx     8348 b- defN 22-Apr-13 23:53 webchanges/worker.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Apr-13 23:53 webchanges/_vendored/__init__.py
+-rw-r--r--  2.0 unx    17527 b- defN 22-Apr-13 23:53 webchanges/_vendored/packaging_version.py
+-rw-r--r--  2.0 unx     2878 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx    11234 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       51 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       11 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/zip-safe
+?rw-rw-r--  2.0 unx     1869 b- defN 22-Apr-13 23:54 webchanges-3.9.2.dist-info/RECORD
+23 files, 397437 bytes uncompressed, 96377 bytes compressed:  75.8%
```

## zipnote {}

```diff
@@ -42,29 +42,29 @@
 
 Filename: webchanges/_vendored/__init__.py
 Comment: 
 
 Filename: webchanges/_vendored/packaging_version.py
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/COPYING
+Filename: webchanges-3.9.2.dist-info/LICENSE
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/METADATA
+Filename: webchanges-3.9.2.dist-info/METADATA
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/WHEEL
+Filename: webchanges-3.9.2.dist-info/WHEEL
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/entry_points.txt
+Filename: webchanges-3.9.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/top_level.txt
+Filename: webchanges-3.9.2.dist-info/top_level.txt
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/zip-safe
+Filename: webchanges-3.9.2.dist-info/zip-safe
 Comment: 
 
-Filename: webchanges-3.9.1.dist-info/RECORD
+Filename: webchanges-3.9.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## webchanges/__init__.py

```diff
@@ -1,26 +1,29 @@
 """Checks web content to detect any changes since the prior run. If any are found, it shows what changed ('diff')
 and/or sends it via e-mail and/or other supported services. Can check the output of local commands as well.
 """
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
+
 # The docstring above (__doc__) and the variables below are used in the program and for builds, including in building
 # documentation with Sphinx.
 
 # Older Python versions are supported for 3 years after being obsoleted by a new major release.
 __min_python_version__ = (3, 7)  # minimum version of Python required to run; supported until 13 October 2022
 
 
 __project_name__ = __package__
 # Version numbering is PEP440-compliant https://www.python.org/dev/peps/pep-0440/
 # Release numbering largely follows Semantic Versioning https://semver.org/spec/v2.0.0.html#semantic-versioning-200
 # * MAJOR version when you make incompatible API changes,
 # * MINOR version when you add functionality in a backwards compatible manner, and
 # * MICRO or PATCH version when you make backwards compatible bug fixes. We no longer use '0'
 # If unsure on increments, use pkg_resources.parse_version to parse
-__version__ = '3.9.1'
+__version__ = '3.9.2'
 __description__ = (
     'Check web (or commands) for changes since last run and notify.\n\nAnonymously alerts you of webpage changes.'
 )
 __author__ = 'Mike Borsetti <mike@borsetti.com>'
 __copyright__ = 'Copyright 2020- Mike Borsetti'
 __license__ = 'MIT, BSD 3-Clause License'
 __url__ = f'https://pypi.org/project/{__project_name__}/'
```

## webchanges/cli.py

```diff
@@ -1,47 +1,40 @@
 #!/usr/bin/env python3
 
 """Module containing the entry point main()."""
 
 # See config module for the command line arguments
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
+
 import logging
 import os
 import shutil
 import signal
 import sys
 import warnings
 from pathlib import Path, PurePath
 from typing import Optional, Union
 
-from platformdirs import user_cache_path, user_config_path
+import platformdirs
 
 from . import __copyright__, __docs_url__, __min_python_version__, __project_name__, __version__
 from .command import UrlwatchCommand
 from .config import CommandConfig
 from .main import Urlwatch
 from .storage import (
     CacheDirStorage,
     CacheRedisStorage,
     CacheSQLite3Storage,
     CacheStorage,
     YamlConfigStorage,
     YamlJobsStorage,
 )
 
-# Directory where the config, jobs and hooks files are located
-if os.name != 'nt':
-    config_path = user_config_path(__project_name__)  # typically ~/.config/{__project_name__}
-else:
-    config_path = Path.home().joinpath('Documents').joinpath(__project_name__)
-
-# directory where the database is located
-# typically ~/.cache/{__project_name__} or %LOCALAPPDATA%\{__project_name__}\{__project_name__}\Cache
-cache_path = user_cache_path(__project_name__)
-
 # Ignore signal SIGPIPE ("broken pipe") for stdout (see https://github.com/thp/urlwatch/issues/77)
 if os.name != 'nt':  # Windows does not have signal.SIGPIPE
     signal.signal(signal.SIGPIPE, signal.SIG_DFL)  # type: ignore[attr-defined]  # not defined in Windows
 
 logger = logging.getLogger(__name__)
 
 
@@ -71,15 +64,15 @@
     :param hooks_file: The Path to the hooks file.
     :param cache_file: The Path to the snapshot database file.
     """
     lg_project_path = Path.home().joinpath(f'.{legacy_package}')
     lg_config_file = lg_project_path.joinpath(f'{legacy_package}.yaml')
     lg_urls_file = lg_project_path.joinpath('urls.yaml')
     lg_hooks_file = lg_project_path.joinpath('hooks.py')
-    lg_cache_path = user_cache_path(legacy_package)
+    lg_cache_path = platformdirs.user_cache_path(legacy_package)
     lg_cache_file = lg_cache_path.joinpath('cache.db')
     for old_file, new_file in zip(
         (lg_config_file, lg_urls_file, lg_hooks_file, lg_cache_file), (config_file, jobs_file, hooks_file, cache_file)
     ):
         if old_file.is_file() and not new_file.is_file():
             new_file.parent.mkdir(parents=True, exist_ok=True)
             shutil.copyfile(old_file, new_file)
@@ -106,33 +99,34 @@
 def locate_storage_file(filename: Path, default_path: Path, ext: Optional[str] = None) -> Path:
     """Searches for file both as specified and in the default directory, then retries with 'ext' extension if defined.
 
     :param filename: The filename.
     :param default_path: The default directory.
     :param ext: The extension, e.g. '.yaml', to add for searching if first scan fails.
 
-    :returns: The filename, either original or with path where found.
+    :returns: The filename, either original or one with path where found and/or extension.
     """
     search_filenames = [filename]
 
     # if ext is given, iterate both on raw filename and the filename with ext if different
     if ext and filename.suffix != ext:
         search_filenames.append(filename.with_suffix(ext))
 
     for file in search_filenames:
         # return if found
         if file.is_file():
             return file
 
-        # no directory specified: add default one
+        # no directory specified (and not in current one): add default one
         if file.parent == PurePath('.'):
             new_file = default_path.joinpath(file)
             if new_file.is_file():
                 return new_file
 
+    # no matches found
     return filename
 
 
 def first_run(command_config: CommandConfig) -> None:
     """Create configuration and jobs files.
 
     :param command_config: the CommandConfig containing the command line arguments selected.
@@ -162,52 +156,63 @@
     """
     # Make sure that PendingDeprecationWarning are displayed from all modules (otherwise only those in __main__ are)
     warnings.filterwarnings('default', category=PendingDeprecationWarning)
 
     # Issue deprecation warning if running on minimum version supported
     python_version_warning()
 
+    # Directory where the config, jobs and hooks files are located
+    if os.name != 'nt':
+        config_path = platformdirs.user_config_path(__project_name__)  # typically ~/.config/{__project_name__}
+    else:
+        config_path = Path.home().joinpath('Documents').joinpath(__project_name__)
+
+    # Directory where the database is located; typically ~/.cache/{__project_name__}
+    # or %LOCALAPPDATA%\{__project_name__}\{__project_name__}\Cache
+    cache_path = platformdirs.user_cache_path(__project_name__)
+
     # The config, jobs, hooks and cache files
     default_config_file = config_path.joinpath('config.yaml')
     default_jobs_file = config_path.joinpath('jobs.yaml')
     default_hooks_file = config_path.joinpath('hooks.py')
     default_cache_file = cache_path.joinpath('cache.db')
 
-    # Migrate legacy (urlwatch 2.23) files
+    # Migrate legacy (urlwatch 2.25) files
     migrate_from_legacy('urlwatch', default_config_file, default_jobs_file, default_hooks_file, default_cache_file)
 
     # Load config files
     command_config = CommandConfig(
+        sys.argv[1:],
         __project_name__,
         config_path,
         default_config_file,
         default_jobs_file,
         default_hooks_file,
         default_cache_file,
     )
 
-    # set up the logger to verbose if needed
+    # Set up the logger to verbose if needed
     if command_config.verbose:
         setup_logger(command_config.log_level)
     else:
         setup_logger()
 
-    # check for location of config files entered in cli
+    # Locate config, job and hooks files
     command_config.config = locate_storage_file(command_config.config, command_config.config_path, '.yaml')
     command_config.jobs = locate_storage_file(command_config.jobs, command_config.config_path, '.yaml')
     command_config.hooks = locate_storage_file(command_config.hooks, command_config.config_path, '.py')
 
-    # check for first run
+    # Check for first run
     if command_config.config == default_config_file and not Path(command_config.config).is_file():
         first_run(command_config)
 
-    # setup config file API
+    # Setup config file API
     config_storage = YamlConfigStorage(command_config.config)  # storage.py
 
-    # setup database API
+    # Setup database API
     if command_config.database_engine == 'sqlite3':
         cache_storage: CacheStorage = CacheSQLite3Storage(
             command_config.cache, command_config.max_snapshots
         )  # storage.py
     elif any(str(command_config.cache).startswith(prefix) for prefix in ('redis://', 'rediss://')):
         cache_storage = CacheRedisStorage(command_config.cache)  # storage.py
     elif command_config.database_engine == 'redis':
@@ -217,20 +222,20 @@
     elif command_config.database_engine == 'minidb':
         from .storage_minidb import CacheMiniDBStorage  # legacy code imported only if needed (requires minidb)
 
         cache_storage = CacheMiniDBStorage(command_config.cache)  # storage.py
     else:
         raise NotImplementedError(f'Database engine {command_config.database_engine} not implemented')
 
-    # setup jobs file API
+    # Setup jobs file API
     jobs_storage = YamlJobsStorage(command_config.jobs)  # storage.py
 
-    # setup urlwatch
+    # Setup 'urlwatch'
     urlwatcher = Urlwatch(command_config, config_storage, cache_storage, jobs_storage)  # main.py
     urlwatch_command = UrlwatchCommand(urlwatcher)  # command.py
 
-    # run urlwatch
+    # Run 'urlwatch'
     urlwatch_command.run()
 
 
 if __name__ == '__main__':
     main()
```

## webchanges/command.py

```diff
@@ -1,23 +1,31 @@
 """Take actions from command line arguments."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 import contextlib
 import logging
 import os
 import shutil
 import subprocess
 import sys
 import time
 import traceback
 from concurrent.futures import ThreadPoolExecutor
+from datetime import datetime
 from pathlib import Path
 from typing import Optional, TYPE_CHECKING, Union
 
 import requests
 
+try:
+    from zoneinfo import ZoneInfo  # not available in Python < 3.9
+except ImportError:
+    from backports.zoneinfo import ZoneInfo  # type: ignore[no-redef]
+
 from . import __docs_url__, __project_name__, __version__
 from .filters import FilterBase
 from .handler import JobState, Report
 from .jobs import BrowserJob, JobBase, UrlJob
 from .mailer import smtp_have_password, smtp_set_password, SMTPMailer
 from .main import Urlwatch
 from .reporters import ReporterBase, xmpp_have_password, xmpp_set_password
@@ -111,15 +119,15 @@
         print(ReporterBase.reporter_documentation())
         print()
         print(f'Please see full documentation at {__docs_url__}')
 
         return 0
 
     @staticmethod
-    def show_chromium_directory() -> int:
+    def show_chromium_directory() -> int:  # pragma: no cover
         try:
             from pyppeteer.chromium_downloader import DOWNLOADS_FOLDER
         except ImportError:
             print("'pyppeteer' module is not installed.")
             return 1
 
         chromium_folder = Path(DOWNLOADS_FOLDER)
@@ -193,15 +201,15 @@
         start = time.perf_counter()
 
         if isinstance(job, UrlJob):
             # Force re-retrieval of job, as we're testing filters
             job.ignore_cached = True
 
         with JobState(self.urlwatcher.cache_storage, job) as job_state:
-            job_state.process()
+            job_state.process(headless=not self.urlwatch_config.no_headless)
             duration = time.perf_counter() - start
             if job_state.exception is not None:
                 # self.print_new_version()
                 raise job_state.exception
             print(job_state.job.pretty_name())
             print('-' * len(job_state.job.pretty_name()))
             if hasattr(job_state.job, 'note') and job_state.job.note:
@@ -249,14 +257,36 @@
                     self._exit(errorlevel)
 
         # We do not save the job state or job on purpose here, since we are possibly modifying the job
         # (ignore_cached) and we do not want to store the newly-retrieved data yet (filter testing)
 
         return 0
 
+    def dump_history(self, job_id: str) -> int:
+        job = self._get_job(job_id)
+        history_data = self.urlwatcher.cache_storage.get_rich_history_data(job.get_guid())
+
+        print(f'History for job {job.get_indexed_location()}:')
+        for i, entry in enumerate(history_data):
+            etag = f"; ETag: {entry['etag']}" if entry.get('etag') else ''
+            tries = f"; failed tries: {entry['tries']}" if entry.get('tries') else ''
+            tz = self.urlwatcher.report.config['report']['tz'] or 'Etc/UTC'
+            dt = datetime.fromtimestamp(entry['timestamp'], ZoneInfo(tz))
+            header = f"{i + 1}) {dt.strftime('%Y-%m-%d %H:%M %Z')}{etag}{tries}"
+            sep_len = max(50, min(50, len(header)))
+            print('=' * sep_len)
+            print(header)
+            print('-' * sep_len)
+            print(entry['data'])
+            print('=' * sep_len, '\n')
+
+        print(f'Found {len(history_data)} snapshot' + ('s' if len(history_data) != 1 else '') + '.')
+
+        return 0
+
     def list_error_jobs(self) -> None:
         start = time.perf_counter()
         print(
             f'Jobs, if any, with errors or returning no data after filtering in jobs file\n'
             f'{self.urlwatch_config.jobs}:\n'
         )
         jobs = [job.with_defaults(self.urlwatcher.config_storage.config) for job in self.urlwatcher.jobs]
@@ -265,15 +295,15 @@
             job.ignore_cached = True
         with contextlib.ExitStack() as stack:
             max_workers = min(32, os.cpu_count() or 1) if any(type(job) == BrowserJob for job in jobs) else None
             logger.debug(f'Max_workers set to {max_workers}')
             executor = ThreadPoolExecutor(max_workers=max_workers)
 
             for job_state in executor.map(
-                lambda jobstate: jobstate.process(),
+                lambda jobstate: jobstate.process(headless=not self.urlwatch_config.no_headless),
                 (stack.enter_context(JobState(self.urlwatcher.cache_storage, job)) for job in jobs),
             ):
                 if job_state.exception is not None:
                     pretty_name = job_state.job.pretty_name()
                     location = job_state.job.get_location()
                     if pretty_name != location:
                         print(
@@ -297,24 +327,24 @@
         dur_str = f'{float(f"{duration:.2g}"):g}' if duration < 10 else f'{duration:.0f}'
         print('--')
         print(f"Checked {len(jobs)} job{'s' if len(jobs) else ''} in {dur_str} seconds")
 
         # We do not save the job state or job on purpose here, since we are possibly modifying the job
         # (ignore_cached) and we do not want to store the newly-retrieved data yet (just showing errors)
 
-    def delete_snapshot(self, job_id: Union[str, int]) -> None:
+    def delete_snapshot(self, job_id: Union[str, int]) -> int:
         job = self._get_job(job_id)
 
         deleted = self.urlwatcher.cache_storage.delete_latest(job.get_guid())
         if deleted:
             print(f'Deleted last snapshot of {job.get_indexed_location()}')
-            self._exit(0)
+            return 0
         else:
             print(f'No snapshots found to be deleted for {job.get_indexed_location()}')
-            self._exit(1)
+            return 1
 
     def modify_urls(self) -> None:
         save = True
         if self.urlwatch_config.delete is not None:
             job = self._find_job(self.urlwatch_config.delete)
             if job is not None:
                 self.urlwatcher.jobs.remove(job)
@@ -424,15 +454,17 @@
 
         reporter_name = self.urlwatch_config.test_reporter
         if reporter_name not in ReporterBase.__subclasses__:
             print(f'No such reporter: {reporter_name}')
             print(f'\nSupported reporters:\n{ReporterBase.reporter_documentation()}\n')
             return 1
 
-        cfg: ConfigReportersList = self.urlwatcher.config_storage.config['report'][reporter_name]  # type: ignore[misc]
+        cfg: ConfigReportersList = self.urlwatcher.config_storage.config['report'][
+            reporter_name  # type: ignore[literal-required]
+        ]
         if job_state:  # we want a full report
             cfg['enabled'] = True  # type: ignore[index]
             self.urlwatcher.config_storage.config['report']['text']['details'] = True
             self.urlwatcher.config_storage.config['report']['text']['footer'] = True
             self.urlwatcher.config_storage.config['report']['text']['minimal'] = False
             self.urlwatcher.config_storage.config['report']['markdown']['details'] = True
             self.urlwatcher.config_storage.config['report']['markdown']['footer'] = True
@@ -594,15 +626,15 @@
         cmd = [str(driver_executable), 'install', 'chrome']
         logger.info(f"Running playwright CLI: {' '.join(cmd)}")
         completed_process = subprocess.run(cmd, env=env, capture_output=True, text=True)
         if completed_process.returncode:
             print(completed_process.stderr)
             return completed_process.returncode
         if completed_process.stdout:
-            logger.info(f'Output of Playwright CLI: {completed_process.stdout}')
+            logger.info(f'Success! Output of Playwright CLI: {completed_process.stdout}')
         return 0
 
     def handle_actions(self) -> None:
         if self.urlwatch_config.list:
             self.list_jobs()
             self._exit(0)
 
@@ -613,14 +645,17 @@
         if self.urlwatch_config.test_job:
             self.test_job(self.urlwatch_config.test_job)
             self._exit(0)
 
         if self.urlwatch_config.test_diff:
             self._exit(self.test_diff(self.urlwatch_config.test_diff))
 
+        if self.urlwatch_config.dump_history:
+            sys.exit(self.dump_history(self.urlwatch_config.dump_history))
+
         if self.urlwatch_config.add or self.urlwatch_config.delete:
             self.modify_urls()
             self._exit(0)
 
         if self.urlwatch_config.test_reporter:
             self._exit(self.check_test_reporter())
 
@@ -652,16 +687,15 @@
 
         if self.urlwatch_config.rollback_cache:
             self.urlwatcher.cache_storage.rollback_cache(self.urlwatch_config.rollback_cache)
             self.urlwatcher.cache_storage.close()
             self._exit(0)
 
         if self.urlwatch_config.delete_snapshot:
-            self.delete_snapshot(self.urlwatch_config.delete_snapshot)
-            self._exit(0)
+            self._exit(self.delete_snapshot(self.urlwatch_config.delete_snapshot))
 
         if self.urlwatch_config.features:
             self._exit(self.show_features())
 
         if self.urlwatch_config.chromium_directory:
             self._exit(self.show_chromium_directory())
```

## webchanges/config.py

```diff
@@ -1,9 +1,11 @@
 """Command-line configuration."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 import argparse
 import os
 from pathlib import Path
 from typing import List, Optional, Union
 
 from . import __doc__, __docs_url__, __project_name__, __version__
 from .util import get_new_version_number
@@ -24,52 +26,57 @@
         """
 
         :param project_name: The name of the project.
         :param config_path: The path of the configuration directory.
         :param config: The path of the configuration file.
         :param jobs: The path of the jobs file.
         :param hooks: The path of the Python hooks file.
-        :param cache: The path of the snapshots database file.
+        :param cache: The path of the database file (or directory if using the textfiles database-engine) where
+           snapshots are stored.
         """
         self.project_name = project_name
         self.config_path = config_path
         self.config = config
         self.jobs = jobs
         self.hooks = hooks
         self.cache = cache
 
 
 class CommandConfig(BaseConfig):
     """Command line arguments configuration; the arguments are stored as class attributes."""
 
     def __init__(
         self,
+        args: List[str],
         project_name: str,
         config_path: Path,
         config: Path,
         jobs: Path,
         hooks: Path,
         cache: Union[str, Path],
     ) -> None:
-        """
+        """Command line arguments configuration; the arguments are stored as class attributes.
 
         :param project_name: The name of the project.
         :param config_path: The path of the configuration directory.
         :param config: The path of the configuration file.
         :param jobs: The path of the jobs file.
         :param hooks: The path of the Python hooks file.
-        :param cache: The path of the snapshots database file.
+        :param cache: The path of the database file (or directory if using the textfiles database-engine) where
+           snapshots are stored.
         """
         super().__init__(project_name, config_path, config, jobs, hooks, cache)
         self.joblist: Optional[List[int]] = None
         self.verbose: bool = False
         self.list: bool = False
         self.errors: bool = False
         self.test_job: Optional[str] = None
+        self.no_headless: bool = False
         self.test_diff: Optional[str] = None
+        self.dump_history: Optional[str] = None
         self.add: Optional[str] = None
         self.delete: Optional[str] = None
         self.test_reporter: Optional[str] = None
         self.smtp_login: bool = False
         self.telegram_chats: bool = False
         self.xmpp_login: bool = False
         self.edit: bool = False
@@ -82,18 +89,18 @@
         self.database_engine: str = 'sqlite3'
         self.max_snapshots: int = 4
         self.features: bool = False
         self.chromium_directory: bool = False
         self.install_chrome: bool = False
         self.log_level: str = 'DEBUG'
 
-        self.parse_args()
+        self.parse_args(args)
 
-    def parse_args(self) -> argparse.ArgumentParser:
-        """Set up the Python arguments parser.
+    def parse_args(self, cmdline_args: List[str]) -> argparse.ArgumentParser:
+        """Set up the Python arguments parser and stores the arguments in the class's variables.
 
         :returns: The Python arguments parser.
         """
 
         new_version = get_new_version_number(timeout=0.5)
         new_version_text = (
             f'\nNew release version {new_version} is available; we recommend updating.' if new_version else ''
@@ -142,21 +149,31 @@
             '--test',
             '--test-filter',
             help='test a job (by index or URL/command) and show filtered output',
             metavar='JOB',
             dest='test_job',
         )
         group.add_argument(
+            '--no-headless',
+            action='store_true',
+            help='turn off browser headless mode (for jobs using a browser)',
+        )
+        group.add_argument(
             '--test-diff',
             '--test-diff-filter',
             help='test and show diff using existing saved snapshots of a job (by index or URL/command)',
             metavar='JOB',
             dest='test_diff',
         )
         group.add_argument(
+            '--dump-history',
+            help='print all saved snapshot history for a job (by index or URL/command)',
+            metavar='JOB',
+        )
+        group.add_argument(
             '--add',
             help='add job (key1=value1,key2=value2,...). WARNING: all remarks are deleted from '
             'jobs file; use --edit instead!',
             metavar='JOB',
         )
         group.add_argument(
             '--delete',
@@ -231,14 +248,14 @@
             default='DEBUG',
             choices=('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'),
             help='level of logging output when -v is selected (default: %(default)s)',
         )
 
         # workaround for avoiding triggering error when invoked by pytest
         if parser.prog != '_jb_pytest_runner.py' and not os.getenv('CI'):
-            args = parser.parse_args()
+            args = parser.parse_args(cmdline_args)
 
             for arg in vars(args):
                 argval = getattr(args, arg)
                 setattr(self, arg, argval)
 
         return parser
```

## webchanges/filters.py

```diff
@@ -1,9 +1,12 @@
 """Filters."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
+
 from __future__ import annotations
 
 import csv
 import hashlib
 import html
 import io
 import itertools
@@ -30,17 +33,17 @@
 
 # https://stackoverflow.com/questions/39740632
 if TYPE_CHECKING:
     from .handler import JobState
     from .jobs import JobBase
 
 try:
-    from bs4 import BeautifulSoup
+    import bs4
 except ImportError:  # pragma: has-bs4
-    BeautifulSoup = None
+    bs4 = None
 
 try:
     import cssbeautifier
 except ImportError:
     cssbeautifier = None
 
 try:
@@ -69,14 +72,15 @@
     pytesseract = None
 
 try:
     import vobject
 except ImportError:
     vobject = None
 
+from ._vendored.packaging_version import parse as parse_version
 
 logger = logging.getLogger(__name__)
 
 
 class FilterBase(object, metaclass=TrackSubClasses):
     """The base class for filters."""
 
@@ -151,15 +155,15 @@
            string-based filter list specification.
         :returns: Iterator of filter_kind, subfilter (where subfilter is a dict).
         """
         for filter_kind, subfilter in cls._internal_normalize_filter_list(filter_spec):
             filtercls = cls.__subclasses__.get(filter_kind, None)
 
             if filtercls is None:
-                raise ValueError(f'Unknown filter kind: {filter_kind} (subfilter {subfilter})')
+                raise ValueError(f'Unknown filter kind: {filter_kind} (subfilter: {subfilter})')
 
             if getattr(filtercls, '__no_subfilter__', False) and subfilter:
                 raise ValueError(f'No subfilters supported for {filter_kind}')
 
             if hasattr(filtercls, '__supported_subfilters__'):
                 provided_keys = set(subfilter.keys())
                 allowed_keys = set(filtercls.__supported_subfilters__.keys())
@@ -341,30 +345,32 @@
 
 class BeautifyFilter(FilterBase):
     """Beautify HTML (requires Python package ``BeautifulSoup`` and optionally ``jsbeautifier`` and/or
     ``cssbeautifier``)."""
 
     __kind__ = 'beautify'
 
-    __no_subfilter__ = True
+    __supported_subfilters__ = {'indent': 'Number of spaces by which to indent HTML output.'}
+
+    __default_subfilter__ = 'indent'
 
     def filter(self, data: Union[str, bytes], subfilter: Dict[str, Any]) -> str:
         """Filter (process) the data.
 
         :param data: The data to be filtered (processed).
         :param subfilter: The subfilter information.
         :returns: The filtered (processed) data.
         """
-        if BeautifulSoup is None:
+        if bs4 is None:
             raise ImportError(
                 f"Python package 'BeautifulSoup' is not installed; cannot use the '{self.__kind__}' "
                 f'filter ({self.job.get_indexed_location()})'
             )
 
-        soup = BeautifulSoup(data, features='lxml')
+        soup = bs4.BeautifulSoup(data, features='lxml')
 
         if jsbeautifier is None:
             logger.warning(
                 f"Python package 'jsbeautifier' is not installed; will not beautify <script> tags"
                 f' ({self.job.get_indexed_location()})'
             )
         else:
@@ -382,15 +388,19 @@
         else:
             styles = soup.find_all('style')
             for style in styles:
                 if style.string:
                     beautified_css = cssbeautifier.beautify(style.string)
                     style.string = beautified_css
 
-        return soup.prettify()
+        if parse_version(bs4.__version__) >= parse_version('4.11'):
+            indent = subfilter['indent']
+            return soup.prettify(formatter=bs4.formatter.HTMLFormatter(indent=indent))
+        else:
+            return soup.prettify()
 
 
 class Html2TextFilter(FilterBase):
     """Convert a string consisting of HTML to Unicode plain text for easy difference checking."""
 
     __kind__ = 'html2text'
 
@@ -469,22 +479,22 @@
                 setattr(parser, k.lower(), v)
                 if k == 'pad_tables':
                     self.job.markdown_padded_tables = v
 
             return parser.handle(data)
 
         elif method == 'bs4':
-            if BeautifulSoup is None:
+            if bs4 is None:
                 raise ImportError(
                     f"Python package 'BeautifulSoup' is not installed; cannot use the '{self.__kind__}: "
                     f"{method}' filter ({self.job.get_indexed_location()})"
                 )
 
             bs4_parser: str = options.pop('parser', 'lxml')
-            soup = BeautifulSoup(data, bs4_parser)
+            soup = bs4.BeautifulSoup(data, bs4_parser)
             separator: str = options.pop('separator', '')
             strip: bool = options.pop('strip', False)
             return soup.get_text(separator=separator, strip=strip)
 
         elif method in ('strip_tags', 're'):  # re for backward compatibility
             if method == 're':
                 warnings.warn(
@@ -885,18 +895,18 @@
                     f"The strip filter's 'side' sub-directive can only be 'right' or 'left' "
                     f'({self.job.get_indexed_location()})'
                 )
 
             return data.strip(subfilter.get('chars'))
 
 
-class StripEachLineFilter(FilterBase):
+class StripLinesFilter(FilterBase):
     """Deprecated; use ``strip`` with subfilter ``splitlines`` instead."""
 
-    __kind__ = 'strip_each_line'
+    __kind__ = 'striplines'
 
     __no_subfilter__ = True
 
     def filter(self, data: str, subfilter: Dict[str, Any]) -> str:  # type: ignore[override]
         warnings.warn(
             f"'strip_each_line' filter is deprecated; replace with 'strip' and sub-directive 'splitlines: "
             f"true' ({self.job.get_indexed_location()})",
```

## webchanges/handler.py

```diff
@@ -1,9 +1,11 @@
 """Handles the running of jobs and, afterwards, of the reports."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from __future__ import annotations
 
 import difflib
 import json
 import logging
 import shlex
 import subprocess
@@ -17,22 +19,28 @@
 from typing import Any, ContextManager, Iterable, List, Optional, Type, TYPE_CHECKING, Union
 
 from .filters import FilterBase
 from .jobs import NotModifiedError
 from .reporters import ReporterBase
 
 try:
-    from zoneinfo import ZoneInfo  # not available in Python < 3.9
+    from deepdiff import DeepDiff
 except ImportError:
-    from backports.zoneinfo import ZoneInfo  # type: ignore[no-redef]
+    DeepDiff = None  # type: ignore[no-redef]
 
 try:
-    from deepdiff import DeepDiff
+    import xmltodict
 except ImportError:
-    DeepDiff = None  # type: ignore[no-redef]
+    xmltodict = None  # type: ignore[no-redef]
+
+try:
+    from zoneinfo import ZoneInfo  # not available in Python < 3.9
+except ImportError:
+    from backports.zoneinfo import ZoneInfo  # type: ignore[no-redef]
+
 
 # https://stackoverflow.com/questions/39740632
 if TYPE_CHECKING:
     # from typing import Literal  # not available in Python < 3.8
 
     from .jobs import JobBase
     from .main import Urlwatch
@@ -52,28 +60,29 @@
     exception: Optional[Exception] = None
     traceback: str
     tries: int = 0
     old_etag: str = ''
     new_etag: str
     error_ignored: Union[bool, str] = False
     _generated_diff: Optional[str] = None
+    _generated_diff_html: Optional[str] = None
 
     def __init__(self, cache_storage: CacheStorage, job: JobBase, playwright: Any = None) -> None:
         """
 
         :param cache_storage: The CacheStorage object with the snapshot database methods.
         :param job: A JobBase object with the job information.
         """
         self.cache_storage = cache_storage
         self.job = job
         self.playwright = playwright
 
     def __enter__(self) -> 'JobState':
         """Context manager invoked on entry to the body of a with statement to make it possible to factor out standard
-        uses of try/finally statements. Calls the main_thread_enter() method of the Job.
+        uses of try/finally statements. Calls the main_threa_enter() method of the Job.
 
         :returns: Class object.
         """
         # Below is legacy code that now does nothing so it's being skipped
         # try:
         #     self.job.main_thread_enter()
         # except Exception as ex:
@@ -122,30 +131,30 @@
             guid=self.job.get_guid(),
             data=self.new_data,
             timestamp=self.new_timestamp,
             tries=self.tries,
             etag=self.new_etag,
         )
 
-    def process(self) -> 'JobState':
+    def process(self, headless: bool = True) -> 'JobState':
         """Processes the job: loads it (i.e. runs it) and handles Exceptions (errors).
 
         :returns: a JobState object containing information of the job run.
         """
         logger.info(f'Job {self.job.index_number}: Processing job {self.job}')
 
         if self.exception:
             return self
 
         try:
             try:
                 self.load()
 
                 self.new_timestamp = time.time()
-                data, self.new_etag = self.job.retrieve(self)
+                data, self.new_etag = self.job.retrieve(self, headless)
 
                 # Apply automatic filters first
                 filtered_data = FilterBase.auto_process(self, data)
 
                 # Apply any specified filters
                 for filter_kind, subfilter in FilterBase.normalize_filter_list(self.job.filter):
                     filtered_data = FilterBase.process(filter_kind, subfilter, self, filtered_data)
@@ -192,15 +201,34 @@
             # Apply any specified diff_filters
             for filter_kind, subfilter in FilterBase.normalize_filter_list(self.job.diff_filter):
                 _generated_diff = FilterBase.process(filter_kind, subfilter, self, _generated_diff)
         self._generated_diff = _generated_diff
 
         return self._generated_diff
 
-    def _generate_diff(self, tz: Optional[str] = None) -> str:
+    def get_diff_html(self, tz: Optional[str] = None) -> str:
+        """Generates the job's HTML diff and applies diff_filters to it (if any). Memoized.
+
+        :parameter tz: The IANA tz_info name of the timezone to use for diff in the job's report (e.g. 'Etc/UTC').
+        :returns: The job's diff.
+        """
+        # Must be initialized as None
+        if self._generated_diff_html is not None:
+            return self._generated_diff_html
+
+        _generated_diff_html = self._generate_diff(tz, html_out=True)
+        if _generated_diff_html:
+            # Apply any specified diff_filters
+            for filter_kind, subfilter in FilterBase.normalize_filter_list(self.job.diff_filter):
+                _generated_diff_html = FilterBase.process(filter_kind, subfilter, self, _generated_diff_html)
+        self._generated_diff_html = _generated_diff_html
+
+        return self._generated_diff_html
+
+    def _generate_diff(self, tz: Optional[str] = None, html_out: bool = False) -> str:
         """Generates the job's diff.
 
         :parameter tz: The IANA tz_info name of the timezone to use for diff in the job's report (e.g. 'Etc/UTC') (
            not used if an external diff tool is used.
         :returns: An empty string if there is no change, otherwise the diff.
         :raises RuntimeError: If the external diff tool returns an error.
         """
@@ -224,37 +252,68 @@
             if self.job.diff_tool.startswith('deepdiff'):
                 if DeepDiff is None:
                     raise ImportError(
                         f"Python package 'deepdiff' is not installed; cannot use 'diff_tool: {self.job.diff_tool}'"
                         f' ({self.job.get_indexed_location()})'
                     )
 
-                def _pretty(diff: DeepDiff) -> str:
+                def _pretty_deepdiff(diff: DeepDiff, html_out: bool = False) -> str:
                     """
                     Customized version of deepdiff.base.pretty() edited to add the values deleted or added.
                     The pretty human readable string output for the diff object
                     regardless of what view was used to generate the diff.
                     """
 
-                    PRETTY_FORM_TEXTS = {
-                        'type_changes': (
-                            'Type of {diff_path} changed from {type_t1} to {type_t2} and value changed '
-                            'from {val_t1} to {val_t2}.'
-                        ),
-                        'values_changed': 'Value of {diff_path} changed from {val_t1} to {val_t2}.',
-                        'dictionary_item_added': 'Item {diff_path} added to dictionary as {val_t2}.',
-                        'dictionary_item_removed': 'Item {diff_path} removed from dictionary (was {val_t1}).',
-                        'iterable_item_added': 'Item {diff_path} added to iterable as {val_t2}.',
-                        'iterable_item_removed': 'Item {diff_path} removed from iterable (was {val_t1}).',
-                        'attribute_added': 'Attribute {diff_path} added as {val_t2}.',
-                        'attribute_removed': 'Attribute {diff_path} removed (was {val_t1}).',
-                        'set_item_added': 'Item root[{val_t2}] added to set as {val_t1}.',
-                        'set_item_removed': 'Item root[{val_t1}] removed from set (was {val_t2}).',
-                        'repetition_change': 'Repetition change for item {diff_path} ({val_t2}).',
-                    }
+                    if html_out:
+                        added = '<span style="background-color:#d1ffd1;color:#082b08">'
+                        delte = '<span style="background-color:#fff0f0;color:#9c1c1c;text-decoration:line-through">'
+                        PRETTY_FORM_TEXTS = {
+                            'type_changes': (
+                                'Type of {diff_path} changed from {type_t1} to {type_t2} and value changed '
+                                f'from {delte}{{val_t1}}</span> to {added}{{val_t2}}</span>.'
+                            ),
+                            'values_changed': (
+                                f'Value of {{diff_path}} changed from {delte}{{val_t1}}</span> to {added}{{val_t2}}'
+                                f'</span>.'
+                            ),
+                            'dictionary_item_added': (
+                                f'Item {{diff_path}} added to dictionary as {added}{{val_t2}}</span>.'
+                            ),
+                            'dictionary_item_removed': (
+                                f'Item {{diff_path}} removed from dictionary (was {delte}{{val_t1}}</span>).'
+                            ),
+                            'iterable_item_added': f'Item {{diff_path}} added to iterable as {added}{{val_t2}}</span>.',
+                            'iterable_item_removed': (
+                                f'Item {{diff_path}} removed from iterable (was {delte}{{val_t1}}</span>).'
+                            ),
+                            'attribute_added': f'Attribute {{diff_path}} added as {added}{{val_t2}}</span>.',
+                            'attribute_removed': f'Attribute {{diff_path}} removed (was {delte}{{val_t1}}</span>).',
+                            'set_item_added': f'Item root[{{val_t2}}] added to set as {added}{{val_t1}}</span>.',
+                            'set_item_removed': (
+                                f'Item root[{{val_t1}}] removed from set (was {delte}{{val_t2}}</span>).'
+                            ),
+                            'repetition_change': 'Repetition change for item {diff_path} ({val_t2}).',
+                        }
+                    else:
+                        PRETTY_FORM_TEXTS = {
+                            'type_changes': (
+                                'Type of {diff_path} changed from {type_t1} to {type_t2} and value changed '
+                                'from {val_t1} to {val_t2}.'
+                            ),
+                            'values_changed': 'Value of {diff_path} changed from {val_t1} to {val_t2}.',
+                            'dictionary_item_added': 'Item {diff_path} added to dictionary as {val_t2}.',
+                            'dictionary_item_removed': 'Item {diff_path} removed from dictionary (was {val_t1}).',
+                            'iterable_item_added': 'Item {diff_path} added to iterable as {val_t2}.',
+                            'iterable_item_removed': 'Item {diff_path} removed from iterable (was {val_t1}).',
+                            'attribute_added': 'Attribute {diff_path} added as {val_t2}.',
+                            'attribute_removed': 'Attribute {diff_path} removed (was {val_t1}).',
+                            'set_item_added': 'Item root[{val_t2}] added to set as {val_t1}.',
+                            'set_item_removed': 'Item root[{val_t1}] removed from set (was {val_t2}).',
+                            'repetition_change': 'Repetition change for item {diff_path} ({val_t2}).',
+                        }
 
                     def _pretty_print_diff(diff: DeepDiff) -> str:
                         type_t1 = type(diff.t1).__name__
                         type_t2 = type(diff.t2).__name__
 
                         val_t1 = (
                             f'"{diff.t1}"'
@@ -283,30 +342,45 @@
                     result = []
                     for key in diff.tree.keys():
                         for item_key in diff.tree[key]:
                             result.append(_pretty_print_diff(item_key))
 
                     return '\n'.join(result)
 
-                data_type = self.job.diff_tool.split()[1] if len(self.job.diff_tool.split()) > 1 else 'json'
+                data_type = self.job.diff_tool.split('-')[1] if len(self.job.diff_tool.split('-')) > 1 else 'json'
                 if data_type == 'json':
-                    old_data = json.loads(self.old_data)
+                    try:
+                        old_data = json.loads(self.old_data)
+                    except json.JSONDecodeError:
+                        old_data = ''
                     new_data = json.loads(self.new_data)
+                elif data_type == 'xml':
+                    if xmltodict is None:
+                        raise ImportError(
+                            f"Python package 'xmltodict' is not installed; cannot use 'diff_tool: {self.job.diff_tool}'"
+                            f' ({self.job.get_indexed_location()})'
+                        )
+                    old_data = xmltodict.parse(self.old_data)
+                    new_data = xmltodict.parse(self.new_data)
                 else:
                     raise NotImplementedError(
                         f"data_type '{data_type}' is not supported by 'diff_tool: deepdiff'"
                         f' ({self.job.get_indexed_location()})'
                     )
                 diff = DeepDiff(old_data, new_data, verbose_level=2)
+                diff_text = _pretty_deepdiff(diff, html_out)
+                if not diff_text:
+                    self.verb = 'changed,no_report'
+                    return ''
                 head = (
                     f'Using {self.job.diff_tool}\n'
                     f'Old: {timestamp_old}\n'
                     f'New: {timestamp_new}\n' + '-' * 36 + '\n'
                 )
-                return head + _pretty(diff)
+                return head + diff_text
 
             else:
                 # External diff tool
                 with tempfile.TemporaryDirectory() as tmp_dir:
                     tmp_path = Path(tmp_dir)
                     old_file_path = tmp_path.joinpath('old_file')
                     new_file_path = tmp_path.joinpath('new_file')
@@ -337,14 +411,17 @@
                 '@',
                 timestamp_old,
                 timestamp_new,
                 contextlines,
                 lineterm='',
             )
         )
+        if not diff:
+            self.verb = 'changed,no_report'
+            return ''
         diff[0] = diff[0].replace('\t', ' ')
         diff[1] = diff[1].replace('\t', ' ')
         if self.job.additions_only:
             if len(self.old_data) and len(self.new_data) / len(self.old_data) <= 0.25:
                 diff = (
                     diff[:2]
                     + ['/**Comparison type: Additions only**']
@@ -447,27 +524,28 @@
         self._result('error', job_state)
 
     def custom(self, job_state: JobState, label: str) -> None:
         """Sets the verb of the job in job_state to a custom label. Called by
         :py:func:`UrlwatchCommand.check_test_reporter`.
 
         :param job_state: The JobState object with the information of the job run.
+        :param label: The label to set the information of the job run to.
         """
         self._result(label, job_state)
 
     def get_filtered_job_states(self, job_states: List[JobState]) -> Iterable[JobState]:
         """Returns JobStates that have reportable changes per config['display'].  Called from :py:Class:`ReporterBase`.
 
         :param job_states: The list of JobState objects with the information of the job runs.
         :returns: An iterable of JobState objects that have reportable changes per config['display'].
         """
         for job_state in job_states:
             if (
                 not any(
-                    job_state.verb == verb and not self.config['display'][verb]  # type: ignore[misc]
+                    job_state.verb == verb and not self.config['display'][verb]  # type: ignore[literal-required]
                     for verb in ('unchanged', 'new', 'error')
                 )
                 and job_state.verb != 'changed,no_report'
             ):
                 yield job_state
 
     def finish(self, jobs_file: Optional[Path] = None) -> None:
```

## webchanges/jobs.py

```diff
@@ -1,9 +1,11 @@
 """Jobs."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from __future__ import annotations
 
 import asyncio
 import copy
 import email.utils
 import hashlib
 import html
@@ -16,15 +18,15 @@
 import textwrap
 import time
 import warnings
 from ftplib import FTP  # nosec: B402
 from http.client import responses as response_names
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING, Union
-from urllib.parse import quote, SplitResult, SplitResultBytes, urldefrag, urlencode, urlparse, urlsplit
+from urllib.parse import parse_qsl, quote, SplitResult, SplitResultBytes, urldefrag, urlencode, urlparse, urlsplit
 
 import html2text
 import requests
 from requests.packages.urllib3.exceptions import InsecureRequestWarning
 from requests.structures import CaseInsensitiveDict
 
 from . import __user_agent__
@@ -78,29 +80,14 @@
                 f'{self.__class__.__name__}: Received response HTTP {self.status_code} '
                 f'{response_names[self.status_code]}'
             )
         else:
             return self.args[0]
 
 
-class ShellError(Exception):
-    """Raised by 'command' jobs when a non-zero execution exit code is received."""
-
-    def __init__(self, result: str) -> None:
-        """
-
-        :param result: The contents of stderr.
-        """
-        Exception.__init__(self)
-        self.result = result
-
-    def __str__(self) -> str:
-        return f'{self.__class__.__name__}: Exit status {self.result}'
-
-
 class JobBase(object, metaclass=TrackSubClasses):
     """The base class for Jobs."""
 
     __subclasses__: Dict[str, 'JobBase'] = {}
     __anonymous_subclasses__: List['JobBase'] = []
 
     __kind__: str = ''  # no longer set at the subclass level
@@ -125,35 +112,39 @@
     data: Union[str, Dict[str, str]] = None  # type: ignore[assignment]
     deletions_only: Optional[bool] = None
     diff_filter: Union[str, List[Union[str, Dict[str, Any]]]] = None  # type: ignore[assignment]
     diff_tool: Optional[str] = None
     encoding: Optional[str] = None
     filter: Union[str, List[Union[str, Dict[str, Any]]]] = None  # type: ignore[assignment]
     headers: Optional[Union[dict, CaseInsensitiveDict]] = None
-    headless: Optional[bool] = None  # Playwright
     http_proxy: Optional[str] = None
     https_proxy: Optional[str] = None
     ignore_cached: Optional[bool] = None
     ignore_connection_errors: Optional[bool] = None
+    ignore_dh_key_too_small: Optional[bool] = None
     ignore_http_error_codes: Optional[bool] = None
     ignore_https_errors: Optional[bool] = None
     ignore_timeout_errors: Optional[bool] = None
     ignore_too_many_redirects: Optional[bool] = None
-    monospace: Optional[bool] = None
+    initialization_js: Optional[str] = None  # Playwright
+    initialization_url: Optional[str] = None  # Playwright
     is_markdown: Optional[bool] = None
+    kind: Optional[str] = None  # hooks.py
     loop: Optional[asyncio.AbstractEventLoop] = None
     markdown_padded_tables: Optional[bool] = None
     max_tries: Optional[int] = None
     method: Optional[Literal['GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE']] = None
+    monospace: Optional[bool] = None
     name: Optional[str] = None
     navigate: Optional[str] = None  # backwards compatibility (deprecated)
     no_redirects: Optional[bool] = None
     note: Optional[str] = None
     referer: Optional[str] = None  # Playwright
     ssl_no_verify: Optional[bool] = None
+    stderr: Optional[str] = None  # ShellJob backwards compatibility (not used)
     switches: Optional[List[str]] = None
     timeout: Optional[float] = None
     user_data_dir: Optional[str] = None
     user_visible_url: Optional[str] = None
     wait_for: Optional[Union[int, str]] = None  # pyppeteer only?
     wait_for_function: Optional[str] = None  # Playwright
     wait_for_navigation: Optional[Union[str, Tuple[str, ...]]] = None  # pyppeteer only?
@@ -237,46 +228,47 @@
             warnings.warn(
                 f"Job directive 'navigate' is deprecated: replace with 'url' and add 'use_browser: true' ({data})",
                 DeprecationWarning,
             )
             data['url'] = data.get('url', data['navigate'])
             data['use_browser'] = True
 
-        # Backwards compatibility with 'kind' directive (deprecated)
         if 'kind' in data:
-            warnings.warn(
-                f"Job directive 'kind' is deprecated and ignored: delete from job ({data})",  # nosec: B608
-                DeprecationWarning,
-            )
-            data.pop('kind')
-
-        # Auto-detect the job subclass based on required directives
-        matched_subclasses = [
-            subclass
-            for subclass in list(cls.__subclasses__.values())[1:]
-            if all(data.get(required) for required in subclass.__required__)
-        ]
-
-        if len(matched_subclasses) == 1:
-            job_subclass = matched_subclasses[0]
-        elif len(matched_subclasses) > 1:
-            number_matched: Dict[JobBase, int] = {}
-            for match in matched_subclasses:
-                number_matched[match] = [data.get(required) is not None for required in match.__required__].count(True)
-            # noinspection PyUnresolvedReferences
-            job_subclass = sorted(number_matched.items(), key=lambda x: x[1], reverse=True)[0][0]
+            # Used for hooks.py.
+            try:
+                job_subclass = cls.__subclasses__[data['kind']]
+            except KeyError:
+                raise ValueError(f"Job directive 'kind' ({data['kind']}) does not match any known job kinds:\n{data}")
         else:
-            if len(data) == 1:
-                raise ValueError(
-                    f"Job directive has no value or doesn't match a job type; check for errors/typos/escaping:\n{data}"
-                )
+            # Auto-detect the job subclass based on required directives.
+            matched_subclasses = [
+                subclass
+                for subclass in list(cls.__subclasses__.values())[1:]
+                if all(data.get(required) for required in subclass.__required__)
+            ]
+            if len(matched_subclasses) == 1:
+                job_subclass = matched_subclasses[0]
+            elif len(matched_subclasses) > 1:
+                number_matched: Dict[JobBase, int] = {}
+                for match in matched_subclasses:
+                    number_matched[match] = [data.get(required) is not None for required in match.__required__].count(
+                        True
+                    )
+                # noinspection PyUnresolvedReferences
+                job_subclass = sorted(number_matched.items(), key=lambda x: x[1], reverse=True)[0][0]
             else:
-                raise ValueError(
-                    f"Job directives (with values) don't match a job type; check for errors/typos/escaping:\n{data}"
-                )
+                if len(data) == 1:
+                    raise ValueError(
+                        f"Job directive has no value or doesn't match a job type; check for errors/typos/escaping:\n"
+                        f'{data}'
+                    )
+                else:
+                    raise ValueError(
+                        f"Job directives (with values) don't match a job type; check for errors/typos/escaping:\n{data}"
+                    )
 
         # Remove extra required directives ("Falsy")
         other_subclasses = list(cls.__subclasses__.values())[1:]
         other_subclasses.remove(job_subclass)
         for other_subclass in other_subclasses:
             for k in other_subclass.__required__:
                 if k not in job_subclass.__required__:
@@ -342,29 +334,34 @@
 
         if isinstance(defaults, dict):
             # merge defaults from configuration (including dicts) into Job attributes without overwriting them
             for key, value in defaults.items():
                 if key in self.__optional__:
                     if getattr(self, key) is None:
                         setattr(self, key, value)
-                    elif isinstance(defaults[key], dict) and isinstance(
+                    elif isinstance(defaults[key], (dict, CaseInsensitiveDict)) and isinstance(
                         getattr(self, key), (dict, CaseInsensitiveDict)
                     ):
                         for subkey, subvalue in defaults[key].items():
                             if hasattr(self, key) and subkey not in getattr(self, key):
                                 getattr(self, key)[subkey] = subvalue
 
     def with_defaults(self, config: Config) -> 'JobBase':
         """Obtain a Job object that also contains defaults from the configuration.
 
         :param config: The configuration as a dict.
         :returns: A JobBase object.
         """
         job_with_defaults = copy.deepcopy(self)
         if job_with_defaults.headers:
+            if not isinstance(job_with_defaults.headers, dict):
+                raise ValueError(
+                    f"Error reading jobs file: 'headers' directive must be a dictionary in job "
+                    f'{job_with_defaults.url or job_with_defaults.command}'
+                )
             job_with_defaults.headers = CaseInsensitiveDict(job_with_defaults.headers)
         cfg = config.get('job_defaults')
         if isinstance(cfg, dict):
             if 'headers' in cfg.get('all', {}):
                 cfg['all']['headers'] = CaseInsensitiveDict(cfg['all']['headers'])
             if 'headers' in cfg.get('url', {}):
                 cfg['url']['headers'] = CaseInsensitiveDict(cfg['url']['headers'])
@@ -378,18 +375,19 @@
         """Calculate the GUID, currently a simple SHA1 hash of the location (URL or command).
 
         :returns: the GUID.
         """
         location = self.get_location()
         return hashlib.sha1(location.encode()).hexdigest()  # nosec: B303
 
-    def retrieve(self, job_state: JobState) -> Tuple[Union[str, bytes], str]:
+    def retrieve(self, job_state: JobState, headless: bool = True) -> Tuple[Union[str, bytes], str]:
         """Runs job to retrieve the data, and returns data and ETag.
 
-        :param job_state: The JobState object, to keep track of the sate of the retrieval.
+        :param job_state: The JobState object, to keep track of the state of the retrieval.
+        :param headless: For browser-based jobs, whether headless mode should be used.
         :returns: The data retrieved and the ETag.
         """
         raise NotImplementedError()
 
     def main_thread_enter(self) -> None:
         """Called from the main thread before running the job. No longer needed (does nothing)."""
         ...
@@ -418,14 +416,15 @@
 
 
 class Job(JobBase):
     """Job class for jobs."""
 
     __required__: Tuple[str, ...] = ()
     __optional__: Tuple[str, ...] = (
+        'kind',  # hooks.py
         'index_number',
         'name',
         'note',
         'additions_only',
         'contextlines',
         'deletions_only',
         'diff_filter',
@@ -455,18 +454,19 @@
     def pretty_name(self) -> str:
         """Get the 'pretty name' of a job, i.e. either its 'name' (if defined) or the 'location' (URL or command).
 
         :returns: A string with the 'pretty name' the job.
         """
         return self.name or self.get_location()
 
-    def retrieve(self, job_state: JobState) -> Tuple[Union[str, bytes], str]:
+    def retrieve(self, job_state: JobState, headless: bool = True) -> Tuple[Union[str, bytes], str]:
         """Runs job to retrieve the data, and returns data and ETag.
 
-        :param job_state: The JobState object, to keep track of the sate of the retrieval.
+        :param job_state: The JobState object, to keep track of the state of the retrieval.
+        :param headless: For browser-based jobs, whether headless mode should be used.
         :returns: The data retrieved and the ETag.
         """
         pass
 
 
 CHARSET_RE = re.compile('text/(html|plain); charset=([^;]*)')
 
@@ -493,31 +493,33 @@
         'cookies',
         'data',
         'encoding',
         'headers',
         'http_proxy',
         'https_proxy',
         'ignore_cached',
+        'ignore_dh_key_too_small',
         'method',
         'no_redirects',
         'ssl_no_verify',
         'timeout',
     )
 
     def get_location(self) -> str:
         """Get the 'location' of a job, i.e. the URL or command.
 
         :returns: A string with user_visible_url or the URL of the job.
         """
         return self.user_visible_url or self.url
 
-    def retrieve(self, job_state: JobState) -> Tuple[Union[str, bytes], str]:
+    def retrieve(self, job_state: JobState, headless: bool = True) -> Tuple[Union[str, bytes], str]:
         """Runs job to retrieve the data, and returns data and ETag.
 
-        :param job_state: The JobState object, to keep track of the sate of the retrieval.
+        :param job_state: The JobState object, to keep track of the state of the retrieval.
+        :param headless: For browser-based jobs, whether headless mode should be used.
         :returns: The data retrieved and the ETag.
         :raises NotModifiedError: If an HTTP 304 response is received.
         """
         if self._delay:
             logger.debug(f'Delaying for {self._delay} seconds (duplicate network location)')
             time.sleep(self._delay)
 
@@ -618,14 +620,22 @@
         else:
             timeout = self.timeout
 
         # cookiejar (called by requests) expects strings or bytes-like objects; PyYAML will try to guess int etc.
         if self.cookies:
             self.cookies = {k: str(v) for k, v in self.cookies.items()}
 
+        if self.ignore_dh_key_too_small:
+            # https://stackoverflow.com/questions/38015537
+            logger.debug(
+                'Setting default cipher list to ciphers that do not make any use of Diffie Hellman Key Exchange and '
+                "thus not affected by the server's weak DH key"
+            )
+            requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += 'HIGH:!DH:!aNULL'  # type: ignore[attr-defined]
+
         response = requests.request(
             method=self.method,
             url=self.url,
             data=self.data,
             headers=headers,
             cookies=self.cookies,
             timeout=timeout,
@@ -735,18 +745,19 @@
     __optional__ = (
         '_beta_use_playwright',
         'block_elements',
         'chromium_revision',
         'cookies',
         'data',
         'headers',
-        'headless',  # Playwright
         'http_proxy',
         'https_proxy',
         'ignore_https_errors',
+        'initialization_js',  # Playwright
+        'initialization_url',  # Playwright  # TODO documentation
         'method',
         'navigate',
         'switches',
         'timeout',
         'user_data_dir',
         'wait_for',  # pyppeteer
         'wait_for_function',  # Playwright
@@ -765,39 +776,40 @@
     def get_location(self) -> str:
         """Get the 'location' of a job, i.e. the URL or command.
 
         :returns: A string with user_visible_url or the URL of the job.
         """
         return self.user_visible_url or self.url
 
-    def retrieve(self, job_state: JobState) -> Tuple[Union[str, bytes], str]:
+    def retrieve(self, job_state: JobState, headless: bool = True) -> Tuple[Union[str, bytes], str]:
         """Runs job to retrieve the data, and returns data and ETag.
 
-        :param job_state: The JobState object, to keep track of the sate of the retrieval.
+        :param job_state: The JobState object, to keep track of the state of the retrieval.
+        :param headless: For browser-based jobs, whether headless mode should be used.
         :returns: The data retrieved and the ETag.
         """
         if self._delay:
             logger.debug(f'Delaying for {self._delay} seconds (duplicate network location)')
             time.sleep(self._delay)
 
         if self._beta_use_playwright:
-            response, etag = self._playwright_retrieve(job_state)
+            response, etag = self._playwright_retrieve(job_state, headless)
         else:
-            response, etag = asyncio.run(self._retrieve(job_state))
+            response, etag = asyncio.run(self._retrieve(job_state))  # pragma: no cover
 
         # if no name directive is given, set it to the title element if found in HTML or XML truncated to 60 characters
         if not self.name:
             title = re.findall(r'<title.*?>(.+?)</title>', response)
             if title:
                 self.name = html.unescape(title[0])[:60]
 
         return response, etag
 
     @staticmethod
-    def current_platform() -> str:
+    def current_platform() -> str:  # pragma: no cover
         """Get current platform name by short string as used by Pyppeteer for downloading Chromium.
         Code originally from pyppeteer.chromium_downloader, but we cannot simply import it as it will trigger
         pyppeteer reading os.environ['PYPPETEER_CHROMIUM_REVISION'] before we can modify it ourselves.
 
         :raises OSError: If the platform is not supported by Pyppeteer.
         """
         if sys.platform.startswith('linux'):
@@ -806,15 +818,15 @@
             return 'mac'
         elif sys.platform.startswith('win') or sys.platform.startswith('msys') or sys.platform.startswith('cyg'):
             if sys.maxsize > 2 ** 31 - 1:
                 return 'win64'
             return 'win32'
         raise OSError(f'Platform unsupported by Pyppeteer (use_browser: true): {sys.platform}')
 
-    async def _retrieve(self, job_state: JobState) -> Tuple[str, str]:
+    async def _retrieve(self, job_state: JobState) -> Tuple[str, str]:  # pragma: no cover
         """
 
         :raises KeyError: If chromium_revision is specified but not for the current operating system.
         :raises ValueError: If there is a problem with the value supplied in one of the keys in the configuration file.
         :raises TypeError: If the value provided in one of the directives is not in the correct type.
         :raises ImportError: If the pyppeteer package is not installed.
         :raises BrowserResponseError: If an HTTP response code between 400 and 599 is received.
@@ -1074,15 +1086,15 @@
             logger.info(
                 f'Job {self.index_number}: Received response HTTP {response_code} {response_names[response_code]}'
             )
             logger.debug(f'Job {self.index_number}: Response headers {response_headers}')
 
         return content, etag
 
-    def _playwright_retrieve(self, job_state: JobState) -> Tuple[str, str]:
+    def _playwright_retrieve(self, job_state: JobState, headless: bool = True) -> Tuple[str, str]:
         """
 
         :raises ValueError: If there is a problem with the value supplied in one of the keys in the configuration file.
         :raises TypeError: If the value provided in one of the directives is not of the correct type.
         :raises ImportError: If the playwright package is not installed.
         :raises BrowserResponseError: If a browser error or an HTTP response code between 400 and 599 is received.
         """
@@ -1169,44 +1181,58 @@
 
         timeout = self.timeout * 1000 if self.timeout else 75000  # Browser could be slow to launch
 
         # launch browser
         with sync_playwright() as p:
             executable_path = os.getenv('WEBCHANGES_BROWSER_PATH')
             channel = None if executable_path else 'chrome'
+            no_viewport = False if not self.switches else any('--window-size' in switch for switch in self.switches)
             if not self.user_data_dir:
                 browser = p.chromium.launch(
                     executable_path=executable_path,  # type: ignore[arg-type]
                     channel=channel,  # type: ignore[arg-type]
                     args=args,  # type: ignore[arg-type]
                     timeout=timeout,  # type: ignore[arg-type]
-                    headless=self.headless,  # type: ignore[arg-type]
+                    headless=headless,  # type: ignore[arg-type]
                     proxy=proxy,  # type: ignore[arg-type]
                 )
+                if 'User-Agent' not in headers:
+                    headers['User-Agent'] = (
+                        f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
+                        f'Chrome/{browser.version} Safari/537.36'
+                    )
                 context = browser.new_context(
+                    no_viewport=no_viewport,
                     ignore_https_errors=self.ignore_https_errors,  # type: ignore[arg-type]
                     extra_http_headers=dict(headers),
                 )
-                logger.debug(f'Job {self.index_number}: Browser {channel} {browser.version} launched')
+                logger.debug(
+                    f'Job {self.index_number}: Launched browser {channel or executable_path} version {browser.version}'
+                )
             else:
                 context = p.chromium.launch_persistent_context(
                     user_data_dir=self.user_data_dir,
                     channel=channel,  # type: ignore[arg-type]
                     executable_path=executable_path,  # type: ignore[arg-type]
                     args=args,  # type: ignore[arg-type]
                     # handle_sigint=False,
                     # handle_sigterm=False,
                     # handle_sighup=False,
                     timeout=timeout,  # type: ignore[arg-type]
-                    headless=self.headless,  # type: ignore[arg-type]
+                    headless=headless,
                     proxy=proxy,  # type: ignore[arg-type]
+                    no_viewport=no_viewport,
                     ignore_https_errors=self.ignore_https_errors,  # type: ignore[arg-type]
                     extra_http_headers=dict(headers),
                 )
-                logger.debug(f'Job {self.index_number}: Browser launched from {executable_path}')
+                logger.debug(
+                    f'Job {self.index_number}: Launched browser {channel or executable_path} version'
+                    f' {context.browser.version} with user data directory '  # type: ignore[union-attr]
+                    f'{self.user_data_dir}'
+                )
 
             # # launch playwright (memoized)
             # if self._playwright is None:
             #     logger.info('Starting the instance of playwright')
             #     self._playwright = sync_playwright().start()  # TODO this should be in a context manager with .stop()
 
             # # launch browser (memoized)
@@ -1236,27 +1262,59 @@
             #
             # context = browser.new_context(
             #     ignore_https_errors=self.ignore_https_errors,
             #     extra_http_headers=dict(headers),
             #     proxy=proxy,
             # )
 
-            if 'User-Agent' not in headers:
-                headers['User-Agent'] = (
-                    f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
-                    f'Chrome/{browser.version} Safari/537.36'
-                )
-
             # open a page
             page = context.new_page()
 
+            if self.initialization_url:
+                logger.info(f'Job {self.index_number}: Initializing website by navigating to {self.initialization_url}')
+                try:
+                    response = page.goto(
+                        self.initialization_url,
+                        timeout=timeout,
+                    )
+                except PlaywrightError as e:
+                    context.close()
+                    logger.error(f'Job {self.index_number}: Website initialization page returned error' f' {e.args[0]}')
+                    raise e
+
+                if not response:
+                    context.close()
+                    raise BrowserResponseError(('No response received from browser',), None)
+
+                if self.initialization_js:
+                    logger.info(f'Job {self.index_number}: Running init script {self.initialization_js}')
+                    page.evaluate(self.initialization_js)
+                    if self.wait_for_url:
+                        logger.info(f'Job {self.index_number}: Waiting for url {self.wait_for_url}')
+                        response.frame.wait_for_url(
+                            self.wait_for_url, wait_until=self.wait_until, timeout=timeout  # type: ignore[arg-type]
+                        )
+                updated_url = page.url
+                params = dict(parse_qsl(urlparse(updated_url).params))
+                try:
+                    new_url = self.url.format(**params)
+                except KeyError as e:
+                    browser.close()
+                    raise ValueError(
+                        f"Job {job_state.job.index_number}: Directive 'initialization_url' did not find key"
+                        f" {e.args[0]} to substitute in 'url'"
+                    )
+                if new_url != self.url:
+                    self.url = new_url
+                    logger.info(f'Job {self.index_number}: URL updated to {self.url}')
+
             if self.method and self.method != 'GET':
 
                 def handle_route(route: Route) -> None:
-                    """Handler function to change the route (probably a pyee.EventEmitter callback)."""
+                    """Handler function to change the route (a pyee.EventEmitter callback)."""
                     logger.info(f'Job {self.index_number}: Intercepted route to change request method to {self.method}')
                     route.continue_(method=str(self.method), post_data=data)  # type: ignore[arg-type]
 
                 page.route(self.url, handler=handle_route)
 
             # if self.block_elements and not self.method or self.method == 'GET':
             #     # FIXME: Pyppeteer freezes on certain sites if this is on; contribute if you know why
@@ -1324,15 +1382,15 @@
                     wait_until=self.wait_until,  # type: ignore[arg-type]
                     referer=self.referer,  # type: ignore[arg-type]
                     timeout=timeout,
                 )
             except PlaywrightError as e:
                 logger.error(f'Job {self.index_number}: Page returned error {e.args[0]}')
                 context.close()
-                raise e
+                raise BrowserResponseError(e.args, None)
 
             if not response:
                 context.close()
                 raise BrowserResponseError(('No response received from browser',), None)
 
             # wait_for-*
             if self.wait_for_url:
@@ -1514,33 +1572,32 @@
 
 class ShellJob(Job):
     """Run a shell command and get its standard output."""
 
     __kind__ = 'shell'
 
     __required__ = ('command',)
-    __optional__ = ()
+    __optional__ = ('stderr',)  # ignored; here for backwards compatibility
 
     def get_location(self) -> str:
         """Get the 'location' of a job, i.e. the URL or command.
 
         :returns: A string with user_visible_url or the command of the job.
         """
         return self.user_visible_url or self.command
 
-    def retrieve(self, job_state: JobState) -> Tuple[Union[str, bytes], str]:
-        """Runs job to retrieve the data, and returns data and ETag.
+    def retrieve(self, job_state: JobState, headless: bool = True) -> Tuple[Union[str, bytes], str]:
+        """Runs job to retrieve the data, and returns data and ETag (which is blank).
 
-        :param job_state: The JobState object, to keep track of the sate of the retrieval.
+        :param job_state: The JobState object, to keep track of the state of the retrieval.
+        :param headless: For browser-based jobs, whether headless mode should be used.
         :returns: The data retrieved and the ETag.
-        :raises ShellError: If the subprocess fails.
+        :raises subprocess.CalledProcessError: Subclass of SubprocessError, raised when a process returns a non-zero
+           exit status.
+        :raises subprocess.TimeoutExpired: Subclass of SubprocessError, raised when a timeout expires while waiting for
+           a child process.
         """
         needs_bytes = FilterBase.filter_chain_needs_bytes(self.filter)
-        try:
-            return (
-                subprocess.run(
-                    self.command, capture_output=True, shell=True, check=True, text=(not needs_bytes)
-                ).stdout,
-                '',
-            )  # noqa: DUO116 use of "shell=True" is insecure
-        except subprocess.CalledProcessError as e:
-            raise ShellError(e.stderr).with_traceback(e.__traceback__)
+        return (
+            subprocess.run(self.command, capture_output=True, shell=True, check=True, text=(not needs_bytes)).stdout,
+            '',
+        )  # noqa: DUO116 use of "shell=True" is insecure
```

## webchanges/mailer.py

```diff
@@ -1,9 +1,11 @@
 """Email handler."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 import getpass
 import logging
 import smtplib
 import subprocess
 from email import policy
 from email.message import EmailMessage
 from pathlib import Path
@@ -29,24 +31,24 @@
 
     @staticmethod
     def msg(
         from_email: str, to_email: str, subject: str, text_body: str, html_body: Optional[str] = None
     ) -> EmailMessage:
         """Create an Email object for a message.
 
-        :param from_email: The 'from' email address
-        :param to_email: The 'to' email address
-        :param subject: The 'subject' of the email
+        :param from_email: The 'From' email address
+        :param to_email: The 'To' email address
+        :param subject: The 'Subject' of the email
         :param text_body: The body in text format
         :param html_body: The body in html format (optional)
         """
         msg = EmailMessage(policy=policy.SMTPUTF8)
-        msg['from'] = from_email
-        msg['to'] = to_email
-        msg['subject'] = subject
+        msg['From'] = from_email
+        msg['To'] = to_email
+        msg['Subject'] = subject
         msg.set_content(text_body, subtype='plain')
         if html_body is not None:
             msg.add_alternative(html_body, subtype='html')
 
         return msg
```

## webchanges/main.py

```diff
@@ -1,12 +1,14 @@
 """The main class.
 
 For the entrypoint, see cli.py.
 """
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 import logging
 from typing import List, Optional, Tuple, Union
 
 from .config import CommandConfig
 from .handler import Report
 from .jobs import JobBase
 from .storage import CacheStorage, YamlConfigStorage, YamlJobsStorage
```

## webchanges/reporters.py

```diff
@@ -1,9 +1,11 @@
 """Runs reports."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from __future__ import annotations
 
 import asyncio
 import difflib
 import functools
 import getpass
 import html
@@ -35,14 +37,15 @@
 
     from .handler import JobState, Report
     from .jobs import JobBase
 
     # TypedDicts only work on Python >= 3.8
     from .storage import (
         ConfigReportBrowser,
+        ConfigReportDiscord,
         ConfigReportEmail,
         ConfigReportIfttt,
         ConfigReportMailgun,
         ConfigReportMatrix,
         ConfigReportProwl,
         ConfigReportPushbullet,
         ConfigReportPushover,
@@ -50,27 +53,28 @@
         ConfigReportStdout,
         ConfigReportTelegram,
         ConfigReportWebhook,
         ConfigReportXmpp,
     )
 
     ConfigReportersList = Union[
-        ConfigReportStdout,
         ConfigReportBrowser,
+        ConfigReportDiscord,
         ConfigReportEmail,
-        ConfigReportPushover,
+        ConfigReportIfttt,
+        ConfigReportMailgun,
+        ConfigReportMatrix,
+        ConfigReportProwl,
         ConfigReportPushbullet,
+        ConfigReportPushover,
+        ConfigReportRunCommand,
+        ConfigReportStdout,
         ConfigReportTelegram,
         ConfigReportWebhook,
-        ConfigReportMatrix,
-        ConfigReportMailgun,
-        ConfigReportIfttt,
         ConfigReportXmpp,
-        ConfigReportProwl,
-        ConfigReportRunCommand,
     ]
 
 try:
     import aioxmpp
 except ImportError:
     aioxmpp = None
 
@@ -139,15 +143,17 @@
     def convert(self, othercls: Type[ReporterBase]) -> ReporterBase:
         """Convert self to a different ReporterBase class (object typecasting).
 
         :param othercls: The ReporterBase class the be cast into.
         :returns: The typecasted object.
         """
         if hasattr(othercls, '__kind__'):
-            config: ConfigReportersList = self.report.config['report'][othercls.__kind__]  # type: ignore[misc]
+            config: ConfigReportersList = self.report.config['report'][
+                othercls.__kind__  # type: ignore[literal-required]
+            ]
         else:
             config = {}  # type: ignore[assignment]
 
         return othercls(self.report, config, self.job_states, self.duration, self.jobs_file)
 
     @classmethod
     def reporter_documentation(cls) -> str:
@@ -177,15 +183,15 @@
         :param job_states: The list of JobState objects containing the information about each job retrieved.
         :param duration: The duration of the retrieval of jobs.
         :param jobs_file: The path to the file containing the list of jobs (optional, used in footers).
         :param check_enabled: Whether to check if the report is marked "enabled" in the configuration (used for
            testing)
         """
         subclass = cls.__subclasses__[name]
-        cfg = report.config['report'][name]  # type: ignore[misc]
+        cfg = report.config['report'][name]  # type: ignore[literal-required]
         if cfg['enabled'] or not check_enabled:
             subclass(report, cfg, job_states, duration, jobs_file).submit()
         else:
             raise ValueError(f'Reporter not enabled: {name}')
 
     @classmethod
     def submit_all(
@@ -327,45 +333,45 @@
                     diff,
                     flags=re.DOTALL,
                 )
                 yield diff
                 yield '</span>'
             elif job.diff_tool.startswith('deepdiff'):
                 yield '<span style="font-family:monospace;white-space:pre-wrap">'
-                diff += '\n'
-                diff = re.sub(
-                    r'^(Item .+?] added to [a-z]* as ["{]*)(.+?)(["}.]\.\n)',
-                    lambda x: (
-                        f'{x.group(1)}'
-                        f'<span style="background-color:#d1ffd1;color:#082b08">{x.group(2)}</span>{x.group(3)}'
-                    ),
-                    diff,
-                    flags=re.DOTALL | re.MULTILINE,
-                )
-                diff = re.sub(
-                    r'^(Item .+?] removed from [a-z]* \(was ["{]*)(.+?)(["}.]\)\.\n)',
-                    lambda x: (
-                        f'{x.group(1)}'
-                        f'<span style="background-color:#fff0f0;color:#9c1c1c;'
-                        f'text-decoration:line-through">{x.group(2)}</span>{x.group(3)}'
-                    ),
-                    diff,
-                    flags=re.DOTALL | re.MULTILINE,
-                    # flags=re.DOTALL | re.MULTILINE,
-                )
-                diff = re.sub(
-                    r'( changed from ["{]*)(.+?)(["}]* to ["{]*)(.+?)(["} .])',
-                    lambda x: (
-                        f'{x.group(1)}'
-                        f'<span style="background-color:#fff0f0;color:#9c1c1c;text-decoration:line-through">'
-                        f'{x.group(2)}</span>{x.group(3)}<span style="background-color:#d1ffd1;color:#082b08">'
-                        f'{x.group(4)}</span>{x.group(5)}'
-                    ),
-                    diff,
-                )
+                # diff += '\n'
+                # diff = re.sub(
+                #     r'^(Item .+?] added to [a-z]* as ["{]*)(.+?)(["}.]\.\n)',
+                #     lambda x: (
+                #         f'{x.group(1)}'
+                #         f'<span style="background-color:#d1ffd1;color:#082b08">{x.group(2)}</span>{x.group(3)}'
+                #     ),
+                #     diff,
+                #     flags=re.DOTALL | re.MULTILINE,
+                # )
+                # diff = re.sub(
+                #     r'^(Item .+?] removed from [a-z]* \(was ["{]*)(.+?)(["}.]\)\.\n)',
+                #     lambda x: (
+                #         f'{x.group(1)}'
+                #         f'<span style="background-color:#fff0f0;color:#9c1c1c;'
+                #         f'text-decoration:line-through">{x.group(2)}</span>{x.group(3)}'
+                #     ),
+                #     diff,
+                #     flags=re.DOTALL | re.MULTILINE,
+                #     # flags=re.DOTALL | re.MULTILINE,
+                # )
+                # diff = re.sub(
+                #     r'( changed from ["{]*)(.+?)(["}]* to ["{]*)(.+?)(["} .])',
+                #     lambda x: (
+                #         f'{x.group(1)}'
+                #         f'<span style="background-color:#fff0f0;color:#9c1c1c;text-decoration:line-through">'
+                #         f'{x.group(2)}</span>{x.group(3)}<span style="background-color:#d1ffd1;color:#082b08">'
+                #         f'{x.group(4)}</span>{x.group(5)}'
+                #     ),
+                #     diff,
+                # )
                 yield diff[:-1]
                 yield '</span>'
         else:
             if job.is_markdown:
                 # rebuild html from markdown using markdown2 library's Markdown
                 markdowner = Markdown(safe_mode='escape', extras=['strike', 'target-blank-links'])
                 ptags = re.compile(r'^<p>|</p>$')
@@ -488,15 +494,15 @@
         if job_state.verb == 'unchanged':
             return f'<pre style="white-space:pre-wrap">{html.escape(str(job_state.old_data))}</pre>'
 
         if job_state.old_data in (None, job_state.new_data):
             return '...'
 
         if difftype == 'unified':
-            diff = job_state.get_diff(tz)
+            diff = job_state.get_diff_html(tz)
             if diff:
                 return '\n'.join(self._diff_to_html(diff, job_state.job))
             else:
                 return None
 
         elif difftype == 'table':
             if tz:
@@ -1324,25 +1330,103 @@
                     pre_status = not pre_status
         except StopIteration:
             chunks.append(''.join(chunk_lines))
 
         return chunks
 
 
+class DiscordReporter(TextReporter):
+    """Send a message to a Discord channel using a discord webhook."""
+
+    __kind__ = 'discord'
+
+    config: ConfigReportDiscord
+
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+        default_max_length = 2000 if not self.config.get('embed', False) else 4096
+        if isinstance(self.config['max_message_length'], int):
+            self.max_length = int(self.config['max_message_length'])  # type: ignore[arg-type]
+        else:
+            self.max_length = default_max_length
+        if self.config.get('colored', True):
+            self.max_length -= 11
+
+    def submit(self) -> Optional[requests.Response]:  # type: ignore[override]
+        webhook_url = self.config['webhook_url']
+        text = '\n'.join(super().submit())
+
+        if not text:
+            logger.debug('Not calling Discord API (no changes)')
+            return None
+
+        result = None
+        for chunk in chunk_string(text, self.max_length, numbering=True):
+            res = self.submit_to_discord(webhook_url, chunk)
+            if res.status_code != requests.codes.ok or res is None:
+                result = res
+
+        return result
+
+    def submit_to_discord(self, webhook_url: str, text: str) -> requests.Response:
+        if self.config.get('colored', True):
+            text = '```diff\n' + text + '```'
+
+        if self.config.get('embed', False):
+            filtered_job_states = list(self.report.get_filtered_job_states(self.job_states))
+
+            subject_args = {
+                'count': len(filtered_job_states),
+                'jobs': ', '.join(job_state.job.pretty_name() for job_state in filtered_job_states),
+            }
+
+            subject = self.config['subject'].format(**subject_args)
+            # Content has a maximum length of 2000 characters, but the combined sum of characters in all title,
+            # description, field.name, field.value, footer.text, and author.name fields across all embeds attached to
+            # a message must not exceed 6000 characters.
+            max_subject_length = min(2000, 6000 - len(text))
+            subject = subject[:max_subject_length]
+
+            post_data = {
+                'content': subject,
+                'embeds': [
+                    {
+                        'type': 'rich',
+                        'description': text,
+                    }
+                ],
+            }
+        else:
+            post_data = {'content': text}
+
+        logger.debug(f'Sending Discord request with post_data: {post_data}')
+
+        result = requests.post(webhook_url, json=post_data)
+        try:
+            if result.status_code in (requests.codes.ok, requests.codes.no_content):
+                logger.info('Discord response: ok')
+            else:
+                logger.error(f'Discord error: {result.text}')
+        except ValueError:
+            logger.error(
+                f'Failed to parse Discord response. HTTP status code: {result.status_code}, content: {result.content!r}'
+            )
+        return result
+
+
 class WebhookReporter(TextReporter):
-    """Send a text message to a webhook such as Slack, Discord channel, or Mattermost.  For Mattermost,
-    set 'markdown' to true."""
+    """Send a text message to a webhook such as Slack or Mattermost.  For Mattermost,  set 'markdown' to true."""
 
     __kind__ = 'webhook'
 
     config: ConfigReportWebhook
 
     def __init__(self, *args: Any, **kwargs: Any) -> None:
         super().__init__(*args, **kwargs)
-        default_max_length = 2000 if self.config['webhook_url'][:23] == 'https://discordapp.com/' else 40000
+        default_max_length = 40000
         if isinstance(self.config['max_message_length'], int):
             self.max_length = int(self.config['max_message_length'])  # type: ignore[arg-type]
         else:
             self.max_length = default_max_length
 
     def submit(self) -> Optional[requests.Response]:  # type: ignore[override]
         webhook_url = self.config['webhook_url']
@@ -1365,19 +1449,19 @@
             if res.status_code != requests.codes.ok or res is None:
                 result = res
 
         return result
 
     @staticmethod
     def submit_to_webhook(webhook_url: str, text: str) -> requests.Response:
-        logger.debug(f'Sending request to webhook with text:{text}')
+        logger.debug(f'Sending request to webhook with text: {text}')
         post_data = {'text': text}
         result = requests.post(webhook_url, json=post_data)
         try:
-            if result.status_code == requests.codes.ok:
+            if result.status_code in (requests.codes.ok, requests.codes.no_content):
                 logger.info('Webhook server response: ok')
             else:
                 raise RuntimeError(f'Webhook server error: {result.text}')
         except ValueError:
             logger.error(
                 f'Failed to parse webhook server response. HTTP status code:'  # type: ignore[str-bytes-safe]
                 f' {result.status_code}, content: {result.content}'
```

## webchanges/storage.py

```diff
@@ -1,21 +1,25 @@
 """Handles all storage: job files, config files, hooks file, and cache database engines."""
+
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from __future__ import annotations
 
 import copy
 import email.utils
 import getpass
 import inspect
 import logging
 import os
 import shutil
 import sqlite3
 import stat
 import sys
 import threading
+import warnings
 from abc import ABC, abstractmethod
 from collections import defaultdict
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Iterable, Iterator, List, NamedTuple, Optional, TextIO, Tuple, TYPE_CHECKING, Union
 
 import msgpack
@@ -83,14 +87,25 @@
     ConfigReportBrowser = TypedDict(
         'ConfigReportBrowser',
         {
             'enabled': bool,
             'title': str,
         },
     )
+    ConfigReportDiscord = TypedDict(
+        'ConfigReportDiscord',
+        {
+            'enabled': bool,
+            'webhook_url': str,
+            'embed': bool,
+            'subject': str,
+            'colored': bool,
+            'max_message_length': Optional[int],
+        },
+    )
     ConfigReportEmailSmtp = TypedDict(
         'ConfigReportEmailSmtp',
         {
             'host': str,
             'user': str,
             'port': int,
             'starttls': bool,
@@ -113,30 +128,77 @@
             'from': str,
             'subject': str,
             'method': Literal['sendmail', 'smtp'],
             'smtp': ConfigReportEmailSmtp,
             'sendmail': ConfigReportEmailSendmail,
         },
     )
+    ConfigReportIfttt = TypedDict(
+        'ConfigReportIfttt',
+        {
+            'enabled': bool,
+            'key': str,
+            'event': str,
+        },
+    )
+    ConfigReportMailgun = TypedDict(
+        'ConfigReportMailgun',
+        {
+            'enabled': bool,
+            'region': str,
+            'api_key': str,
+            'domain': str,
+            'from_mail': str,
+            'from_name': str,
+            'to': str,
+            'subject': str,
+        },
+    )
+    ConfigReportMatrix = TypedDict(
+        'ConfigReportMatrix',
+        {
+            'enabled': bool,
+            'homeserver': str,
+            'access_token': str,
+            'room_id': str,
+        },
+    )
+    ConfigReportProwl = TypedDict(
+        'ConfigReportProwl',
+        {
+            'enabled': bool,
+            'api_key': str,
+            'priority': int,
+            'application': str,
+            'subject': str,
+        },
+    )
+    ConfigReportPushbullet = TypedDict(
+        'ConfigReportPushbullet',
+        {
+            'enabled': bool,
+            'api_key': str,
+        },
+    )
     ConfigReportPushover = TypedDict(
         'ConfigReportPushover',
         {
             'enabled': bool,
             'app': str,
             'device': Optional[str],
             'sound': str,
             'user': str,
             'priority': str,
         },
     )
-    ConfigReportPushbullet = TypedDict(
-        'ConfigReportPushbullet',
+    ConfigReportRunCommand = TypedDict(
+        'ConfigReportRunCommand',
         {
             'enabled': bool,
-            'api_key': str,
+            'command': str,
         },
     )
     ConfigReportTelegram = TypedDict(
         'ConfigReportTelegram',
         {
             'enabled': bool,
             'bot_token': str,
@@ -149,91 +211,45 @@
         {
             'enabled': bool,
             'markdown': bool,
             'webhook_url': str,
             'max_message_length': Optional[int],
         },
     )
-    ConfigReportMatrix = TypedDict(
-        'ConfigReportMatrix',
-        {
-            'enabled': bool,
-            'homeserver': str,
-            'access_token': str,
-            'room_id': str,
-        },
-    )
-    ConfigReportMailgun = TypedDict(
-        'ConfigReportMailgun',
-        {
-            'enabled': bool,
-            'region': str,
-            'api_key': str,
-            'domain': str,
-            'from_mail': str,
-            'from_name': str,
-            'to': str,
-            'subject': str,
-        },
-    )
-    ConfigReportIfttt = TypedDict(
-        'ConfigReportIfttt',
-        {
-            'enabled': bool,
-            'key': str,
-            'event': str,
-        },
-    )
     ConfigReportXmpp = TypedDict(
         'ConfigReportXmpp',
         {
             'enabled': bool,
             'sender': str,
             'recipient': str,
             'insecure_password': Optional[str],
         },
     )
-    ConfigReportProwl = TypedDict(
-        'ConfigReportProwl',
-        {
-            'enabled': bool,
-            'api_key': str,
-            'priority': int,
-            'application': str,
-            'subject': str,
-        },
-    )
-    ConfigReportRunCommand = TypedDict(
-        'ConfigReportRunCommand',
-        {
-            'enabled': bool,
-            'command': str,
-        },
-    )
 
     ConfigReport = TypedDict(
         'ConfigReport',
         {
             'tz': Optional[str],
             'text': ConfigReportText,
             'html': ConfigReportHtml,
             'markdown': ConfigReportMarkdown,
             'stdout': ConfigReportStdout,
             'browser': ConfigReportBrowser,
+            'discord': ConfigReportDiscord,
             'email': ConfigReportEmail,
-            'pushover': ConfigReportPushover,
+            'ifttt': ConfigReportIfttt,
+            'mailgun': ConfigReportMailgun,
+            'matrix': ConfigReportMatrix,
+            'prowl': ConfigReportProwl,
             'pushbullet': ConfigReportPushbullet,
+            'pushover': ConfigReportPushover,
+            'run_command': ConfigReportRunCommand,
             'telegram': ConfigReportTelegram,
             'webhook': ConfigReportWebhook,
-            'matrix': ConfigReportMatrix,
-            'mailgun': ConfigReportMailgun,
-            'ifttt': ConfigReportIfttt,
             'xmpp': ConfigReportXmpp,
-            'prowl': ConfigReportProwl,
-            'run_command': ConfigReportRunCommand,
         },
     )
     ConfigJobDefaults = TypedDict(
         'ConfigJobDefaults',
         {
             'all': Dict[str, Any],
             'url': Dict[str, Any],
@@ -278,14 +294,22 @@
             'enabled': True,
             'color': True,
         },
         'browser': {  # the system's default browser; uses html
             'enabled': False,
             'title': f'[{__project_name__}] {{count}} changes: {{jobs}}',
         },
+        'discord': {
+            'enabled': False,
+            'webhook_url': '',
+            'embed': True,
+            'subject': '{count} changes: {jobs}',
+            'colored': True,
+            'max_message_length': None,
+        },
         'email': {  # email (except mailgun); uses text or both html and text if 'html' is set to true
             'enabled': False,
             'html': True,
             'to': '',
             'from': '',
             'subject': f'[{__project_name__}] {{count}} changes: {{jobs}}',
             'method': 'smtp',  # either 'smtp' or 'sendmail'
@@ -606,15 +630,15 @@
 
             :param d1_: The first dict.
             :param d2_: The second dict.
             :return: A dict with elements on the first dict that are not in the second.
             """
             for key, value in d1_.copy().items():
                 if isinstance(value, dict) and isinstance(d2_.get(key), dict):  # type: ignore[misc]
-                    _sub_dict_deep_difference(value, d2_[key])  # type: ignore[arg-type,misc]
+                    _sub_dict_deep_difference(value, d2_[key])  # type: ignore[arg-type,literal-required]
                     if not len(value):
                         d1_.pop(key)  # type: ignore[misc]
                 else:
                     if key in d2_:
                         d1_.pop(key)  # type: ignore[misc]
             return d1_
 
@@ -640,56 +664,74 @@
             """
             for key, value in source_.items():
                 if isinstance(value, dict):
                     # get node or create one
                     node = destination_.setdefault(key, {})  # type: ignore[misc]
                     _sub_dict_deep_merge(value, node)  # type: ignore[arg-type]
                 else:
-                    destination_[key] = value  # type: ignore[misc]
+                    destination_[key] = value  # type: ignore[literal-required]
 
             return destination_
 
         return _sub_dict_deep_merge(source, copy.deepcopy(destination))
 
     def check_for_unrecognized_keys(self, config: Config) -> None:
         """Test if config has keys not in DEFAULT_CONFIG (bad keys, e.g. typos); if so, raise ValueError.
 
         :param config: The configuration.
         :raises ValueError: If the configuration has keys not in DEFAULT_CONFIG (bad keys, e.g. typos)
         """
-
         config_for_extras = copy.deepcopy(config)
         if 'job_defaults' in config_for_extras:
             # 'job_defaults' is not set in DEFAULT_CONFIG
             for key in DEFAULT_CONFIG['job_defaults']:
-                config_for_extras['job_defaults'][key] = {}  # type: ignore[misc]
+                config_for_extras['job_defaults'][key] = {}  # type: ignore[literal-required]
         if 'slack' in config_for_extras.get('report', {}):  # legacy key; ignore
             config_for_extras['report'].pop('slack')  # type: ignore[typeddict-item]
         extras: Config = self.dict_deep_difference(config_for_extras, DEFAULT_CONFIG)
         if extras.get('report') and 'hooks' in sys.modules:
             # skip reports added by hooks
             for name, obj in inspect.getmembers(sys.modules['hooks'], inspect.isclass):
                 if obj.__module__ == 'hooks' and issubclass(obj, ReporterBase):
-                    extras['report'].pop(obj.__kind__)  # type: ignore[misc]
+                    extras['report'].pop(obj.__kind__, None)  # type: ignore[misc]
             if not len(extras['report']):
                 extras.pop('report')  # type: ignore[misc]
         if extras:
-            raise ValueError(
+            warnings.warn(
                 f'Unrecognized directive(s) in the configuration file {self.filename}:\n'
-                f'{yaml.safe_dump(extras)}Check for typos (documentation at {__docs_url__})'
+                f'{yaml.safe_dump(extras)}Check for typos (documentation at {__docs_url__})\n',
+                SyntaxWarning,
             )
 
+    @staticmethod
+    def replace_none_keys(config: Config) -> None:
+        """Fixes None keys in loaded config that should be empty dicts instead."""
+        if 'job_defaults' not in config:
+            config['job_defaults'] = {
+                'all': {},
+                'url': {},
+                'browser': {},
+                'shell': {},
+            }
+        else:
+            for key in ('all', 'url', 'browser', 'shell'):
+                if key not in config['job_defaults']:
+                    config['job_defaults'][key] = {}  # type: ignore[literal-required]
+                elif config['job_defaults'][key] is None:  # type: ignore[literal-required]
+                    config['job_defaults'][key] = {}  # type: ignore[literal-required]
+
     def load(self, *args: Any) -> None:
         """Load configuration file from self.filename into self.config adding missing keys from DEFAULT_CONFIG.
 
         :param args: None used.
         """
         config: Config = self.parse(self.filename)
 
         if config:
+            self.replace_none_keys(config)
             self.check_for_unrecognized_keys(config)
 
             # If config is missing keys in DEFAULT_CONFIG, log the missing keys and deep merge DEFAULT_CONFIG
             missing = self.dict_deep_difference(DEFAULT_CONFIG, config)
             if missing:
                 logger.info(
                     f'The configuration file {self.filename} is missing directive(s); the following default '
@@ -814,14 +856,18 @@
         pass
 
     @abstractmethod
     def get_history_data(self, guid: str, count: Optional[int] = None) -> Dict[str, float]:
         pass
 
     @abstractmethod
+    def get_rich_history_data(self, guid: str, count: Optional[int] = None) -> List[Dict[str, Any]]:
+        pass
+
+    @abstractmethod
     def save(self, *args: Any, guid: str, data: str, timestamp: float, tries: int, etag: str, **kwargs: Any) -> None:
         pass
 
     @abstractmethod
     def delete(self, guid: str) -> None:
         pass
 
@@ -933,20 +979,27 @@
             logger.warning(f'Found and ignored Unicode-related errors when retrieving saved snapshot {guid}')
 
         timestamp = filename.stat().st_mtime
 
         return Snapshot(data, timestamp, 0, '')
 
     def get_history_data(self, guid: str, count: Optional[int] = None) -> Dict[str, float]:
-        if isinstance(count, int) and count < 1:
+        if count is not None and count < 1:
             return {}
         else:
             data, timestamp, tries, etag = self.load(guid)
             return {data: timestamp} if data and timestamp else {}
 
+    def get_rich_history_data(self, guid: str, count: Optional[int] = None) -> List[Dict[str, Any]]:
+        if count is not None and count < 1:
+            return []
+        else:
+            data, timestamp, tries, etag = self.load(guid)
+            return [{'timestamp': timestamp, 'data': data}] if data and timestamp else []
+
     def save(
         self,
         *args: Any,
         guid: str,
         data: str,
         timestamp: float,
         tries: int,
@@ -1117,15 +1170,14 @@
             if self.max_snapshots:
                 num_del = self.keep_latest(self.max_snapshots)
                 logger.debug(
                     f'Keeping no more than {self.max_snapshots} snapshots per job: purged {num_del} older entries'
                 )
             else:
                 self.db.commit()
-            self._execute('VACUUM')
             self.db.close()
             logger.info(f'Closed main sqlite3 database file {self.filename}')
         del self.temp_cur
         del self.temp_db
         del self.temp_lock
         del self.cur
         del self.db
@@ -1176,15 +1228,15 @@
         :returns: A dict (key: value)
             WHERE
 
             - key is the data;
             - value is the timestamp.
         """
         history: Dict[str, float] = {}
-        if isinstance(count, int) and count < 1:
+        if count is not None and count < 1:
             return history
 
         with self.lock:
             rows = self._execute(
                 'SELECT msgpack_data, timestamp FROM webchanges WHERE uuid = ? ORDER BY timestamp DESC', (guid,)
             ).fetchall()
         if rows:
@@ -1193,14 +1245,51 @@
                 if not r['t']:
                     if r['d'] not in history:
                         history[r['d']] = timestamp
                         if count is not None and len(history) >= count:
                             break
         return history
 
+    def get_rich_history_data(self, guid: str, count: Optional[int] = None) -> List[Dict[str, Any]]:
+        """Return all data from the last 'count' (None = all) entries matching a 'guid'.
+
+        :param guid: The guid.
+        :param count: The maximum number of entries to return; if None return all.
+
+        :returns: A list of dicts
+            WHERE the keys are:
+
+            - timestamp: The timestamp (float);
+            - data: The data (str);
+            - tries (optional): The number of tries (int);
+            - etag (optional): The ETag (str, could be empty).
+        """
+        history: List[Dict[str, Any]] = []
+        if count is not None and count < 1:
+            return history
+
+        with self.lock:
+            rows = self._execute(
+                'SELECT msgpack_data, timestamp FROM webchanges WHERE uuid = ? ORDER BY timestamp DESC', (guid,)
+            ).fetchall()
+        if rows:
+            for msgpack_data, timestamp in rows:
+                r = msgpack.unpackb(msgpack_data)
+                history.append(
+                    {
+                        'timestamp': timestamp,
+                        'data': r['d'],
+                        'tries': r['t'],
+                        'etag': r['e'],
+                    }
+                )
+                if count is not None and len(history) >= count:
+                    break
+        return history
+
     def save(
         self,
         *args: Any,
         guid: str,
         data: str,
         timestamp: float,
         tries: int,
@@ -1260,15 +1349,14 @@
                 '    WHERE uuid = ? '
                 '    ORDER BY timestamp DESC '
                 '    LIMIT ? '
                 ')',
                 (guid, delete_entries),
             )
             num_del: int = self._execute('SELECT changes()').fetchone()[0]
-            self.db.commit()
         return num_del
 
     def clean(self, guid: str, keep_entries: int = 1) -> int:
         """For the given 'guid', keep only the latest 'keep_entries' number of entries and delete all other (older)
         ones. To delete older entries from all guids, use clean_all() instead.
 
         :param guid: The guid.
@@ -1285,14 +1373,15 @@
                 '    ORDER BY timestamp DESC '
                 '    LIMIT -1 OFFSET ? '
                 ')',
                 (guid, keep_entries),
             )
             num_del: int = self._execute('SELECT changes()').fetchone()[0]
             self.db.commit()
+            self._execute('VACUUM')
         return num_del
 
     def clean_all(self) -> int:
         """Delete all older entries for each 'guid' (keep only last one).
 
         :returns: Number of records deleted.
         """
@@ -1302,14 +1391,15 @@
                 'WHERE EXISTS ( '
                 '    SELECT 1 FROM webchanges w '
                 '    WHERE w.uuid = webchanges.uuid AND w.timestamp > webchanges.timestamp '
                 ')'
             )
             num_del: int = self._execute('SELECT changes()').fetchone()[0]
             self.db.commit()
+            self._execute('VACUUM')
         return num_del
 
     def keep_latest(self, keep_entries: int = 1) -> int:
         """Delete all older entries keeping only the 'keep_num' per guid.
 
         :param keep_entries: Number of entries to keep after deletion.
 
@@ -1328,14 +1418,15 @@
                 '               WHERE webchanges.uuid = cte.uuid '
                 '                 AND webchanges.timestamp = cte.timestamp '
                 '                 AND cte.rn > ? );',
                 (keep_entries,),
             )
             num_del: int = self._execute('SELECT changes()').fetchone()[0]
             self.db.commit()
+            self._execute('VACUUM')
         return num_del
 
     def rollback(self, timestamp: float) -> int:
         """Rollback database to the entries present at timestamp.
 
         :param timestamp: The timestamp.
 
@@ -1415,28 +1506,35 @@
             r = msgpack.unpackb(data)
             return Snapshot(r['data'], r['timestamp'], r['tries'], r['etag'])
 
         return Snapshot('', 0, 0, '')
 
     def get_history_data(self, guid: str, count: Optional[int] = None) -> Dict[str, float]:
         history: Dict[str, float] = {}
-        if isinstance(count, int) and count < 1:
+        if count is not None and count < 1:
             return history
 
         key = self._make_key(guid)
         for i in range(0, self.db.llen(key)):
             r = self.db.lindex(key, i)
             c = msgpack.unpackb(r)
             if c['tries'] == 0 or c['tries'] is None:
                 if c['data'] not in history:
                     history[c['data']] = c['timestamp']
                     if count is not None and len(history) >= count:
                         break
         return history
 
+    def get_rich_history_data(self, guid: str, count: Optional[int] = None) -> List[Dict[str, Any]]:
+        if count is not None and count < 1:
+            return []
+        else:
+            data, timestamp, tries, etag = self.load(guid)
+            return [{'timestamp': timestamp, 'data': data}] if data and timestamp else []
+
     def save(
         self,
         *args: Any,
         guid: str,
         data: str,
         timestamp: float,
         tries: int,
```

## webchanges/storage_minidb.py

```diff
@@ -4,14 +4,17 @@
 
 * when reading databases created in version < 3.2.0
 * when running with the '--database-engine minidb' switch
 * testing migration of legacy database
 
 Having it into a standalone module allows running the program without requiring minidb package to be installed.
 """
+
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Union
 
 from .storage import CacheStorage, Snapshot
 
 try:
     import minidb
@@ -73,14 +76,30 @@
         ):
             if data not in history:
                 history[data] = timestamp
                 if count is not None and len(history) >= count:
                     break
         return history
 
+    def get_rich_history_data(self, guid: str, count: Optional[int] = None) -> List[Dict[str, Any]]:
+        history: List[Dict[str, Any]] = []
+        if count is not None and count < 1:
+            return history
+        for data, timestamp in self.CacheEntry.query(
+            self.db,
+            self.CacheEntry.c.data // self.CacheEntry.c.timestamp,
+            order_by=minidb.columns(self.CacheEntry.c.timestamp.desc, self.CacheEntry.c.tries.desc),
+            where=(self.CacheEntry.c.guid == guid)
+            & ((self.CacheEntry.c.tries == 0) | (self.CacheEntry.c.tries is None)),
+        ):
+            history.append({'timestamp': timestamp, 'data': data})
+            if count is not None and len(history) >= count:
+                break
+        return history
+
     def save(
         self,
         *args: Any,
         guid: str,
         data: str,
         timestamp: float,
         tries: int,
```

## webchanges/util.py

```diff
@@ -1,9 +1,11 @@
 """A few utilities used elsewhere."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from __future__ import annotations
 
 import html
 import importlib.machinery
 import importlib.util
 import logging
 import os
@@ -24,15 +26,15 @@
 
 logger = logging.getLogger(__name__)
 
 
 class TrackSubClasses(type):
     """A metaclass that stores subclass name-to-class mappings in the base class."""
 
-    # __subclasses__ gets redefined in a non-Pythonic way from default "Callable[[_TT], List[_TT]]
+    # __subclasses__ gets redefined from default "Callable[[_TT], List[_TT]]
     __subclasses__: Dict[str, TrackSubClasses]  # type: ignore[assignment]
     __anonymous_subclasses__: List[TrackSubClasses]
     __required__: Tuple[str, ...] = ()
     __optional__: Tuple[str, ...] = ()
 
     __kind__: str
     __supported_subfilters__: Dict[str, str]
@@ -97,53 +99,53 @@
     spec = importlib.util.spec_from_file_location(module_name, source_path, loader=loader)
     module = importlib.util.module_from_spec(spec)  # type: ignore[arg-type]
     sys.modules[module_name] = module
     loader.exec_module(module)
     return module
 
 
-def chunk_string(string: str, length: int, numbering: bool = False) -> Iterable[str]:
+def chunk_string(text: str, length: int, numbering: bool = False) -> Iterable[str]:
     """Chunks a string.
 
-    :param string: The string
-    :param length: The length of the chunked string
-    :param numbering: Whether to number each chunk on the right
+    :param text: The text to be chunked.
+    :param length: The length of the chunked text.
+    :param numbering: Whether to number each chunk on the left if more than one chunk is generated.
 
     :returns: a list of chunked strings
     """
-    if numbering and len(string) > length:
+    if numbering and len(text) > length:
         try:
             text_length = length - 4 - 2
-            digits_try = 1 if text_length <= 0 else floor(log10(len(string) / text_length))  # initialization floor
+            digits_try = 1 if text_length <= 0 else floor(log10(len(text) / text_length))  # initialization floor
             digits_guess = digits_try + 1
             while digits_guess > digits_try:
                 digits_try += 1
                 text_length = length - 4 - 2 * digits_try
                 if text_length <= 0:
                     raise ValueError('Not enough space to chunkify string with line numbering (1)')
-                lines_guess = len(string) / text_length
+                lines_guess = len(text) / text_length
                 digits_guess = floor(log10(lines_guess)) + 1
 
-            chunks = textwrap.wrap(string, text_length, replace_whitespace=False)
+            chunks = textwrap.wrap(text, text_length, replace_whitespace=False)
             actual_digits = floor(log10(len(chunks))) + 1
             while actual_digits > digits_try:
                 digits_try += 1
                 text_length = length - 4 - 2 * digits_try
                 if text_length <= 0:
                     raise ValueError('Not enough space to chunkify string with line numbering (2)')
-                chunks = textwrap.wrap(string, text_length, replace_whitespace=False)
+                chunks = textwrap.wrap(text, text_length, replace_whitespace=False)
                 actual_digits = floor(log10(len(chunks))) + 1
 
             length = len(chunks)
             return [line + ' (' + f'{{:{digits_try}d}}'.format(i + 1) + f'/{length})' for i, line in enumerate(chunks)]
 
         except ValueError as e:
             logger.error(f'{e}')
 
-    return textwrap.wrap(string, length, replace_whitespace=False)
+    return textwrap.wrap(text, length, replace_whitespace=False)
 
 
 def linkify(
     text: str,
     shorten: bool = False,
     extra_params: Union[str, Callable[[str], str]] = '',
     require_protocol: bool = False,
```

## webchanges/worker.py

```diff
@@ -1,9 +1,11 @@
 """The worker that runs jobs in parallel.  Called from main.py."""
 
+# The code below is subject to the license contained in the LICENSE file, which is part of the source code.
+
 from __future__ import annotations
 
 import logging
 import os
 import random
 import urllib.parse
 from concurrent.futures import ThreadPoolExecutor
@@ -74,15 +76,15 @@
         executor = ThreadPoolExecutor(max_workers=max_workers)
 
         # launch future to retrieve if new version is available
         if urlwatcher.report.new_release_future is None:
             urlwatcher.report.new_release_future = executor.submit(urlwatcher.get_new_release_version)
 
         for job_state in executor.map(
-            lambda jobstate: jobstate.process(),
+            lambda jobstate: jobstate.process(headless=not urlwatcher.urlwatch_config.no_headless),
             (stack.enter_context(JobState(cache_storage, job)) for job in jobs),
         ):
 
             max_tries = 0 if not job_state.job.max_tries else job_state.job.max_tries
 
             if job_state.exception is not None:
                 # Oops, we have captured an error to ignore!
@@ -167,14 +169,14 @@
                     import psutil
                 except ImportError:
                     raise ImportError(
                         "Python package psutil is not installed; cannot use 'use_browser: true' directives with "
                         "'_beta_use_playwright: true'. Please install dependencies with 'pip install webchanges["
                         "playwright]'."
                     )
-                avail_vm = psutil.virtual_memory().available + psutil.swap_memory().free
-                logger.debug(f'Found {avail_vm:,} in available virtual + swap memory')
-                max_workers = min(32, max(1, int(avail_vm / 140e6)), os.cpu_count() or 1)
+                avail_mem = psutil.virtual_memory().available + psutil.swap_memory().free
+                logger.debug(f'Found {avail_mem:,} in available virtual + swap memory')
+                max_workers = min(32, max(1, int(avail_mem / 140e6)), os.cpu_count() or 1)
             else:
                 max_workers = min(32, os.cpu_count() or 1)
             logger.debug(f"Running 'use_browser: true' jobs in parallel with {max_workers} max_workers")
             job_runner(stack, jobs_to_run, cache_storage, report, max_workers)
```

## Comparing `webchanges-3.9.1.dist-info/COPYING` & `webchanges-3.9.2.dist-info/LICENSE`

 * *Files 8% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+========
+Licenses
+========
+
 The MIT License (MIT)
 
 Copyright (c) 2020- Mike Borsetti <mike@borsetti.com>
 
 Permission is hereby granted, free of charge, to any person obtaining a copy of
 this software and associated documentation files (the "Software"), to deal in
 the Software without restriction, including without limitation the rights to
@@ -15,19 +19,21 @@
 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
 FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
 COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
 IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
-The above license applies to all contributions after 2020-09-10
-
 --------------------------------------------------------------------------------
 
-This project includes code from https://github.com/thp/urlwatch
+This software redistributes source code of release 2.21 of urlwatch
+https://github.com/thp/urlwatch/tree/346b25914b0418342ffe2fb0529bed702fddc01f
+which is subject to the following license, hereby retained with the source code,
+of which this file is part of:
+
 Copyright (c) 2008-2020 Thomas Perl <m@thp.io>
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions
 are met:
 
@@ -35,15 +41,15 @@
    notice, this list of conditions and the following disclaimer.
 2. Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.
 3. The name of the author may not be used to endorse or promote products
    derived from this software without specific prior written permission.
 
-THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR ''AS IS'' AND ANY EXPRESS OR
 IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
```

## Comparing `webchanges-3.9.1.dist-info/METADATA` & `webchanges-3.9.2.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: webchanges
-Version: 3.9.1
+Version: 3.9.2
 Summary: Check web (or commands) for changes since last run and notify.  Anonymously alerts you of webpage changes.
 Home-page: https://pypi.org/project/webchanges/
 Author: Mike Borsetti
 Author-email: mike@borsetti.com
 License: MIT, BSD 3-Clause License
 Project-URL: Issues, https://www.github.com/mborsetti/webchanges/issues
 Project-URL: CI, https://www.github.com/mborsetti/webchanges/actions
@@ -32,15 +32,15 @@
 Classifier: Operating System :: OS Independent
 Classifier: Natural Language :: English
 Classifier: Intended Audience :: End Users/Desktop
 Classifier: Intended Audience :: System Administrators
 Classifier: Intended Audience :: Developers
 Requires-Python: >=3.7
 Description-Content-Type: text/x-rst
-License-File: COPYING
+License-File: LICENSE
 Requires-Dist: cssselect
 Requires-Dist: html2text
 Requires-Dist: lxml
 Requires-Dist: markdown2
 Requires-Dist: msgpack
 Requires-Dist: platformdirs
 Requires-Dist: pyyaml
@@ -111,28 +111,27 @@
 
 ==========
 webchanges
 ==========
 
 **webchanges** checks web content and notifies you via e-mail (or one of many other supported services) if a change is
 detected. **webchanges** can also check the output of local commands. The notification includes the changed URL or
-command and a summary of what has changed. This project is a fork of `urlwatch <https://github.com/thp/urlwatch>`__ as
-suggested by its author to optimize it for HTML.
+command and a summary of what has changed.
 
 **webchanges** *anonymously* alerts you of webpage changes.
 
 
 
 Requirements
 ============
 **webchanges** requires |support|.
 
-You should use the latest version of `Python <https://www.python.org/downloads/>`__ if possible. If you’re using an
-older version, be aware that for each minor version (3.x), only the latest bugfix release (3.x.y) is supported. Older
-Python versions are supported for 3 years beyond being obsoleted by a new major release.
+You should use the latest version of `Python <https://www.python.org/downloads/>`__ if possible, but older
+Python versions are supported for 3 years after being obsoleted by a new major release (3.x). For each major release,
+only the latest bugfix version (3.x.y) is supported.
 
 
 Installation
 ============
 Install **webchanges** |pypi_version| |format| |status| with::
 
    pip install webchanges
@@ -204,23 +203,23 @@
 
 License
 =======
 |license|
 
 Released under the `MIT License <https://opensource.org/licenses/MIT>`__, but including code licensed under the
 `BSD 3-Clause License <https://opensource.org/licenses/BSD-3-Clause>`__. See the full license `here
-<https://github.com/mborsetti/webchanges/blob/main/COPYING>`__.
+<https://github.com/mborsetti/webchanges/blob/main/LICENSE>`__.
 
 
-Improvements from **urlwatch**
-==============================
+Compatibility with **urlwatch**
+================================
 
-You can seamlessly upgrade from **urlwatch** 2.23 (see `here
-<https://webchanges.readthedocs.io/en/stable/migration.html>`__) and benefit from many HTML-focused improvements,
-including:
+This project is based on code of`urlwatch <https://github.com/thp/urlwatch>`__ 2.21 and you can easily upgrade from
+**urlwatch** 2.25 (see `here <https://webchanges.readthedocs.io/en/stable/migration.html>`__) using the same job and
+configuration files and benefit from many HTML-focused improvements, including:
 
 * Report links that are `clickable <https://pypi.org/project/webchanges/>`__!
 * Original formatting such as **bolding / headers**, *italics*, :underline:`underlining`, list bullets (•) and
   indentation;
 * :additions:`Added` and :deletions:`deleted` lines clearly highlighted by color and strikethrough, and long lines that
   wrap around;
 * Correct rendering by email clients who override stylesheets (e.g. Gmail);
```

## Comparing `webchanges-3.9.1.dist-info/RECORD` & `webchanges-3.9.2.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-webchanges/__init__.py,sha256=zF41-2PyEtw7CEpdh8OedJKMT2p4L_H3LdhzDao0sqE,1891
-webchanges/cli.py,sha256=BIiha8vIrKpe2l1m9ZdrsAw3ACokbV6ESDuuM_gdEug,9835
-webchanges/command.py,sha256=_23NEtrqF6GgtKWEtO3WZx9CFFWlrYO0oG7kMrBBX4E,27307
-webchanges/config.py,sha256=jW1HLASLlkiKvq5EqiaVfCnCZSwsYGD1rKi9yfIZ7T0,9493
-webchanges/filters.py,sha256=HkKWX6zsJr44V6k5BJDK_KIbHhV0VyCih09xe6wD2xk,56151
-webchanges/handler.py,sha256=lTbwqglCiiL71FsI4HeOBhMKAPWM3mcLkDfKrFSwiT4,21606
-webchanges/jobs.py,sha256=phYZH-p4AAXCbRyftBWpKnWq9fMK71346Kcy1gY6VLI,67575
-webchanges/mailer.py,sha256=kjDZ7wYU6t6jtvvEiwjOLK3X_cRi3dGX8b4Do_4Gm5Q,5275
-webchanges/main.py,sha256=gGEnlpzh8C2te8SjaLvWiA66OM0T4bbuZ3VBGQT9Iv4,4174
-webchanges/reporters.py,sha256=G7oAnFPpej4AUwkJwrW0dVKwDEa1_W7hcTiR1F4b3pk,65049
-webchanges/storage.py,sha256=797__ZlFX9etDRarR0Gyh8zyTrlgiZ0kcAK1MPc2-pM,51935
-webchanges/storage_minidb.py,sha256=Jth-N55Wy6Q3PbdlF-j_SoeUC0_r3UIwPWntpCRRL88,4498
-webchanges/util.py,sha256=CACVjIgU1jlWuVU28Kgt_Z6knfVFRmAhgEPc6qubatk,10430
-webchanges/worker.py,sha256=VxM1o6lar19J0wjMwqCYemL3coD4jpzohAhs5UijXbQ,8185
+webchanges/__init__.py,sha256=uOo2Kh228LrMjW9KSABFksP74i5mT5cFQZWPUFrK1jU,2001
+webchanges/cli.py,sha256=bYpZRBS7lWKNgq96aBQd-JNG1R-9U20uUb8fjF7ib5Q,10058
+webchanges/command.py,sha256=A1wFDchx-sUoK6iBxFvtW04GVbHBCC0eYQ4vwx8NT-w,28877
+webchanges/config.py,sha256=yLoG5Pz-iFJZDhxQuS7f7sR_mLekl-izO_CAck2T9QM,10414
+webchanges/filters.py,sha256=DcpYHVt9jGtlqe7snDdwa-OQxID9EoCT1xbIOAqPN_4,56595
+webchanges/handler.py,sha256=v999yBGLhI97IOnbT8e4-0_XWgrf3YN8xY8A4iRo3ik,26040
+webchanges/jobs.py,sha256=q_ziaZ7VqwRaDgfNnBM9FK1cmLRUq5YnqZETcd8jb60,71766
+webchanges/mailer.py,sha256=nRDR8qzwqateWf8X9C8F6Pf1wMEX0ls0XuvTN8ACidc,5384
+webchanges/main.py,sha256=DmspEOiRuKtGUC-qtPW2YtK8kY44STNjjUH4bzt5RRU,4283
+webchanges/reporters.py,sha256=7z5HqRfUNncNDBVRq9q61CcCcYIt6Aym0oA5DxweREQ,68352
+webchanges/storage.py,sha256=UahehPtIyv9cqCpBrwipmSGOAHukcNY9sOtFMhJvjrI,55722
+webchanges/storage_minidb.py,sha256=denU63ij1NN2mdfL0kb90mgyCuVvDMkbSWAo4UnYVWs,5385
+webchanges/util.py,sha256=a6XCP82N1mAA041WrayF3UDln1tm-Oa0vrj4QvtxBtE,10549
+webchanges/worker.py,sha256=02dhfUFzCZ8w2vLksBvDGRAj_fEtkCuXihDpeHqJ4SY,8348
 webchanges/_vendored/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 webchanges/_vendored/packaging_version.py,sha256=2yglAtqVoSEBlpgGsZ7FCV4Tc1zRBGH9oRsj-DdtuNA,17527
-webchanges-3.9.1.dist-info/COPYING,sha256=-PvGovSBF0OmKEOd-ZXh8x-6mGmk9LTTR8Qruiz0PHA,2720
-webchanges-3.9.1.dist-info/METADATA,sha256=bTVzgSpPApBZ336iHSvKTK9MPtz2KFPVX_yoWw3eNOc,11276
-webchanges-3.9.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-webchanges-3.9.1.dist-info/entry_points.txt,sha256=iRyrFeQdgWbg7NoewK6GalgBQwatRyR4oAiFVb6YuH4,52
-webchanges-3.9.1.dist-info/top_level.txt,sha256=BsTDVs3S8a-xuKJlv-mjiUKXGiwrHg_Nx_UJXwbYsjw,11
-webchanges-3.9.1.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-webchanges-3.9.1.dist-info/RECORD,,
+webchanges-3.9.2.dist-info/LICENSE,sha256=b3o7ivAIwJHRBTfTi3oPnL3UUO5WoAxV-hBIMUPouZo,2878
+webchanges-3.9.2.dist-info/METADATA,sha256=FnOvNIRAUb8bRlikOfnjO9gMfCz-0LJRiCKg0CqpeWk,11234
+webchanges-3.9.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+webchanges-3.9.2.dist-info/entry_points.txt,sha256=qZEhX7YA_fASjvVSpoQHPcF8d_t4jK1P4DX5FyKelPY,51
+webchanges-3.9.2.dist-info/top_level.txt,sha256=BsTDVs3S8a-xuKJlv-mjiUKXGiwrHg_Nx_UJXwbYsjw,11
+webchanges-3.9.2.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+webchanges-3.9.2.dist-info/RECORD,,
```

