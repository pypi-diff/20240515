# Comparing `tmp/pds_data_upload_manager-1.1.0-py3-none-any.whl.zip` & `tmp/pds_data_upload_manager-1.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 29583 bytes, number of entries: 23
--rw-r--r--  2.0 unx        6 b- defN 24-Apr-25 20:02 pds/ingress/VERSION.txt
--rw-r--r--  2.0 unx      320 b- defN 24-Apr-25 20:01 pds/ingress/__init__.py
--rw-r--r--  2.0 unx     1910 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/README.md
--rw-r--r--  2.0 unx     3926 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/index.js
--rw-r--r--  2.0 unx     8692 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/package-lock.json
--rw-r--r--  2.0 unx      322 b- defN 24-Apr-25 20:01 pds/ingress/authorizer/package.json
--rw-r--r--  2.0 unx    12328 b- defN 24-Apr-25 20:01 pds/ingress/client/pds_ingress_client.py
--rw-r--r--  2.0 unx     7714 b- defN 24-Apr-25 20:01 pds/ingress/service/pds_ingress_app.py
--rw-r--r--  2.0 unx      454 b- defN 24-Apr-25 20:01 pds/ingress/service/config/bucket-map.yaml
--rw-r--r--  2.0 unx     2688 b- defN 24-Apr-25 20:01 pds/ingress/util/auth_util.py
--rw-r--r--  2.0 unx      555 b- defN 24-Apr-25 20:01 pds/ingress/util/conf.default.ini
--rw-r--r--  2.0 unx     1975 b- defN 24-Apr-25 20:01 pds/ingress/util/config_util.py
--rw-r--r--  2.0 unx    11032 b- defN 24-Apr-25 20:01 pds/ingress/util/log_util.py
--rw-r--r--  2.0 unx     1077 b- defN 24-Apr-25 20:01 pds/ingress/util/node_util.py
--rw-r--r--  2.0 unx     3815 b- defN 24-Apr-25 20:01 pds/ingress/util/path_util.py
--rw-r--r--  2.0 unx    10480 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/LICENSE.md
--rw-r--r--  2.0 unx     4842 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx     1632 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       82 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        4 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 24-Apr-25 20:01 pds_data_upload_manager-1.1.0.dist-info/zip-safe
--rw-rw-r--  2.0 unx     2122 b- defN 24-Apr-25 20:02 pds_data_upload_manager-1.1.0.dist-info/RECORD
-23 files, 76069 bytes uncompressed, 26055 bytes compressed:  65.7%
+Zip file size: 31553 bytes, number of entries: 23
+-rw-r--r--  2.0 unx        6 b- defN 24-May-14 22:13 pds/ingress/VERSION.txt
+-rw-r--r--  2.0 unx      320 b- defN 24-May-14 22:11 pds/ingress/__init__.py
+-rw-r--r--  2.0 unx     1910 b- defN 24-May-14 22:11 pds/ingress/authorizer/README.md
+-rw-r--r--  2.0 unx     3926 b- defN 24-May-14 22:11 pds/ingress/authorizer/index.js
+-rw-r--r--  2.0 unx     8692 b- defN 24-May-14 22:11 pds/ingress/authorizer/package-lock.json
+-rw-r--r--  2.0 unx      322 b- defN 24-May-14 22:11 pds/ingress/authorizer/package.json
+-rw-r--r--  2.0 unx    18798 b- defN 24-May-14 22:11 pds/ingress/client/pds_ingress_client.py
+-rw-r--r--  2.0 unx     7714 b- defN 24-May-14 22:11 pds/ingress/service/pds_ingress_app.py
+-rw-r--r--  2.0 unx      454 b- defN 24-May-14 22:11 pds/ingress/service/config/bucket-map.yaml
+-rw-r--r--  2.0 unx     4132 b- defN 24-May-14 22:11 pds/ingress/util/auth_util.py
+-rw-r--r--  2.0 unx      555 b- defN 24-May-14 22:11 pds/ingress/util/conf.default.ini
+-rw-r--r--  2.0 unx     1975 b- defN 24-May-14 22:11 pds/ingress/util/config_util.py
+-rw-r--r--  2.0 unx    11032 b- defN 24-May-14 22:11 pds/ingress/util/log_util.py
+-rw-r--r--  2.0 unx     1077 b- defN 24-May-14 22:11 pds/ingress/util/node_util.py
+-rw-r--r--  2.0 unx     3833 b- defN 24-May-14 22:11 pds/ingress/util/path_util.py
+-rw-r--r--  2.0 unx    10480 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx     4842 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx     1632 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/NOTICE.txt
+-rw-r--r--  2.0 unx       92 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       82 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        4 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-May-14 22:12 pds_data_upload_manager-1.2.0.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     2122 b- defN 24-May-14 22:13 pds_data_upload_manager-1.2.0.dist-info/RECORD
+23 files, 84001 bytes uncompressed, 28025 bytes compressed:  66.6%
```

## zipnote {}

```diff
@@ -39,32 +39,32 @@
 
 Filename: pds/ingress/util/node_util.py
 Comment: 
 
 Filename: pds/ingress/util/path_util.py
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/LICENSE.md
+Filename: pds_data_upload_manager-1.2.0.dist-info/LICENSE.md
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/METADATA
+Filename: pds_data_upload_manager-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt
+Filename: pds_data_upload_manager-1.2.0.dist-info/NOTICE.txt
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/WHEEL
+Filename: pds_data_upload_manager-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/entry_points.txt
+Filename: pds_data_upload_manager-1.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/top_level.txt
+Filename: pds_data_upload_manager-1.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/zip-safe
+Filename: pds_data_upload_manager-1.2.0.dist-info/zip-safe
 Comment: 
 
-Filename: pds_data_upload_manager-1.1.0.dist-info/RECORD
+Filename: pds_data_upload_manager-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pds/ingress/VERSION.txt

```diff
@@ -1 +1 @@
-1.1.0
+1.2.0
```

## pds/ingress/client/pds_ingress_client.py

```diff
@@ -6,29 +6,50 @@
 
 Client side script used to perform ingress request to the DUM service in AWS.
 """
 import argparse
 import hashlib
 import json
 import os
+import sched
+import time
+from datetime import datetime
+from datetime import timezone
+from threading import Thread
 
 import backoff
 import pds.ingress.util.log_util as log_util
 import requests
 from joblib import delayed
 from joblib import Parallel
 from pds.ingress.util.auth_util import AuthUtil
 from pds.ingress.util.config_util import ConfigUtil
 from pds.ingress.util.log_util import get_log_level
 from pds.ingress.util.log_util import get_logger
 from pds.ingress.util.node_util import NodeUtil
 from pds.ingress.util.path_util import PathUtil
 
+BEARER_TOKEN = None
+"""Placeholder for authentication bearer token used to authenticate to API gateway"""
+
 PARALLEL = Parallel(require="sharedmem")
 
+REFRESH_SCHEDULER = sched.scheduler(time.time, time.sleep)
+"""Scheduler object used to periodically refresh the Cognito authentication token"""
+
+SUMMARY_TABLE = {
+    "uploaded": set(),
+    "skipped": set(),
+    "failed": set(),
+    "transferred": 0,
+    "start_time": time.time(),
+    "end_time": None,
+}
+"""Stores the information for use with the Summary report"""
+
 
 def fatal_code(err: requests.exceptions.RequestException) -> bool:
     """Only retry for common transient errors"""
     return 400 <= err.response.status_code < 500
 
 
 def backoff_logger(details):
@@ -37,32 +58,29 @@
     logger.warning(
         f"Backing off {details['target']} function for {details['wait']:0.1f} "
         f"seconds after {details['tries']} tries."
     )
     logger.warning(f"Total time elapsed: {details['elapsed']:0.1f} seconds.")
 
 
-def _perform_ingress(ingress_path, node_id, prefix, bearer_token, api_gateway_config):
+def _perform_ingress(ingress_path, node_id, prefix, api_gateway_config):
     """
     Performs an ingress request and transfer to S3 using credentials obtained from
     Cognito. This helper function is intended for use with a Joblib parallelized
     loop.
 
     Parameters
     ----------
     ingress_path : str
         Path to the file to request ingress for.
     node_id : str
         The PDS Node Identifier to associate with the ingress request.
     prefix : str
         Global path prefix to trim from the ingress path before making the
         ingress request.
-    bearer_token : str
-        JWT Bearer token string obtained from a successful authentication to
-        Cognito.
     api_gateway_config : dict
         Dictionary containing configuration details for the API Gateway instance
         used to request ingress.
 
     """
     logger = get_logger(__name__)
 
@@ -71,35 +89,103 @@
     with open(ingress_path, "rb") as object_file:
         object_body = object_file.read()
 
     # Remove path prefix if one was configured
     trimmed_path = PathUtil.trim_ingress_path(ingress_path, prefix)
 
     try:
-        s3_ingress_url = request_file_for_ingress(
-            object_body, ingress_path, trimmed_path, node_id, api_gateway_config, bearer_token
-        )
+        s3_ingress_url = request_file_for_ingress(object_body, ingress_path, trimmed_path, node_id, api_gateway_config)
 
         if s3_ingress_url:
-            ingress_file_to_s3(object_body, trimmed_path, s3_ingress_url)
+            ingress_file_to_s3(object_body, ingress_path, trimmed_path, s3_ingress_url)
+            SUMMARY_TABLE["uploaded"].add(trimmed_path)
+        else:
+            SUMMARY_TABLE["skipped"].add(trimmed_path)
     except Exception as err:
         # Only log the error as a warning, so we don't bring down the entire
         # transfer process
         logger.warning(f"{trimmed_path} : Ingress failed, reason: {str(err)}")
+        SUMMARY_TABLE["failed"].add(trimmed_path)
+
+
+def _schedule_token_refresh(refresh_token, token_expiration, offset=60):
+    """
+    Schedules a refresh of the Cognito authentication token using the provided
+    refresh token. This function is inteded to be executed with a separate daemon
+    thread to prevent blocking on the main thread.
+
+    Parameters
+    ----------
+    refresh_token : str
+        The refresh token provided by Cognito.
+    token_expiration : int
+        Time in seconds before the current authentication token is expected to
+        expire.
+    offset : int, optional
+        Offset in seconds to subtract from the token expiration duration to ensure
+        a refresh occurs some time before the expiration deadline. Defaults to
+        60 seconds.
+
+    """
+    # Offset the expiration, so we refresh a bit ahead of time
+    delay = max(token_expiration - offset, offset)
+
+    REFRESH_SCHEDULER.enter(delay, priority=1, action=_token_refresh_event, argument=(refresh_token,))
+
+    # Kick off scheduler
+    # Since this function should be running in a seperate thread, it should be
+    # safe to block until the scheduler fires the next refresh event
+    REFRESH_SCHEDULER.run(blocking=True)
+
+
+def _token_refresh_event(refresh_token):
+    """
+    Callback event evoked when refresh scheduler kicks off a Cognito token refresh.
+    This function will submit the refresh request to Cognito, and if successful,
+    schedules the next refresh interval.
+
+    Parameters
+    ----------
+    refresh_token : str
+        The refresh token provided by Cognito.
+
+    """
+    global BEARER_TOKEN
+
+    logger = get_logger(__name__)
+
+    logger.debug("_token_refresh_event fired")
+
+    config = ConfigUtil.get_config()
+
+    cognito_config = config["COGNITO"]
+
+    # Submit the token refresh request via boto3
+    authentication_result = AuthUtil.refresh_auth_token(cognito_config, refresh_token)
+
+    # Update the authentication token referenced by each ingress worker thread,
+    # as well as the Cloudwatch logger
+    BEARER_TOKEN = AuthUtil.create_bearer_token(authentication_result)
+    log_util.CLOUDWATCH_HANDLER.bearer_token = BEARER_TOKEN
+
+    # Schedule the next refresh iteration
+    expiration = authentication_result["ExpiresIn"]
+
+    _schedule_token_refresh(refresh_token, expiration)
 
 
 @backoff.on_exception(
     backoff.constant,
     requests.exceptions.RequestException,
     max_time=300,
     giveup=fatal_code,
     on_backoff=backoff_logger,
     interval=15,
 )
-def request_file_for_ingress(object_body, ingress_path, trimmed_path, node_id, api_gateway_config, bearer_token):
+def request_file_for_ingress(object_body, ingress_path, trimmed_path, node_id, api_gateway_config):
     """
     Submits a request for file ingress to the PDS Ingress App API.
 
     Parameters
     ----------
     object_body : bytes
         Contents of the file to be copied to S3.
@@ -108,17 +194,14 @@
     trimmed_path : str
         Ingress path with any user-configured prefix removed
     node_id : str
         PDS node identifier.
     api_gateway_config : dict
         Dictionary or dictionary-like containing key/value pairs used to
         configure the API Gateway endpoint url.
-    bearer_token : str
-        The Bearer token authorizing the current user to access the Ingress
-        Lambda function.
 
     Returns
     -------
     s3_ingress_url : str
         The presigned S3 URL returned from the Ingress service lambda, which
         identifies the location in S3 the client should upload the file to and
         includes temporary credentials to allow the client to upload to
@@ -127,14 +210,16 @@
 
     Raises
     ------
     RuntimeError
         If the request to the Ingress Service fails.
 
     """
+    global BEARER_TOKEN
+
     logger = get_logger(__name__)
 
     logger.info(f"{trimmed_path} : Requesting ingress for node ID {node_id}")
 
     # Extract the API Gateway configuration params
     api_gateway_template = api_gateway_config["url_template"]
     api_gateway_id = api_gateway_config["id"]
@@ -152,15 +237,15 @@
     # Get the size and last modified time of the file
     file_size = os.stat(ingress_path).st_size
     last_modified_time = os.path.getmtime(ingress_path)
 
     params = {"node": node_id, "node_name": NodeUtil.node_id_to_long_name[node_id]}
     payload = {"url": trimmed_path}
     headers = {
-        "Authorization": bearer_token,
+        "Authorization": BEARER_TOKEN,
         "UserGroup": NodeUtil.node_id_to_group_name(node_id),
         "ContentMD5": md5_digest,
         "ContentLength": str(file_size),
         "LastModified": str(last_modified_time),
         "content-type": "application/json",
         "x-amz-docs-region": api_gateway_region,
     }
@@ -188,22 +273,24 @@
     backoff.constant,
     requests.exceptions.RequestException,
     max_time=300,
     giveup=fatal_code,
     on_backoff=backoff_logger,
     interval=15,
 )
-def ingress_file_to_s3(object_body, trimmed_path, s3_ingress_url):
+def ingress_file_to_s3(object_body, ingress_path, trimmed_path, s3_ingress_url):
     """
     Copies the local file path to the S3 location returned from the Ingress App.
 
     Parameters
     ----------
     object_body : bytes
         Contents of the file to be copied to S3.
+    ingress_path : str
+        Local path to the file to be ingressed.
     trimmed_path : str
         Trimmed version of the ingress file path. Used for logging purposes.
     s3_ingress_url : str
         The presigned S3 URL used for upload returned from the Ingress Service
         Lambda function.
 
     Raises
@@ -217,14 +304,77 @@
     logger.info(f"{trimmed_path} : Ingesting to {s3_ingress_url.split('?')[0]}")
 
     response = requests.put(s3_ingress_url, data=object_body)
     response.raise_for_status()
 
     logger.info(f"{trimmed_path} : Ingest complete")
 
+    # Update total number of bytes transferrred
+    SUMMARY_TABLE["transferred"] += os.stat(ingress_path).st_size
+
+
+def print_ingress_summary():
+    """Prints the summary report for last execution of the client script."""
+    logger = get_logger(__name__)
+
+    num_uploaded = len(SUMMARY_TABLE["uploaded"])
+    num_skipped = len(SUMMARY_TABLE["skipped"])
+    num_failed = len(SUMMARY_TABLE["failed"])
+    start_time = SUMMARY_TABLE["start_time"]
+    end_time = SUMMARY_TABLE["end_time"]
+    transferred = SUMMARY_TABLE["transferred"]
+
+    title = f"Ingress Summary Report for {str(datetime.now())}"
+
+    logger.info(title)
+    logger.info("-" * len(title))
+    logger.info("Uploaded: %d file(s)", num_uploaded)
+    logger.info("Skipped: %d file(s)", num_skipped)
+    logger.info("Failed: %d file(s)", num_failed)
+    logger.info("Total: %d files(s)", num_uploaded + num_skipped + num_failed)
+    logger.info("Time elapsed: %.2f seconds", end_time - start_time)
+    logger.info("Bytes tranferred: %d", transferred)
+
+
+def create_report_file(args):
+    """
+    Writes a detailed report for the last transfer in JSON format to disk.
+
+    Parameters
+    ----------
+    args : argparse.Namespace
+        The parsed command-line arguments, including the path to write the
+        summary report to. A listing of all provided arguments is included in
+        the report file.
+
+    """
+    logger = get_logger(__name__)
+
+    report = {
+        "Arguments": str(args),
+        "Start Time": str(datetime.fromtimestamp(SUMMARY_TABLE["start_time"], tz=timezone.utc)),
+        "Finish Time": str(datetime.fromtimestamp(SUMMARY_TABLE["end_time"], tz=timezone.utc)),
+        "Uploaded": list(sorted(SUMMARY_TABLE["uploaded"])),
+        "Total Uploaded": len(SUMMARY_TABLE["uploaded"]),
+        "Skipped": list(sorted(SUMMARY_TABLE["skipped"])),
+        "Total Skipped": len(SUMMARY_TABLE["skipped"]),
+        "Failed": list(sorted(SUMMARY_TABLE["failed"])),
+        "Total Failed": len(SUMMARY_TABLE["failed"]),
+        "Bytes Transferred": SUMMARY_TABLE["transferred"],
+    }
+
+    report["Total Files"] = report["Total Uploaded"] + report["Total Skipped"] + report["Total Failed"]
+
+    try:
+        logger.info("Writing JSON summary report to %s", args.report_path)
+        with open(args.report_path, "w") as outfile:
+            json.dump(report, outfile, indent=4)
+    except OSError as err:
+        logger.warning("Failed to write summary report to %s, reason: %s", args.report_path, str(err))
+
 
 def setup_argparser():
     """
     Helper function to perform setup of the ArgumentParser for the Ingress client
     script.
 
     Returns
@@ -276,17 +426,26 @@
         type=int,
         default=-1,
         help="Specify the number of threads to use when uploading "
         "files to S3 in parallel. By default, all available "
         "cores are used.",
     )
     parser.add_argument(
+        "--report-path",
+        "-r",
+        type=str,
+        default=None,
+        help="Specify a path to write a JSON summary report containing "
+        "the full listing of all files ingressed, skipped or failed. "
+        "By default, no report is created.",
+    )
+    parser.add_argument(
         "--dry-run",
         action="store_true",
-        help="Derive the full set of ingress paths without " "performing any submission requests to the server.",
+        help="Derive the full set of ingress paths without performing any submission requests to the server.",
     )
     parser.add_argument(
         "--log-level",
         "-l",
         type=str,
         default=None,
         choices=["warn", "warning", "info", "debug"],
@@ -315,14 +474,16 @@
     Raises
     ------
     ValueError
         If a username and password are not defined within the parsed config,
         and dry-run is not enabled.
 
     """
+    global BEARER_TOKEN
+
     parser = setup_argparser()
 
     args = parser.parse_args()
 
     config = ConfigUtil.get_config(args.config_path)
 
     logger = get_logger(__name__, log_level=get_log_level(args.log_level))
@@ -340,29 +501,51 @@
 
         # TODO: add support for command-line username/password?
         if not cognito_config["username"] and cognito_config["password"]:
             raise ValueError("Username and Password must be specified in the COGNITO portion of the INI config")
 
         authentication_result = AuthUtil.perform_cognito_authentication(cognito_config)
 
-        bearer_token = AuthUtil.create_bearer_token(authentication_result)
+        BEARER_TOKEN = AuthUtil.create_bearer_token(authentication_result)
 
         # Set the bearer token on the CloudWatchHandler singleton, so it can
         # be used to authenticate submissions to the CloudWatch Logs API endpoint
-        log_util.CLOUDWATCH_HANDLER.bearer_token = bearer_token
+        log_util.CLOUDWATCH_HANDLER.bearer_token = BEARER_TOKEN
         log_util.CLOUDWATCH_HANDLER.node_id = node_id
 
+        # Schedule automatic refresh of the Cognito token prior to expiration within
+        # a separate thread. Since this thread will not allocate any
+        # resources, we can designate the thread as a daemon, so it will not
+        # preempt completion of the main thread.
+        refresh_thread = Thread(
+            target=_schedule_token_refresh,
+            name="token_refresh",
+            args=(authentication_result["RefreshToken"], authentication_result["ExpiresIn"]),
+            daemon=True,
+        )
+        refresh_thread.start()
+
         # Perform uploads in parallel using the number of requested threads
         PARALLEL.n_jobs = args.num_threads
 
         PARALLEL(
-            delayed(_perform_ingress)(resolved_ingress_path, node_id, args.prefix, bearer_token, config["API_GATEWAY"])
+            delayed(_perform_ingress)(resolved_ingress_path, node_id, args.prefix, config["API_GATEWAY"])
             for resolved_ingress_path in resolved_ingress_paths
         )
 
+        # Capture completion time of transfer
+        SUMMARY_TABLE["end_time"] = time.time()
+
+        # Print the summary table
+        print_ingress_summary()
+
+        # Create the JSON report file, if requested
+        if args.report_path:
+            create_report_file(args)
+
         # Flush all logged statements to CloudWatch Logs
         log_util.CLOUDWATCH_HANDLER.flush()
     else:
         logger.info("Dry run requested, skipping ingress request submission.")
 
 
 if __name__ == "__main__":
```

## pds/ingress/util/auth_util.py

```diff
@@ -87,7 +87,49 @@
         logger.info("Creating Bearer token")
 
         access_token = authentication_result["AccessToken"]
 
         bearer_token = f"Bearer {access_token}"
 
         return bearer_token
+
+    @staticmethod
+    def refresh_auth_token(cognito_config, refresh_token):
+        """
+        Performs a Cognito authentication token refresh request, returning a
+        new authentication token for use with the worker threads and CloudWatch
+        logger.
+
+        Parameters
+        ----------
+        cognito_config : dict
+            The Cognito configuration parameters as read from the INI config.
+        refresh_token : str
+            The refresh token provided by Cognito.
+
+        Returns
+        -------
+        authentication_result : dict
+            Dictionary containing the results of the authentication refresh.
+            This includes an updated authentication token and expiration time.
+
+        """
+        logger = get_logger(__name__)
+
+        client = boto3.client("cognito-idp", region_name=cognito_config["region"])
+
+        auth_params = {"REFRESH_TOKEN": refresh_token}
+
+        logger.info("Refreshing authentication token")
+
+        try:
+            response = client.initiate_auth(
+                AuthFlow="REFRESH_TOKEN_AUTH", AuthParameters=auth_params, ClientId=cognito_config["client_id"]
+            )
+        except Exception as err:
+            raise RuntimeError(f"Failed to refresh Cognito authentication token, reason: {str(err)}") from err
+
+        logger.info("Token refresh successful")
+
+        authentication_result = response["AuthenticationResult"]
+
+        return authentication_result
```

## pds/ingress/util/path_util.py

```diff
@@ -50,15 +50,15 @@
 
             if os.path.isfile(abs_user_path):
                 logger.debug(f"Resolved path {abs_user_path}")
 
                 resolved_paths.append(abs_user_path)
             elif os.path.isdir(abs_user_path):
                 logger.debug(f"Resolving directory {abs_user_path}")
-                for grouping in os.walk(abs_user_path, topdown=True):
+                for grouping in os.walk(abs_user_path, topdown=True, followlinks=True):
                     dirpath, _, filenames = grouping
 
                     # TODO: add option to include hidden files
                     # TODO: add support for include/exclude path filters
                     product_paths = [
                         os.path.join(dirpath, filename)
                         for filename in filter(lambda name: not name.startswith("."), filenames)
```

## Comparing `pds_data_upload_manager-1.1.0.dist-info/LICENSE.md` & `pds_data_upload_manager-1.2.0.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `pds_data_upload_manager-1.1.0.dist-info/METADATA` & `pds_data_upload_manager-1.2.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pds-data-upload-manager
-Version: 1.1.0
+Version: 1.2.0
 Summary: Planetary Data Service Data Delivery Manager
 Home-page: https://github.com/NASA-PDS/data-upload-manager
 Download-URL: https://github.com/NASA-PDS/data-upload-manager/releases/
 Author: PDS
 Author-email: pds_operator@jpl.nasa.gov
 License: apache-2.0
 Keywords: pds,planetary data,aws,s3,ingress,data upload
```

## Comparing `pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt` & `pds_data_upload_manager-1.2.0.dist-info/NOTICE.txt`

 * *Files identical despite different names*

## Comparing `pds_data_upload_manager-1.1.0.dist-info/RECORD` & `pds_data_upload_manager-1.2.0.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-pds/ingress/VERSION.txt,sha256=FXXhr0qV8S9wtO5qatzoFglT2T6hfcJhG5CIPMw607g,6
+pds/ingress/VERSION.txt,sha256=HltRzeUVOWqfp2KQnPjKZYTMxWSzJdLuvup2F1_pXE0,6
 pds/ingress/__init__.py,sha256=agL6n-Xp3A2qjl4e1JnNWNeqxuWrwOOPa3uMO1KlFCQ,320
 pds/ingress/authorizer/README.md,sha256=0dtXHIm_7xFziuFtaoKMOwAiRB9Jo9tVnJPybkf_Cug,1910
 pds/ingress/authorizer/index.js,sha256=FvSrByKrvh8OG9Te-raJ60EslmXiZdJ1SHzhUiQ-qnU,3926
 pds/ingress/authorizer/package-lock.json,sha256=3N148t7ZjRnHiWv8twJWToIXMxvh6Tp8_pV8fa6JzwA,8692
 pds/ingress/authorizer/package.json,sha256=78mRokNuOqcEA6ZVo-8OnZxsDoY8b88mPrXp639xw5g,322
-pds/ingress/client/pds_ingress_client.py,sha256=VnRar3QqCd98c7xxJE19L8C8mMpJdRXZR8FjQiUcEvw,12328
+pds/ingress/client/pds_ingress_client.py,sha256=WBlv7EK2SwWlsuZEZ6VDt6iZkyGwN9fz5jLecUbtViQ,18798
 pds/ingress/service/pds_ingress_app.py,sha256=axs2ei7GhbynxQ0XCpYfQ-oxqQUdrKuuN_ZhAV1ZMGc,7714
 pds/ingress/service/config/bucket-map.yaml,sha256=f1y6GK5JkBObGZFnmuP5xwWYQp66Lsl8s-JfqBI4Ml0,454
-pds/ingress/util/auth_util.py,sha256=w7wSlkIqzVoFqE6XO-7ZCSuT8dgJbCqMb-uwTr5kGFk,2688
+pds/ingress/util/auth_util.py,sha256=s13pTyG_4v1L1Sl7YzKUCSoZluIjtYUuLGgJoNG2LgQ,4132
 pds/ingress/util/conf.default.ini,sha256=tEpMh443CJBmqUW0vEhoBB9mwiAG5EzZEL2hmCk5dmE,555
 pds/ingress/util/config_util.py,sha256=QqCVqhtiiLgegHjrOMPCfe1CQzSWnJI41O9aYgPLjAw,1975
 pds/ingress/util/log_util.py,sha256=420Me8QHQJQRtIWmlnGIb7Yvq0sJKEAXo2eRnDRAZzk,11032
 pds/ingress/util/node_util.py,sha256=tLiPvu4sg3zYNUig_phSeQBGuerZ8XmXJvQHrrfx1wM,1077
-pds/ingress/util/path_util.py,sha256=-W9TdVmz4WfayHn2LFKCCc4VYTxpCiPua1tjhHEHpHI,3815
-pds_data_upload_manager-1.1.0.dist-info/LICENSE.md,sha256=Lh-qBbuRV0-jiCIBhfV7NgdwFxQFOXH3BKOzK865hRs,10480
-pds_data_upload_manager-1.1.0.dist-info/METADATA,sha256=h1LT9z4h-j8vyQDlu6k51KlO2yYa8wvjLcNv8M_Gap4,4842
-pds_data_upload_manager-1.1.0.dist-info/NOTICE.txt,sha256=DwSivJgxEEg-PP1U7fHOhwR2vKotAX4enw5v32OYE1c,1632
-pds_data_upload_manager-1.1.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-pds_data_upload_manager-1.1.0.dist-info/entry_points.txt,sha256=4fRxirWyCD8J9t0zAW3NEeDY4IbUfNC1316OPeg3XWI,82
-pds_data_upload_manager-1.1.0.dist-info/top_level.txt,sha256=5SacHJznU3B9RSALhTt0yjidG0EmPyBMNQgoU-bxvo4,4
-pds_data_upload_manager-1.1.0.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-pds_data_upload_manager-1.1.0.dist-info/RECORD,,
+pds/ingress/util/path_util.py,sha256=md2mj7BWFCR4xxXVh2197JsASYnUHvFHexkTEYSpYto,3833
+pds_data_upload_manager-1.2.0.dist-info/LICENSE.md,sha256=Lh-qBbuRV0-jiCIBhfV7NgdwFxQFOXH3BKOzK865hRs,10480
+pds_data_upload_manager-1.2.0.dist-info/METADATA,sha256=0le0tFBG7LFVksFhdpdPmz4Sv5iWDQc1_X3e9R1pASk,4842
+pds_data_upload_manager-1.2.0.dist-info/NOTICE.txt,sha256=DwSivJgxEEg-PP1U7fHOhwR2vKotAX4enw5v32OYE1c,1632
+pds_data_upload_manager-1.2.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+pds_data_upload_manager-1.2.0.dist-info/entry_points.txt,sha256=4fRxirWyCD8J9t0zAW3NEeDY4IbUfNC1316OPeg3XWI,82
+pds_data_upload_manager-1.2.0.dist-info/top_level.txt,sha256=5SacHJznU3B9RSALhTt0yjidG0EmPyBMNQgoU-bxvo4,4
+pds_data_upload_manager-1.2.0.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+pds_data_upload_manager-1.2.0.dist-info/RECORD,,
```

